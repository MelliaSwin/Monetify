{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelliaSwin/C-Projects/blob/main/NST_%2B_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B2NppHabebH",
        "outputId": "e38607ba-671f-4d94-e248-c90a0c25ae39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C00RmAdLFxeX",
        "outputId": "9f68694a-8796-4420-fdbc-c96519f4cb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio)\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.0 (from gradio)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.2)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=0f428f5e3b80ec3a70a6d0597494238950b52551c63bb2d87d7826a1feda9a0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, altair, fastapi, gradio\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed aiofiles-23.2.1 altair-5.3.0 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.38.1 gradio-client-1.1.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.5.2 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ZFOQ76PWtK"
      },
      "source": [
        "MONET_DATASET: WRAPPING THE IMAGES INTO A DATATSET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRbuDfBSfkpo",
        "outputId": "b9c664b6-5ba0-47a2-a305-2206958c21f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in dataset: 1072\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define a custom dataset\n",
        "class MonetDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # For RGB images\n",
        "])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "monet_dataset = MonetDataset('/content/drive/MyDrive/monetproj/monet_paintings', transform=transform)\n",
        "dataloader = DataLoader(monet_dataset, batch_size=64, shuffle=True)  # Batch size is 64\n",
        "\n",
        "# Print the number of images in the dataset\n",
        "print(f'Number of images in dataset: {len(monet_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C1mc0gAPk1D"
      },
      "source": [
        "GAN TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oLOzZFS3u0Uh",
        "outputId": "67e1531a-bf41-42d6-d60a-ee4305a804c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/2500], Step [0/17], d_loss: 1.5028843879699707, g_loss: 3.47491455078125\n",
            "Epoch [1/2500], Step [0/17], d_loss: 0.056156739592552185, g_loss: 8.824560165405273\n",
            "Epoch [2/2500], Step [0/17], d_loss: 2.488489866256714, g_loss: 13.823477745056152\n",
            "Epoch [3/2500], Step [0/17], d_loss: 0.2883727252483368, g_loss: 8.395459175109863\n",
            "Epoch [4/2500], Step [0/17], d_loss: 0.07261165976524353, g_loss: 4.3854594230651855\n",
            "Epoch [5/2500], Step [0/17], d_loss: 1.219720721244812, g_loss: 9.9318208694458\n",
            "Epoch [6/2500], Step [0/17], d_loss: 2.403557777404785, g_loss: 1.9664618968963623\n",
            "Epoch [7/2500], Step [0/17], d_loss: 0.9108132123947144, g_loss: 2.929827928543091\n",
            "Epoch [8/2500], Step [0/17], d_loss: 0.807418704032898, g_loss: 2.384847640991211\n",
            "Epoch [9/2500], Step [0/17], d_loss: 0.5663180351257324, g_loss: 1.8668241500854492\n",
            "Epoch [10/2500], Step [0/17], d_loss: 0.4756142497062683, g_loss: 3.1676225662231445\n",
            "Epoch [11/2500], Step [0/17], d_loss: 0.8700436949729919, g_loss: 3.0607001781463623\n",
            "Epoch [12/2500], Step [0/17], d_loss: 0.7822205424308777, g_loss: 2.0886635780334473\n",
            "Epoch [13/2500], Step [0/17], d_loss: 1.2569345235824585, g_loss: 1.972470998764038\n",
            "Epoch [14/2500], Step [0/17], d_loss: 0.888100266456604, g_loss: 1.725414752960205\n",
            "Epoch [15/2500], Step [0/17], d_loss: 0.7979913353919983, g_loss: 1.732372522354126\n",
            "Epoch [16/2500], Step [0/17], d_loss: 0.8690609931945801, g_loss: 1.7820789813995361\n",
            "Epoch [17/2500], Step [0/17], d_loss: 0.5437057614326477, g_loss: 2.8276479244232178\n",
            "Epoch [18/2500], Step [0/17], d_loss: 0.7095621228218079, g_loss: 2.124910831451416\n",
            "Epoch [19/2500], Step [0/17], d_loss: 0.4563841223716736, g_loss: 2.67378830909729\n",
            "Epoch [20/2500], Step [0/17], d_loss: 0.7754851579666138, g_loss: 2.4454925060272217\n",
            "Epoch [21/2500], Step [0/17], d_loss: 0.769150972366333, g_loss: 2.8755877017974854\n",
            "Epoch [22/2500], Step [0/17], d_loss: 1.0462360382080078, g_loss: 7.896824359893799\n",
            "Epoch [23/2500], Step [0/17], d_loss: 0.6018138527870178, g_loss: 2.6413283348083496\n",
            "Epoch [24/2500], Step [0/17], d_loss: 0.7090686559677124, g_loss: 2.6818172931671143\n",
            "Epoch [25/2500], Step [0/17], d_loss: 0.7646307945251465, g_loss: 5.173629283905029\n",
            "Epoch [26/2500], Step [0/17], d_loss: 0.7807429432868958, g_loss: 4.322177410125732\n",
            "Epoch [27/2500], Step [0/17], d_loss: 0.5745745897293091, g_loss: 3.1690080165863037\n",
            "Epoch [28/2500], Step [0/17], d_loss: 1.7501716613769531, g_loss: 8.23495864868164\n",
            "Epoch [29/2500], Step [0/17], d_loss: 1.1777887344360352, g_loss: 3.5833399295806885\n",
            "Epoch [30/2500], Step [0/17], d_loss: 0.7023525834083557, g_loss: 2.1393661499023438\n",
            "Epoch [31/2500], Step [0/17], d_loss: 0.725543737411499, g_loss: 1.8500686883926392\n",
            "Epoch [32/2500], Step [0/17], d_loss: 0.8332110643386841, g_loss: 2.9833803176879883\n",
            "Epoch [33/2500], Step [0/17], d_loss: 0.7162817120552063, g_loss: 3.730865001678467\n",
            "Epoch [34/2500], Step [0/17], d_loss: 0.7859545350074768, g_loss: 1.9826301336288452\n",
            "Epoch [35/2500], Step [0/17], d_loss: 0.5813145041465759, g_loss: 3.302124500274658\n",
            "Epoch [36/2500], Step [0/17], d_loss: 1.002081274986267, g_loss: 4.06190299987793\n",
            "Epoch [37/2500], Step [0/17], d_loss: 0.7986212968826294, g_loss: 2.391446352005005\n",
            "Epoch [38/2500], Step [0/17], d_loss: 1.1512330770492554, g_loss: 2.496241807937622\n",
            "Epoch [39/2500], Step [0/17], d_loss: 0.46531540155410767, g_loss: 2.9076480865478516\n",
            "Epoch [40/2500], Step [0/17], d_loss: 0.47983208298683167, g_loss: 2.6896727085113525\n",
            "Epoch [41/2500], Step [0/17], d_loss: 0.7254579067230225, g_loss: 3.223188638687134\n",
            "Epoch [42/2500], Step [0/17], d_loss: 0.9519495964050293, g_loss: 2.1467816829681396\n",
            "Epoch [43/2500], Step [0/17], d_loss: 0.494851678609848, g_loss: 2.6426968574523926\n",
            "Epoch [44/2500], Step [0/17], d_loss: 0.5695581436157227, g_loss: 3.8497724533081055\n",
            "Epoch [45/2500], Step [0/17], d_loss: 0.4410895109176636, g_loss: 2.8199827671051025\n",
            "Epoch [46/2500], Step [0/17], d_loss: 0.499264121055603, g_loss: 3.2071385383605957\n",
            "Epoch [47/2500], Step [0/17], d_loss: 0.8699148297309875, g_loss: 2.1631577014923096\n",
            "Epoch [48/2500], Step [0/17], d_loss: 0.5928840637207031, g_loss: 2.4010162353515625\n",
            "Epoch [49/2500], Step [0/17], d_loss: 0.5727801322937012, g_loss: 2.770582675933838\n",
            "Epoch [50/2500], Step [0/17], d_loss: 0.7698007822036743, g_loss: 3.9954535961151123\n",
            "Epoch [51/2500], Step [0/17], d_loss: 1.1667110919952393, g_loss: 4.083275318145752\n",
            "Epoch [52/2500], Step [0/17], d_loss: 0.6218880414962769, g_loss: 3.4861464500427246\n",
            "Epoch [53/2500], Step [0/17], d_loss: 0.6260783672332764, g_loss: 2.9685959815979004\n",
            "Epoch [54/2500], Step [0/17], d_loss: 0.7489302158355713, g_loss: 3.7303061485290527\n",
            "Epoch [55/2500], Step [0/17], d_loss: 0.6846094727516174, g_loss: 3.2310571670532227\n",
            "Epoch [56/2500], Step [0/17], d_loss: 0.8681939244270325, g_loss: 2.538691520690918\n",
            "Epoch [57/2500], Step [0/17], d_loss: 0.6191043257713318, g_loss: 4.1100921630859375\n",
            "Epoch [58/2500], Step [0/17], d_loss: 1.1503865718841553, g_loss: 6.442649841308594\n",
            "Epoch [59/2500], Step [0/17], d_loss: 0.6052946448326111, g_loss: 3.3858025074005127\n",
            "Epoch [60/2500], Step [0/17], d_loss: 0.4890762269496918, g_loss: 3.4330687522888184\n",
            "Epoch [61/2500], Step [0/17], d_loss: 0.6113710403442383, g_loss: 2.856076717376709\n",
            "Epoch [62/2500], Step [0/17], d_loss: 0.7724946737289429, g_loss: 4.672738075256348\n",
            "Epoch [63/2500], Step [0/17], d_loss: 0.8645639419555664, g_loss: 5.411184310913086\n",
            "Epoch [64/2500], Step [0/17], d_loss: 0.5336371064186096, g_loss: 3.686307668685913\n",
            "Epoch [65/2500], Step [0/17], d_loss: 0.5783964395523071, g_loss: 4.712632179260254\n",
            "Epoch [66/2500], Step [0/17], d_loss: 0.6511086821556091, g_loss: 3.0160017013549805\n",
            "Epoch [67/2500], Step [0/17], d_loss: 0.7078894376754761, g_loss: 4.37197732925415\n",
            "Epoch [68/2500], Step [0/17], d_loss: 0.7100436687469482, g_loss: 4.081088066101074\n",
            "Epoch [69/2500], Step [0/17], d_loss: 0.8611617684364319, g_loss: 6.419496536254883\n",
            "Epoch [70/2500], Step [0/17], d_loss: 0.4049462676048279, g_loss: 3.8272173404693604\n",
            "Epoch [71/2500], Step [0/17], d_loss: 0.7043366432189941, g_loss: 4.939145088195801\n",
            "Epoch [72/2500], Step [0/17], d_loss: 0.5182458758354187, g_loss: 3.517573356628418\n",
            "Epoch [73/2500], Step [0/17], d_loss: 0.48591554164886475, g_loss: 3.1750826835632324\n",
            "Epoch [74/2500], Step [0/17], d_loss: 0.41832828521728516, g_loss: 3.748631238937378\n",
            "Epoch [75/2500], Step [0/17], d_loss: 0.814140796661377, g_loss: 6.0801801681518555\n",
            "Epoch [76/2500], Step [0/17], d_loss: 0.4134071469306946, g_loss: 4.084352970123291\n",
            "Epoch [77/2500], Step [0/17], d_loss: 0.38436681032180786, g_loss: 4.432075500488281\n",
            "Epoch [78/2500], Step [0/17], d_loss: 0.415439248085022, g_loss: 3.7948970794677734\n",
            "Epoch [79/2500], Step [0/17], d_loss: 0.5622125864028931, g_loss: 2.763441801071167\n",
            "Epoch [80/2500], Step [0/17], d_loss: 0.40704864263534546, g_loss: 5.035872936248779\n",
            "Epoch [81/2500], Step [0/17], d_loss: 0.6344931125640869, g_loss: 6.512798309326172\n",
            "Epoch [82/2500], Step [0/17], d_loss: 0.26136940717697144, g_loss: 4.129261493682861\n",
            "Epoch [83/2500], Step [0/17], d_loss: 0.7469770312309265, g_loss: 5.762082576751709\n",
            "Epoch [84/2500], Step [0/17], d_loss: 0.5993945598602295, g_loss: 4.081473350524902\n",
            "Epoch [85/2500], Step [0/17], d_loss: 0.2037869393825531, g_loss: 4.082597732543945\n",
            "Epoch [86/2500], Step [0/17], d_loss: 0.4271555542945862, g_loss: 4.829294681549072\n",
            "Epoch [87/2500], Step [0/17], d_loss: 0.30997616052627563, g_loss: 2.9568653106689453\n",
            "Epoch [88/2500], Step [0/17], d_loss: 0.40414559841156006, g_loss: 5.228833198547363\n",
            "Epoch [89/2500], Step [0/17], d_loss: 0.4710383117198944, g_loss: 4.056565284729004\n",
            "Epoch [90/2500], Step [0/17], d_loss: 1.1512465476989746, g_loss: 7.208467483520508\n",
            "Epoch [91/2500], Step [0/17], d_loss: 0.6452020406723022, g_loss: 5.480279445648193\n",
            "Epoch [92/2500], Step [0/17], d_loss: 0.39081576466560364, g_loss: 3.3406622409820557\n",
            "Epoch [93/2500], Step [0/17], d_loss: 0.35373032093048096, g_loss: 5.893393516540527\n",
            "Epoch [94/2500], Step [0/17], d_loss: 0.7440653443336487, g_loss: 5.717611789703369\n",
            "Epoch [95/2500], Step [0/17], d_loss: 0.35080862045288086, g_loss: 4.556906223297119\n",
            "Epoch [96/2500], Step [0/17], d_loss: 0.32879847288131714, g_loss: 4.29788875579834\n",
            "Epoch [97/2500], Step [0/17], d_loss: 0.3317224979400635, g_loss: 4.13508939743042\n",
            "Epoch [98/2500], Step [0/17], d_loss: 0.370699405670166, g_loss: 4.491147041320801\n",
            "Epoch [99/2500], Step [0/17], d_loss: 0.3258994221687317, g_loss: 4.839510440826416\n",
            "Epoch [100/2500], Step [0/17], d_loss: 2.4172616004943848, g_loss: 10.756166458129883\n",
            "Epoch [101/2500], Step [0/17], d_loss: 0.7008607387542725, g_loss: 6.24763298034668\n",
            "Epoch [102/2500], Step [0/17], d_loss: 0.3147692084312439, g_loss: 4.662967681884766\n",
            "Epoch [103/2500], Step [0/17], d_loss: 0.29614192247390747, g_loss: 5.404557228088379\n",
            "Epoch [104/2500], Step [0/17], d_loss: 0.3629397451877594, g_loss: 5.684895038604736\n",
            "Epoch [105/2500], Step [0/17], d_loss: 0.33524978160858154, g_loss: 4.679037570953369\n",
            "Epoch [106/2500], Step [0/17], d_loss: 0.3966570496559143, g_loss: 5.411779403686523\n",
            "Epoch [107/2500], Step [0/17], d_loss: 0.7642844319343567, g_loss: 1.9576468467712402\n",
            "Epoch [108/2500], Step [0/17], d_loss: 0.3717833161354065, g_loss: 4.629598140716553\n",
            "Epoch [109/2500], Step [0/17], d_loss: 0.4035542905330658, g_loss: 3.953518867492676\n",
            "Epoch [110/2500], Step [0/17], d_loss: 0.42796260118484497, g_loss: 6.424470901489258\n",
            "Epoch [111/2500], Step [0/17], d_loss: 0.2676158845424652, g_loss: 4.660783767700195\n",
            "Epoch [112/2500], Step [0/17], d_loss: 0.5639342069625854, g_loss: 3.3130993843078613\n",
            "Epoch [113/2500], Step [0/17], d_loss: 1.0240501165390015, g_loss: 8.478479385375977\n",
            "Epoch [114/2500], Step [0/17], d_loss: 0.4170028269290924, g_loss: 5.705169200897217\n",
            "Epoch [115/2500], Step [0/17], d_loss: 0.26146915555000305, g_loss: 4.257331371307373\n",
            "Epoch [116/2500], Step [0/17], d_loss: 0.19462735950946808, g_loss: 4.482946872711182\n",
            "Epoch [117/2500], Step [0/17], d_loss: 0.26460716128349304, g_loss: 5.441224098205566\n",
            "Epoch [118/2500], Step [0/17], d_loss: 0.23903924226760864, g_loss: 5.124265670776367\n",
            "Epoch [119/2500], Step [0/17], d_loss: 0.33982470631599426, g_loss: 4.2898101806640625\n",
            "Epoch [120/2500], Step [0/17], d_loss: 0.2867044508457184, g_loss: 5.64027214050293\n",
            "Epoch [121/2500], Step [0/17], d_loss: 0.2376597672700882, g_loss: 3.7257182598114014\n",
            "Epoch [122/2500], Step [0/17], d_loss: 0.29842501878738403, g_loss: 4.03749418258667\n",
            "Epoch [123/2500], Step [0/17], d_loss: 0.18464213609695435, g_loss: 4.951612949371338\n",
            "Epoch [124/2500], Step [0/17], d_loss: 0.2285422384738922, g_loss: 4.853801727294922\n",
            "Epoch [125/2500], Step [0/17], d_loss: 0.37923046946525574, g_loss: 7.7662248611450195\n",
            "Epoch [126/2500], Step [0/17], d_loss: 0.22918647527694702, g_loss: 4.832305908203125\n",
            "Epoch [127/2500], Step [0/17], d_loss: 0.3667808175086975, g_loss: 4.840556621551514\n",
            "Epoch [128/2500], Step [0/17], d_loss: 0.15435370802879333, g_loss: 3.678637981414795\n",
            "Epoch [129/2500], Step [0/17], d_loss: 0.28943824768066406, g_loss: 4.576704502105713\n",
            "Epoch [130/2500], Step [0/17], d_loss: 0.15626847743988037, g_loss: 5.010203838348389\n",
            "Epoch [131/2500], Step [0/17], d_loss: 0.6937278509140015, g_loss: 6.63674259185791\n",
            "Epoch [132/2500], Step [0/17], d_loss: 0.3072952330112457, g_loss: 4.316483020782471\n",
            "Epoch [133/2500], Step [0/17], d_loss: 0.23701056838035583, g_loss: 4.537605285644531\n",
            "Epoch [134/2500], Step [0/17], d_loss: 0.2708650231361389, g_loss: 3.7367608547210693\n",
            "Epoch [135/2500], Step [0/17], d_loss: 0.16758111119270325, g_loss: 4.266772270202637\n",
            "Epoch [136/2500], Step [0/17], d_loss: 0.14613574743270874, g_loss: 4.083893775939941\n",
            "Epoch [137/2500], Step [0/17], d_loss: 0.11939822137355804, g_loss: 5.373835563659668\n",
            "Epoch [138/2500], Step [0/17], d_loss: 0.154220312833786, g_loss: 4.218473434448242\n",
            "Epoch [139/2500], Step [0/17], d_loss: 0.09128040820360184, g_loss: 5.289684772491455\n",
            "Epoch [140/2500], Step [0/17], d_loss: 0.30031558871269226, g_loss: 4.134067058563232\n",
            "Epoch [141/2500], Step [0/17], d_loss: 0.09529179334640503, g_loss: 4.688053607940674\n",
            "Epoch [142/2500], Step [0/17], d_loss: 0.151868537068367, g_loss: 4.895208835601807\n",
            "Epoch [143/2500], Step [0/17], d_loss: 0.14543914794921875, g_loss: 5.273033142089844\n",
            "Epoch [144/2500], Step [0/17], d_loss: 0.2790161967277527, g_loss: 7.299489974975586\n",
            "Epoch [145/2500], Step [0/17], d_loss: 0.13025274872779846, g_loss: 5.072550296783447\n",
            "Epoch [146/2500], Step [0/17], d_loss: 0.28793877363204956, g_loss: 4.893026351928711\n",
            "Epoch [147/2500], Step [0/17], d_loss: 0.17881403863430023, g_loss: 5.660344123840332\n",
            "Epoch [148/2500], Step [0/17], d_loss: 3.5645668506622314, g_loss: 10.87697982788086\n",
            "Epoch [149/2500], Step [0/17], d_loss: 0.3951547145843506, g_loss: 4.511667251586914\n",
            "Epoch [150/2500], Step [0/17], d_loss: 0.19042839109897614, g_loss: 5.40762186050415\n",
            "Epoch [151/2500], Step [0/17], d_loss: 0.17044803500175476, g_loss: 4.600963592529297\n",
            "Epoch [152/2500], Step [0/17], d_loss: 0.14889605343341827, g_loss: 4.281820297241211\n",
            "Epoch [153/2500], Step [0/17], d_loss: 0.16663634777069092, g_loss: 4.253711700439453\n",
            "Epoch [154/2500], Step [0/17], d_loss: 0.13186880946159363, g_loss: 4.433870315551758\n",
            "Epoch [155/2500], Step [0/17], d_loss: 0.22633734345436096, g_loss: 4.276111602783203\n",
            "Epoch [156/2500], Step [0/17], d_loss: 0.15295232832431793, g_loss: 4.625778675079346\n",
            "Epoch [157/2500], Step [0/17], d_loss: 0.08424809575080872, g_loss: 4.6119465827941895\n",
            "Epoch [158/2500], Step [0/17], d_loss: 0.12254126369953156, g_loss: 4.047987461090088\n",
            "Epoch [159/2500], Step [0/17], d_loss: 0.0687158927321434, g_loss: 4.8250322341918945\n",
            "Epoch [160/2500], Step [0/17], d_loss: 0.10048989951610565, g_loss: 4.30671501159668\n",
            "Epoch [161/2500], Step [0/17], d_loss: 0.09117382764816284, g_loss: 4.7924652099609375\n",
            "Epoch [162/2500], Step [0/17], d_loss: 0.12248378247022629, g_loss: 5.429978370666504\n",
            "Epoch [163/2500], Step [0/17], d_loss: 0.08058734238147736, g_loss: 4.787881851196289\n",
            "Epoch [164/2500], Step [0/17], d_loss: 0.08275182545185089, g_loss: 4.205984115600586\n",
            "Epoch [165/2500], Step [0/17], d_loss: 0.45182105898857117, g_loss: 6.915032386779785\n",
            "Epoch [166/2500], Step [0/17], d_loss: 1.1316800117492676, g_loss: 8.580679893493652\n",
            "Epoch [167/2500], Step [0/17], d_loss: 0.15954004228115082, g_loss: 4.833035945892334\n",
            "Epoch [168/2500], Step [0/17], d_loss: 0.18141566216945648, g_loss: 4.320158958435059\n",
            "Epoch [169/2500], Step [0/17], d_loss: 0.1567910611629486, g_loss: 5.0594987869262695\n",
            "Epoch [170/2500], Step [0/17], d_loss: 0.12060587108135223, g_loss: 5.045016288757324\n",
            "Epoch [171/2500], Step [0/17], d_loss: 0.10869234055280685, g_loss: 5.0660858154296875\n",
            "Epoch [172/2500], Step [0/17], d_loss: 0.06371474266052246, g_loss: 4.9161224365234375\n",
            "Epoch [173/2500], Step [0/17], d_loss: 0.07503481209278107, g_loss: 4.584527015686035\n",
            "Epoch [174/2500], Step [0/17], d_loss: 0.11471407860517502, g_loss: 4.780760765075684\n",
            "Epoch [175/2500], Step [0/17], d_loss: 0.0643424466252327, g_loss: 4.7653398513793945\n",
            "Epoch [176/2500], Step [0/17], d_loss: 0.11211429536342621, g_loss: 4.028910160064697\n",
            "Epoch [177/2500], Step [0/17], d_loss: 0.08898389339447021, g_loss: 4.4606828689575195\n",
            "Epoch [178/2500], Step [0/17], d_loss: 0.04265805333852768, g_loss: 4.503486633300781\n",
            "Epoch [179/2500], Step [0/17], d_loss: 0.04658421128988266, g_loss: 4.872861862182617\n",
            "Epoch [180/2500], Step [0/17], d_loss: 0.06281678378582001, g_loss: 4.225602149963379\n",
            "Epoch [181/2500], Step [0/17], d_loss: 0.10864883661270142, g_loss: 5.4822282791137695\n",
            "Epoch [182/2500], Step [0/17], d_loss: 0.06362985819578171, g_loss: 4.173073768615723\n",
            "Epoch [183/2500], Step [0/17], d_loss: 0.06526502966880798, g_loss: 4.821895599365234\n",
            "Epoch [184/2500], Step [0/17], d_loss: 0.08590950071811676, g_loss: 4.711148262023926\n",
            "Epoch [185/2500], Step [0/17], d_loss: 0.08555957674980164, g_loss: 5.524316310882568\n",
            "Epoch [186/2500], Step [0/17], d_loss: 0.18735027313232422, g_loss: 6.164021015167236\n",
            "Epoch [187/2500], Step [0/17], d_loss: 0.09661290049552917, g_loss: 5.295534610748291\n",
            "Epoch [188/2500], Step [0/17], d_loss: 0.04007788747549057, g_loss: 4.825262069702148\n",
            "Epoch [189/2500], Step [0/17], d_loss: 2.874248743057251, g_loss: 0.21280109882354736\n",
            "Epoch [190/2500], Step [0/17], d_loss: 0.6162279844284058, g_loss: 7.324323654174805\n",
            "Epoch [191/2500], Step [0/17], d_loss: 0.6812877655029297, g_loss: 8.779414176940918\n",
            "Epoch [192/2500], Step [0/17], d_loss: 0.1962050199508667, g_loss: 5.132452011108398\n",
            "Epoch [193/2500], Step [0/17], d_loss: 0.14733336865901947, g_loss: 5.545652866363525\n",
            "Epoch [194/2500], Step [0/17], d_loss: 0.11526789516210556, g_loss: 4.844662189483643\n",
            "Epoch [195/2500], Step [0/17], d_loss: 0.17588672041893005, g_loss: 5.298982620239258\n",
            "Epoch [196/2500], Step [0/17], d_loss: 0.0813806802034378, g_loss: 4.696043014526367\n",
            "Epoch [197/2500], Step [0/17], d_loss: 0.06667158007621765, g_loss: 4.971762657165527\n",
            "Epoch [198/2500], Step [0/17], d_loss: 0.06386912614107132, g_loss: 4.9130706787109375\n",
            "Epoch [199/2500], Step [0/17], d_loss: 0.05438894033432007, g_loss: 4.973781585693359\n",
            "Epoch [200/2500], Step [0/17], d_loss: 0.06828045845031738, g_loss: 4.824777603149414\n",
            "Epoch [201/2500], Step [0/17], d_loss: 0.052610013633966446, g_loss: 4.9219794273376465\n",
            "Epoch [202/2500], Step [0/17], d_loss: 0.05568631738424301, g_loss: 4.677995204925537\n",
            "Epoch [203/2500], Step [0/17], d_loss: 0.059161871671676636, g_loss: 4.433133125305176\n",
            "Epoch [204/2500], Step [0/17], d_loss: 0.05518314987421036, g_loss: 4.592626094818115\n",
            "Epoch [205/2500], Step [0/17], d_loss: 0.052205733954906464, g_loss: 5.109106063842773\n",
            "Epoch [206/2500], Step [0/17], d_loss: 0.052076634019613266, g_loss: 4.990645408630371\n",
            "Epoch [207/2500], Step [0/17], d_loss: 0.045252446085214615, g_loss: 4.686648368835449\n",
            "Epoch [208/2500], Step [0/17], d_loss: 0.05018651857972145, g_loss: 4.696988582611084\n",
            "Epoch [209/2500], Step [0/17], d_loss: 0.04089609533548355, g_loss: 4.85656213760376\n",
            "Epoch [210/2500], Step [0/17], d_loss: 0.03911041468381882, g_loss: 5.1807708740234375\n",
            "Epoch [211/2500], Step [0/17], d_loss: 0.04865892603993416, g_loss: 4.751128196716309\n",
            "Epoch [212/2500], Step [0/17], d_loss: 0.05380399152636528, g_loss: 4.815703392028809\n",
            "Epoch [213/2500], Step [0/17], d_loss: 0.04208272695541382, g_loss: 4.8843278884887695\n",
            "Epoch [214/2500], Step [0/17], d_loss: 0.029658464714884758, g_loss: 5.085097312927246\n",
            "Epoch [215/2500], Step [0/17], d_loss: 0.027465004473924637, g_loss: 5.433124542236328\n",
            "Epoch [216/2500], Step [0/17], d_loss: 0.04845651984214783, g_loss: 4.530797004699707\n",
            "Epoch [217/2500], Step [0/17], d_loss: 0.054275888949632645, g_loss: 5.169704914093018\n",
            "Epoch [218/2500], Step [0/17], d_loss: 0.024142973124980927, g_loss: 5.551772117614746\n",
            "Epoch [219/2500], Step [0/17], d_loss: 0.05418678745627403, g_loss: 5.366057872772217\n",
            "Epoch [220/2500], Step [0/17], d_loss: 0.0466390885412693, g_loss: 5.16760778427124\n",
            "Epoch [221/2500], Step [0/17], d_loss: 0.033066630363464355, g_loss: 5.250665664672852\n",
            "Epoch [222/2500], Step [0/17], d_loss: 0.033382490277290344, g_loss: 5.293570518493652\n",
            "Epoch [223/2500], Step [0/17], d_loss: 0.04438776895403862, g_loss: 4.9777069091796875\n",
            "Epoch [224/2500], Step [0/17], d_loss: 0.06972531974315643, g_loss: 5.712979316711426\n",
            "Epoch [225/2500], Step [0/17], d_loss: 0.057892199605703354, g_loss: 5.615181922912598\n",
            "Epoch [226/2500], Step [0/17], d_loss: 0.05999097600579262, g_loss: 5.787921905517578\n",
            "Epoch [227/2500], Step [0/17], d_loss: 0.05554071441292763, g_loss: 5.799533843994141\n",
            "Epoch [228/2500], Step [0/17], d_loss: 0.09572261571884155, g_loss: 5.376760482788086\n",
            "Epoch [229/2500], Step [0/17], d_loss: 0.02447245456278324, g_loss: 5.336259841918945\n",
            "Epoch [230/2500], Step [0/17], d_loss: 0.041472017765045166, g_loss: 5.363602638244629\n",
            "Epoch [231/2500], Step [0/17], d_loss: 0.027538929134607315, g_loss: 5.880109786987305\n",
            "Epoch [232/2500], Step [0/17], d_loss: 0.0633510872721672, g_loss: 6.070001602172852\n",
            "Epoch [233/2500], Step [0/17], d_loss: 0.026331540197134018, g_loss: 5.427104473114014\n",
            "Epoch [234/2500], Step [0/17], d_loss: 2.402132987976074, g_loss: 3.736478328704834\n",
            "Epoch [235/2500], Step [0/17], d_loss: 0.612661600112915, g_loss: 5.7961626052856445\n",
            "Epoch [236/2500], Step [0/17], d_loss: 0.6055783629417419, g_loss: 6.829188346862793\n",
            "Epoch [237/2500], Step [0/17], d_loss: 0.37072646617889404, g_loss: 7.791346549987793\n",
            "Epoch [238/2500], Step [0/17], d_loss: 0.28943222761154175, g_loss: 6.257801055908203\n",
            "Epoch [239/2500], Step [0/17], d_loss: 0.16899323463439941, g_loss: 5.234516620635986\n",
            "Epoch [240/2500], Step [0/17], d_loss: 0.12782803177833557, g_loss: 5.685342788696289\n",
            "Epoch [241/2500], Step [0/17], d_loss: 0.177012637257576, g_loss: 6.3308634757995605\n",
            "Epoch [242/2500], Step [0/17], d_loss: 0.07728499919176102, g_loss: 5.76215934753418\n",
            "Epoch [243/2500], Step [0/17], d_loss: 0.11494645476341248, g_loss: 5.152045249938965\n",
            "Epoch [244/2500], Step [0/17], d_loss: 0.07340744137763977, g_loss: 5.001633167266846\n",
            "Epoch [245/2500], Step [0/17], d_loss: 0.09057138860225677, g_loss: 5.360955715179443\n",
            "Epoch [246/2500], Step [0/17], d_loss: 0.07838764786720276, g_loss: 4.9022064208984375\n",
            "Epoch [247/2500], Step [0/17], d_loss: 0.04776207357645035, g_loss: 4.784242153167725\n",
            "Epoch [248/2500], Step [0/17], d_loss: 0.04920688271522522, g_loss: 5.156917572021484\n",
            "Epoch [249/2500], Step [0/17], d_loss: 0.04546120762825012, g_loss: 4.930738925933838\n",
            "Epoch [250/2500], Step [0/17], d_loss: 0.04699180647730827, g_loss: 4.775545120239258\n",
            "Epoch [251/2500], Step [0/17], d_loss: 0.03753969445824623, g_loss: 5.338549613952637\n",
            "Epoch [252/2500], Step [0/17], d_loss: 0.06896528601646423, g_loss: 5.160133361816406\n",
            "Epoch [253/2500], Step [0/17], d_loss: 0.030720792710781097, g_loss: 5.533064842224121\n",
            "Epoch [254/2500], Step [0/17], d_loss: 0.03838052973151207, g_loss: 4.987389087677002\n",
            "Epoch [255/2500], Step [0/17], d_loss: 0.04026574641466141, g_loss: 4.934696674346924\n",
            "Epoch [256/2500], Step [0/17], d_loss: 0.04663576930761337, g_loss: 5.063571929931641\n",
            "Epoch [257/2500], Step [0/17], d_loss: 0.03901920095086098, g_loss: 5.2428083419799805\n",
            "Epoch [258/2500], Step [0/17], d_loss: 0.030036885291337967, g_loss: 5.220260143280029\n",
            "Epoch [259/2500], Step [0/17], d_loss: 0.03535047546029091, g_loss: 5.1820878982543945\n",
            "Epoch [260/2500], Step [0/17], d_loss: 0.028713349252939224, g_loss: 5.559256553649902\n",
            "Epoch [261/2500], Step [0/17], d_loss: 0.039219100028276443, g_loss: 4.9972991943359375\n",
            "Epoch [262/2500], Step [0/17], d_loss: 0.024111375212669373, g_loss: 5.069687366485596\n",
            "Epoch [263/2500], Step [0/17], d_loss: 0.02641225978732109, g_loss: 5.583295822143555\n",
            "Epoch [264/2500], Step [0/17], d_loss: 0.04048663750290871, g_loss: 5.2298903465271\n",
            "Epoch [265/2500], Step [0/17], d_loss: 0.021216247230768204, g_loss: 5.991704940795898\n",
            "Epoch [266/2500], Step [0/17], d_loss: 0.03538292646408081, g_loss: 5.104972839355469\n",
            "Epoch [267/2500], Step [0/17], d_loss: 0.0282699353992939, g_loss: 5.45413875579834\n",
            "Epoch [268/2500], Step [0/17], d_loss: 0.03512817621231079, g_loss: 5.052989482879639\n",
            "Epoch [269/2500], Step [0/17], d_loss: 0.018501006066799164, g_loss: 5.436975479125977\n",
            "Epoch [270/2500], Step [0/17], d_loss: 0.03558143973350525, g_loss: 5.8295698165893555\n",
            "Epoch [271/2500], Step [0/17], d_loss: 0.025669271126389503, g_loss: 5.386168956756592\n",
            "Epoch [272/2500], Step [0/17], d_loss: 0.025849081575870514, g_loss: 5.182554244995117\n",
            "Epoch [273/2500], Step [0/17], d_loss: 0.02899264171719551, g_loss: 5.321478843688965\n",
            "Epoch [274/2500], Step [0/17], d_loss: 0.02625831961631775, g_loss: 5.3068952560424805\n",
            "Epoch [275/2500], Step [0/17], d_loss: 0.03361429274082184, g_loss: 5.132770538330078\n",
            "Epoch [276/2500], Step [0/17], d_loss: 0.01959832012653351, g_loss: 5.5553460121154785\n",
            "Epoch [277/2500], Step [0/17], d_loss: 0.0395759642124176, g_loss: 5.863828182220459\n",
            "Epoch [278/2500], Step [0/17], d_loss: 0.02051585167646408, g_loss: 5.536585330963135\n",
            "Epoch [279/2500], Step [0/17], d_loss: 0.018534719944000244, g_loss: 5.886412143707275\n",
            "Epoch [280/2500], Step [0/17], d_loss: 0.03675878047943115, g_loss: 5.774439811706543\n",
            "Epoch [281/2500], Step [0/17], d_loss: 0.01993730664253235, g_loss: 5.785393238067627\n",
            "Epoch [282/2500], Step [0/17], d_loss: 0.043955426663160324, g_loss: 6.412021636962891\n",
            "Epoch [283/2500], Step [0/17], d_loss: 0.020657898858189583, g_loss: 5.401522636413574\n",
            "Epoch [284/2500], Step [0/17], d_loss: 0.01520983874797821, g_loss: 6.119036674499512\n",
            "Epoch [285/2500], Step [0/17], d_loss: 0.024182360619306564, g_loss: 5.306460380554199\n",
            "Epoch [286/2500], Step [0/17], d_loss: 0.01951664686203003, g_loss: 5.420324802398682\n",
            "Epoch [287/2500], Step [0/17], d_loss: 0.01686711609363556, g_loss: 5.847917079925537\n",
            "Epoch [288/2500], Step [0/17], d_loss: 0.019067294895648956, g_loss: 5.259139060974121\n",
            "Epoch [289/2500], Step [0/17], d_loss: 0.02495187148451805, g_loss: 5.867706298828125\n",
            "Epoch [290/2500], Step [0/17], d_loss: 0.032969504594802856, g_loss: 5.902249336242676\n",
            "Epoch [291/2500], Step [0/17], d_loss: 0.012643968686461449, g_loss: 6.003630638122559\n",
            "Epoch [292/2500], Step [0/17], d_loss: 0.014153851196169853, g_loss: 5.809502601623535\n",
            "Epoch [293/2500], Step [0/17], d_loss: 0.016597572714090347, g_loss: 6.218500137329102\n",
            "Epoch [294/2500], Step [0/17], d_loss: 0.0114122424274683, g_loss: 5.858198165893555\n",
            "Epoch [295/2500], Step [0/17], d_loss: 0.017749827355146408, g_loss: 5.801046371459961\n",
            "Epoch [296/2500], Step [0/17], d_loss: 0.01634262315928936, g_loss: 6.438117027282715\n",
            "Epoch [297/2500], Step [0/17], d_loss: 0.01318642869591713, g_loss: 6.110363006591797\n",
            "Epoch [298/2500], Step [0/17], d_loss: 0.020928198471665382, g_loss: 5.739980697631836\n",
            "Epoch [299/2500], Step [0/17], d_loss: 0.017685692757368088, g_loss: 6.083765983581543\n",
            "Epoch [300/2500], Step [0/17], d_loss: 0.014976773411035538, g_loss: 5.767120361328125\n",
            "Epoch [301/2500], Step [0/17], d_loss: 0.013308729976415634, g_loss: 6.302143096923828\n",
            "Epoch [302/2500], Step [0/17], d_loss: 0.02197135239839554, g_loss: 5.762837886810303\n",
            "Epoch [303/2500], Step [0/17], d_loss: 0.014232881367206573, g_loss: 5.6219587326049805\n",
            "Epoch [304/2500], Step [0/17], d_loss: 0.016916394233703613, g_loss: 6.330135822296143\n",
            "Epoch [305/2500], Step [0/17], d_loss: 0.008515283465385437, g_loss: 5.635190963745117\n",
            "Epoch [306/2500], Step [0/17], d_loss: 0.011234434321522713, g_loss: 6.299615859985352\n",
            "Epoch [307/2500], Step [0/17], d_loss: 3.8778393268585205, g_loss: 2.689725160598755\n",
            "Epoch [308/2500], Step [0/17], d_loss: 1.5098369121551514, g_loss: 1.283247709274292\n",
            "Epoch [309/2500], Step [0/17], d_loss: 1.5865414142608643, g_loss: 4.662583351135254\n",
            "Epoch [310/2500], Step [0/17], d_loss: 0.7286754846572876, g_loss: 1.9995481967926025\n",
            "Epoch [311/2500], Step [0/17], d_loss: 2.05702543258667, g_loss: 5.523864269256592\n",
            "Epoch [312/2500], Step [0/17], d_loss: 1.6283797025680542, g_loss: 3.555504322052002\n",
            "Epoch [313/2500], Step [0/17], d_loss: 0.6781891584396362, g_loss: 5.737427711486816\n",
            "Epoch [314/2500], Step [0/17], d_loss: 1.2192726135253906, g_loss: 10.800386428833008\n",
            "Epoch [315/2500], Step [0/17], d_loss: 0.2211211621761322, g_loss: 6.429679870605469\n",
            "Epoch [316/2500], Step [0/17], d_loss: 0.6690833568572998, g_loss: 7.583958625793457\n",
            "Epoch [317/2500], Step [0/17], d_loss: 0.28161078691482544, g_loss: 6.677036762237549\n",
            "Epoch [318/2500], Step [0/17], d_loss: 0.3603939712047577, g_loss: 7.639341831207275\n",
            "Epoch [319/2500], Step [0/17], d_loss: 0.4536570906639099, g_loss: 4.9861836433410645\n",
            "Epoch [320/2500], Step [0/17], d_loss: 0.2465883046388626, g_loss: 7.114824295043945\n",
            "Epoch [321/2500], Step [0/17], d_loss: 0.26387107372283936, g_loss: 8.200212478637695\n",
            "Epoch [322/2500], Step [0/17], d_loss: 0.15012763440608978, g_loss: 5.772202491760254\n",
            "Epoch [323/2500], Step [0/17], d_loss: 0.18989905714988708, g_loss: 7.392788887023926\n",
            "Epoch [324/2500], Step [0/17], d_loss: 0.18373949825763702, g_loss: 7.225977420806885\n",
            "Epoch [325/2500], Step [0/17], d_loss: 0.08611571788787842, g_loss: 6.229728698730469\n",
            "Epoch [326/2500], Step [0/17], d_loss: 0.06930948793888092, g_loss: 5.624600410461426\n",
            "Epoch [327/2500], Step [0/17], d_loss: 0.07024993002414703, g_loss: 5.171185493469238\n",
            "Epoch [328/2500], Step [0/17], d_loss: 0.12083344906568527, g_loss: 6.13677978515625\n",
            "Epoch [329/2500], Step [0/17], d_loss: 0.059555042535066605, g_loss: 5.2936859130859375\n",
            "Epoch [330/2500], Step [0/17], d_loss: 0.08871951699256897, g_loss: 5.934783458709717\n",
            "Epoch [331/2500], Step [0/17], d_loss: 0.10645604133605957, g_loss: 6.246016502380371\n",
            "Epoch [332/2500], Step [0/17], d_loss: 0.05608607828617096, g_loss: 5.398519515991211\n",
            "Epoch [333/2500], Step [0/17], d_loss: 0.05287669599056244, g_loss: 5.337904930114746\n",
            "Epoch [334/2500], Step [0/17], d_loss: 0.045057378709316254, g_loss: 5.5152130126953125\n",
            "Epoch [335/2500], Step [0/17], d_loss: 0.04074603691697121, g_loss: 5.574165344238281\n",
            "Epoch [336/2500], Step [0/17], d_loss: 0.04283416271209717, g_loss: 5.472572326660156\n",
            "Epoch [337/2500], Step [0/17], d_loss: 0.0475195050239563, g_loss: 5.651623725891113\n",
            "Epoch [338/2500], Step [0/17], d_loss: 0.040390726178884506, g_loss: 5.238623142242432\n",
            "Epoch [339/2500], Step [0/17], d_loss: 0.03760215640068054, g_loss: 5.507935523986816\n",
            "Epoch [340/2500], Step [0/17], d_loss: 0.03430280089378357, g_loss: 5.316900253295898\n",
            "Epoch [341/2500], Step [0/17], d_loss: 0.06526794284582138, g_loss: 5.276579856872559\n",
            "Epoch [342/2500], Step [0/17], d_loss: 0.04237746447324753, g_loss: 5.384195804595947\n",
            "Epoch [343/2500], Step [0/17], d_loss: 0.03164830058813095, g_loss: 5.2318549156188965\n",
            "Epoch [344/2500], Step [0/17], d_loss: 0.03357846289873123, g_loss: 5.834373474121094\n",
            "Epoch [345/2500], Step [0/17], d_loss: 0.04727713018655777, g_loss: 5.1414313316345215\n",
            "Epoch [346/2500], Step [0/17], d_loss: 0.02923819050192833, g_loss: 5.139708995819092\n",
            "Epoch [347/2500], Step [0/17], d_loss: 0.03530975058674812, g_loss: 5.322826385498047\n",
            "Epoch [348/2500], Step [0/17], d_loss: 0.04501042515039444, g_loss: 5.7531585693359375\n",
            "Epoch [349/2500], Step [0/17], d_loss: 0.02990119531750679, g_loss: 5.601415157318115\n",
            "Epoch [350/2500], Step [0/17], d_loss: 0.02312682382762432, g_loss: 5.7885332107543945\n",
            "Epoch [351/2500], Step [0/17], d_loss: 0.023189114406704903, g_loss: 5.402461051940918\n",
            "Epoch [352/2500], Step [0/17], d_loss: 0.025622352957725525, g_loss: 5.451786518096924\n",
            "Epoch [353/2500], Step [0/17], d_loss: 0.025272507220506668, g_loss: 5.724023818969727\n",
            "Epoch [354/2500], Step [0/17], d_loss: 0.023389965295791626, g_loss: 5.3250508308410645\n",
            "Epoch [355/2500], Step [0/17], d_loss: 0.025282055139541626, g_loss: 5.324813365936279\n",
            "Epoch [356/2500], Step [0/17], d_loss: 0.023293975740671158, g_loss: 5.421107292175293\n",
            "Epoch [357/2500], Step [0/17], d_loss: 0.02850666455924511, g_loss: 5.144740104675293\n",
            "Epoch [358/2500], Step [0/17], d_loss: 0.017453230917453766, g_loss: 5.8331804275512695\n",
            "Epoch [359/2500], Step [0/17], d_loss: 0.024906165897846222, g_loss: 5.692981243133545\n",
            "Epoch [360/2500], Step [0/17], d_loss: 0.021968265995383263, g_loss: 5.611715316772461\n",
            "Epoch [361/2500], Step [0/17], d_loss: 0.023772630840539932, g_loss: 5.895459175109863\n",
            "Epoch [362/2500], Step [0/17], d_loss: 0.022317824885249138, g_loss: 5.306416988372803\n",
            "Epoch [363/2500], Step [0/17], d_loss: 0.026711180806159973, g_loss: 5.047303199768066\n",
            "Epoch [364/2500], Step [0/17], d_loss: 0.022752059623599052, g_loss: 5.592514991760254\n",
            "Epoch [365/2500], Step [0/17], d_loss: 0.013984003104269505, g_loss: 6.055995941162109\n",
            "Epoch [366/2500], Step [0/17], d_loss: 0.02054399624466896, g_loss: 5.867000579833984\n",
            "Epoch [367/2500], Step [0/17], d_loss: 0.014495030045509338, g_loss: 5.706562042236328\n",
            "Epoch [368/2500], Step [0/17], d_loss: 0.015493495389819145, g_loss: 5.644319534301758\n",
            "Epoch [369/2500], Step [0/17], d_loss: 0.024123217910528183, g_loss: 5.77724552154541\n",
            "Epoch [370/2500], Step [0/17], d_loss: 0.01413355115801096, g_loss: 5.653831481933594\n",
            "Epoch [371/2500], Step [0/17], d_loss: 0.010897066444158554, g_loss: 5.829720973968506\n",
            "Epoch [372/2500], Step [0/17], d_loss: 0.011876160278916359, g_loss: 6.370091438293457\n",
            "Epoch [373/2500], Step [0/17], d_loss: 0.013836745172739029, g_loss: 6.045733451843262\n",
            "Epoch [374/2500], Step [0/17], d_loss: 0.020527441054582596, g_loss: 5.579051971435547\n",
            "Epoch [375/2500], Step [0/17], d_loss: 0.02115480974316597, g_loss: 5.973861217498779\n",
            "Epoch [376/2500], Step [0/17], d_loss: 0.025760812684893608, g_loss: 6.004514217376709\n",
            "Epoch [377/2500], Step [0/17], d_loss: 0.013103986158967018, g_loss: 5.796175003051758\n",
            "Epoch [378/2500], Step [0/17], d_loss: 0.011205952614545822, g_loss: 6.074000358581543\n",
            "Epoch [379/2500], Step [0/17], d_loss: 0.009119401685893536, g_loss: 6.317098617553711\n",
            "Epoch [380/2500], Step [0/17], d_loss: 0.01630261167883873, g_loss: 5.958681106567383\n",
            "Epoch [381/2500], Step [0/17], d_loss: 0.024160537868738174, g_loss: 6.079434394836426\n",
            "Epoch [382/2500], Step [0/17], d_loss: 0.010678268037736416, g_loss: 6.698092937469482\n",
            "Epoch [383/2500], Step [0/17], d_loss: 0.011275839060544968, g_loss: 5.802789688110352\n",
            "Epoch [384/2500], Step [0/17], d_loss: 0.014567692764103413, g_loss: 5.832325458526611\n",
            "Epoch [385/2500], Step [0/17], d_loss: 0.013478944078087807, g_loss: 5.992368221282959\n",
            "Epoch [386/2500], Step [0/17], d_loss: 0.022205322980880737, g_loss: 5.71937894821167\n",
            "Epoch [387/2500], Step [0/17], d_loss: 0.014585135504603386, g_loss: 6.189984321594238\n",
            "Epoch [388/2500], Step [0/17], d_loss: 0.02196391485631466, g_loss: 6.5525970458984375\n",
            "Epoch [389/2500], Step [0/17], d_loss: 0.017650635913014412, g_loss: 6.57014274597168\n",
            "Epoch [390/2500], Step [0/17], d_loss: 0.010698330588638783, g_loss: 6.32487678527832\n",
            "Epoch [391/2500], Step [0/17], d_loss: 0.02212546579539776, g_loss: 6.614633560180664\n",
            "Epoch [392/2500], Step [0/17], d_loss: 0.00733020156621933, g_loss: 6.601543426513672\n",
            "Epoch [393/2500], Step [0/17], d_loss: 0.012586141005158424, g_loss: 6.068487644195557\n",
            "Epoch [394/2500], Step [0/17], d_loss: 0.007813448086380959, g_loss: 6.506524562835693\n",
            "Epoch [395/2500], Step [0/17], d_loss: 0.006583837326616049, g_loss: 6.460378646850586\n",
            "Epoch [396/2500], Step [0/17], d_loss: 0.00892312079668045, g_loss: 6.68011474609375\n",
            "Epoch [397/2500], Step [0/17], d_loss: 0.011999363079667091, g_loss: 7.068381309509277\n",
            "Epoch [398/2500], Step [0/17], d_loss: 0.016841232776641846, g_loss: 6.20028829574585\n",
            "Epoch [399/2500], Step [0/17], d_loss: 0.010228358209133148, g_loss: 6.4921464920043945\n",
            "Epoch [400/2500], Step [0/17], d_loss: 0.008168823085725307, g_loss: 6.418300628662109\n",
            "Epoch [401/2500], Step [0/17], d_loss: 0.018606502562761307, g_loss: 6.554009437561035\n",
            "Epoch [402/2500], Step [0/17], d_loss: 0.006576607935130596, g_loss: 6.7923383712768555\n",
            "Epoch [403/2500], Step [0/17], d_loss: 0.006971349939703941, g_loss: 6.616297245025635\n",
            "Epoch [404/2500], Step [0/17], d_loss: 0.01293165422976017, g_loss: 6.31392765045166\n",
            "Epoch [405/2500], Step [0/17], d_loss: 0.007274826988577843, g_loss: 6.760552406311035\n",
            "Epoch [406/2500], Step [0/17], d_loss: 0.008078962564468384, g_loss: 6.226311683654785\n",
            "Epoch [407/2500], Step [0/17], d_loss: 0.0055488962680101395, g_loss: 6.759598255157471\n",
            "Epoch [408/2500], Step [0/17], d_loss: 0.00848367903381586, g_loss: 6.360726356506348\n",
            "Epoch [409/2500], Step [0/17], d_loss: 0.007479125168174505, g_loss: 6.587843418121338\n",
            "Epoch [410/2500], Step [0/17], d_loss: 0.008836009539663792, g_loss: 6.56776237487793\n",
            "Epoch [411/2500], Step [0/17], d_loss: 0.011873182840645313, g_loss: 6.439783573150635\n",
            "Epoch [412/2500], Step [0/17], d_loss: 0.009271003305912018, g_loss: 6.269596576690674\n",
            "Epoch [413/2500], Step [0/17], d_loss: 0.008475535549223423, g_loss: 6.825834274291992\n",
            "Epoch [414/2500], Step [0/17], d_loss: 0.005855311639606953, g_loss: 6.614169120788574\n",
            "Epoch [415/2500], Step [0/17], d_loss: 0.008302412927150726, g_loss: 6.693511962890625\n",
            "Epoch [416/2500], Step [0/17], d_loss: 0.0066331056877970695, g_loss: 6.693935394287109\n",
            "Epoch [417/2500], Step [0/17], d_loss: 0.007390875834971666, g_loss: 6.59261417388916\n",
            "Epoch [418/2500], Step [0/17], d_loss: 0.006822047755122185, g_loss: 6.566053867340088\n",
            "Epoch [419/2500], Step [0/17], d_loss: 0.00798867642879486, g_loss: 6.248787879943848\n",
            "Epoch [420/2500], Step [0/17], d_loss: 0.006013275124132633, g_loss: 6.652561187744141\n",
            "Epoch [421/2500], Step [0/17], d_loss: 0.008428050205111504, g_loss: 6.144141674041748\n",
            "Epoch [422/2500], Step [0/17], d_loss: 0.00594849418848753, g_loss: 6.413593292236328\n",
            "Epoch [423/2500], Step [0/17], d_loss: 0.009476442821323872, g_loss: 6.522876262664795\n",
            "Epoch [424/2500], Step [0/17], d_loss: 0.006156955845654011, g_loss: 6.583808898925781\n",
            "Epoch [425/2500], Step [0/17], d_loss: 0.010136360302567482, g_loss: 6.468906879425049\n",
            "Epoch [426/2500], Step [0/17], d_loss: 0.005200720392167568, g_loss: 6.715548515319824\n",
            "Epoch [427/2500], Step [0/17], d_loss: 0.006490964442491531, g_loss: 6.66403865814209\n",
            "Epoch [428/2500], Step [0/17], d_loss: 0.002669653855264187, g_loss: 7.419961929321289\n",
            "Epoch [429/2500], Step [0/17], d_loss: 0.003839914221316576, g_loss: 6.988332748413086\n",
            "Epoch [430/2500], Step [0/17], d_loss: 0.01095945667475462, g_loss: 6.424853324890137\n",
            "Epoch [431/2500], Step [0/17], d_loss: 0.005412330850958824, g_loss: 6.9718403816223145\n",
            "Epoch [432/2500], Step [0/17], d_loss: 8.453865051269531, g_loss: 1.4311540126800537\n",
            "Epoch [433/2500], Step [0/17], d_loss: 1.0892338752746582, g_loss: 2.8176870346069336\n",
            "Epoch [434/2500], Step [0/17], d_loss: 0.9857306480407715, g_loss: 5.0386128425598145\n",
            "Epoch [435/2500], Step [0/17], d_loss: 1.816586971282959, g_loss: 9.58210563659668\n",
            "Epoch [436/2500], Step [0/17], d_loss: 1.5263479948043823, g_loss: 6.1363654136657715\n",
            "Epoch [437/2500], Step [0/17], d_loss: 0.39549198746681213, g_loss: 5.643497467041016\n",
            "Epoch [438/2500], Step [0/17], d_loss: 1.9211117029190063, g_loss: 6.564762115478516\n",
            "Epoch [439/2500], Step [0/17], d_loss: 1.0135900974273682, g_loss: 9.537720680236816\n",
            "Epoch [440/2500], Step [0/17], d_loss: 0.4178372025489807, g_loss: 6.378482818603516\n",
            "Epoch [441/2500], Step [0/17], d_loss: 0.28421124815940857, g_loss: 6.3596906661987305\n",
            "Epoch [442/2500], Step [0/17], d_loss: 0.6533195376396179, g_loss: 10.283039093017578\n",
            "Epoch [443/2500], Step [0/17], d_loss: 0.8587327599525452, g_loss: 9.02167797088623\n",
            "Epoch [444/2500], Step [0/17], d_loss: 0.2337220311164856, g_loss: 6.489062309265137\n",
            "Epoch [445/2500], Step [0/17], d_loss: 0.266150563955307, g_loss: 7.225090980529785\n",
            "Epoch [446/2500], Step [0/17], d_loss: 0.2717338502407074, g_loss: 8.280040740966797\n",
            "Epoch [447/2500], Step [0/17], d_loss: 0.14054587483406067, g_loss: 7.117964744567871\n",
            "Epoch [448/2500], Step [0/17], d_loss: 0.2655394673347473, g_loss: 7.817429542541504\n",
            "Epoch [449/2500], Step [0/17], d_loss: 0.24966192245483398, g_loss: 6.683366775512695\n",
            "Epoch [450/2500], Step [0/17], d_loss: 0.13504712283611298, g_loss: 6.159588813781738\n",
            "Epoch [451/2500], Step [0/17], d_loss: 0.1743970811367035, g_loss: 5.978837966918945\n",
            "Epoch [452/2500], Step [0/17], d_loss: 0.2091991901397705, g_loss: 6.570323467254639\n",
            "Epoch [453/2500], Step [0/17], d_loss: 0.13628384470939636, g_loss: 6.224732398986816\n",
            "Epoch [454/2500], Step [0/17], d_loss: 0.17039936780929565, g_loss: 5.59208869934082\n",
            "Epoch [455/2500], Step [0/17], d_loss: 0.17698422074317932, g_loss: 6.953537940979004\n",
            "Epoch [456/2500], Step [0/17], d_loss: 0.10989323258399963, g_loss: 6.555527687072754\n",
            "Epoch [457/2500], Step [0/17], d_loss: 0.1726357489824295, g_loss: 7.890628337860107\n",
            "Epoch [458/2500], Step [0/17], d_loss: 0.10255851596593857, g_loss: 5.573535919189453\n",
            "Epoch [459/2500], Step [0/17], d_loss: 0.08472999930381775, g_loss: 6.359307289123535\n",
            "Epoch [460/2500], Step [0/17], d_loss: 0.12002872675657272, g_loss: 6.945534706115723\n",
            "Epoch [461/2500], Step [0/17], d_loss: 0.08318772912025452, g_loss: 5.506616115570068\n",
            "Epoch [462/2500], Step [0/17], d_loss: 0.09894061088562012, g_loss: 5.7150139808654785\n",
            "Epoch [463/2500], Step [0/17], d_loss: 0.04786919802427292, g_loss: 5.863112449645996\n",
            "Epoch [464/2500], Step [0/17], d_loss: 0.056489113718271255, g_loss: 5.776177883148193\n",
            "Epoch [465/2500], Step [0/17], d_loss: 0.052042990922927856, g_loss: 5.754881858825684\n",
            "Epoch [466/2500], Step [0/17], d_loss: 0.06000588834285736, g_loss: 5.798180103302002\n",
            "Epoch [467/2500], Step [0/17], d_loss: 0.061999958008527756, g_loss: 5.651772499084473\n",
            "Epoch [468/2500], Step [0/17], d_loss: 0.06384102255105972, g_loss: 5.3165740966796875\n",
            "Epoch [469/2500], Step [0/17], d_loss: 0.04197148606181145, g_loss: 5.411429405212402\n",
            "Epoch [470/2500], Step [0/17], d_loss: 0.07908117771148682, g_loss: 5.192621231079102\n",
            "Epoch [471/2500], Step [0/17], d_loss: 0.0453394278883934, g_loss: 5.201282978057861\n",
            "Epoch [472/2500], Step [0/17], d_loss: 0.04167703539133072, g_loss: 6.08116340637207\n",
            "Epoch [473/2500], Step [0/17], d_loss: 0.07512669265270233, g_loss: 5.83582878112793\n",
            "Epoch [474/2500], Step [0/17], d_loss: 0.03231050446629524, g_loss: 5.261083602905273\n",
            "Epoch [475/2500], Step [0/17], d_loss: 0.024990171194076538, g_loss: 5.813394546508789\n",
            "Epoch [476/2500], Step [0/17], d_loss: 0.02622065506875515, g_loss: 6.0317792892456055\n",
            "Epoch [477/2500], Step [0/17], d_loss: 0.0450555682182312, g_loss: 5.403815269470215\n",
            "Epoch [478/2500], Step [0/17], d_loss: 0.04311851039528847, g_loss: 5.843373775482178\n",
            "Epoch [479/2500], Step [0/17], d_loss: 0.032976143062114716, g_loss: 5.754693984985352\n",
            "Epoch [480/2500], Step [0/17], d_loss: 0.02593264728784561, g_loss: 5.77712345123291\n",
            "Epoch [481/2500], Step [0/17], d_loss: 0.02376333624124527, g_loss: 5.622274398803711\n",
            "Epoch [482/2500], Step [0/17], d_loss: 0.04373463615775108, g_loss: 5.33667516708374\n",
            "Epoch [483/2500], Step [0/17], d_loss: 0.038965772837400436, g_loss: 5.29477596282959\n",
            "Epoch [484/2500], Step [0/17], d_loss: 0.028059033676981926, g_loss: 5.336040496826172\n",
            "Epoch [485/2500], Step [0/17], d_loss: 0.029245350509881973, g_loss: 6.005953788757324\n",
            "Epoch [486/2500], Step [0/17], d_loss: 0.02677677944302559, g_loss: 5.887360095977783\n",
            "Epoch [487/2500], Step [0/17], d_loss: 0.027776747941970825, g_loss: 5.243971824645996\n",
            "Epoch [488/2500], Step [0/17], d_loss: 0.02537626400589943, g_loss: 5.752572059631348\n",
            "Epoch [489/2500], Step [0/17], d_loss: 0.023213768377900124, g_loss: 5.82514762878418\n",
            "Epoch [490/2500], Step [0/17], d_loss: 0.026400208473205566, g_loss: 5.9031829833984375\n",
            "Epoch [491/2500], Step [0/17], d_loss: 0.020816529169678688, g_loss: 5.635946273803711\n",
            "Epoch [492/2500], Step [0/17], d_loss: 0.01849912852048874, g_loss: 5.857402324676514\n",
            "Epoch [493/2500], Step [0/17], d_loss: 0.028298908844590187, g_loss: 5.6339240074157715\n",
            "Epoch [494/2500], Step [0/17], d_loss: 0.024038711562752724, g_loss: 5.615642547607422\n",
            "Epoch [495/2500], Step [0/17], d_loss: 0.025068942457437515, g_loss: 5.734478950500488\n",
            "Epoch [496/2500], Step [0/17], d_loss: 0.02345418557524681, g_loss: 5.381959438323975\n",
            "Epoch [497/2500], Step [0/17], d_loss: 0.02169310674071312, g_loss: 5.783970832824707\n",
            "Epoch [498/2500], Step [0/17], d_loss: 0.019594278186559677, g_loss: 5.647029876708984\n",
            "Epoch [499/2500], Step [0/17], d_loss: 0.02918413281440735, g_loss: 5.487767219543457\n",
            "Epoch [500/2500], Step [0/17], d_loss: 0.0224461629986763, g_loss: 5.719557762145996\n",
            "Epoch [501/2500], Step [0/17], d_loss: 0.01820673793554306, g_loss: 5.970767974853516\n",
            "Epoch [502/2500], Step [0/17], d_loss: 0.01772131212055683, g_loss: 5.810023784637451\n",
            "Epoch [503/2500], Step [0/17], d_loss: 0.01793086901307106, g_loss: 5.896144866943359\n",
            "Epoch [504/2500], Step [0/17], d_loss: 0.014665357768535614, g_loss: 5.718202590942383\n",
            "Epoch [505/2500], Step [0/17], d_loss: 0.012754891067743301, g_loss: 5.821628570556641\n",
            "Epoch [506/2500], Step [0/17], d_loss: 0.021447625011205673, g_loss: 5.680429458618164\n",
            "Epoch [507/2500], Step [0/17], d_loss: 0.013578345067799091, g_loss: 5.6198410987854\n",
            "Epoch [508/2500], Step [0/17], d_loss: 0.01140567660331726, g_loss: 6.096077919006348\n",
            "Epoch [509/2500], Step [0/17], d_loss: 0.012133399955928326, g_loss: 5.972128868103027\n",
            "Epoch [510/2500], Step [0/17], d_loss: 0.014779536053538322, g_loss: 5.702204704284668\n",
            "Epoch [511/2500], Step [0/17], d_loss: 0.01303313858807087, g_loss: 5.679985046386719\n",
            "Epoch [512/2500], Step [0/17], d_loss: 0.014697430655360222, g_loss: 5.8512983322143555\n",
            "Epoch [513/2500], Step [0/17], d_loss: 0.015002099797129631, g_loss: 6.119402885437012\n",
            "Epoch [514/2500], Step [0/17], d_loss: 0.028761832043528557, g_loss: 5.4501752853393555\n",
            "Epoch [515/2500], Step [0/17], d_loss: 0.01624419167637825, g_loss: 5.947847366333008\n",
            "Epoch [516/2500], Step [0/17], d_loss: 0.017199605703353882, g_loss: 5.743063926696777\n",
            "Epoch [517/2500], Step [0/17], d_loss: 0.012976077385246754, g_loss: 6.223149299621582\n",
            "Epoch [518/2500], Step [0/17], d_loss: 0.0076509080827236176, g_loss: 6.4210381507873535\n",
            "Epoch [519/2500], Step [0/17], d_loss: 0.015168741345405579, g_loss: 5.672112464904785\n",
            "Epoch [520/2500], Step [0/17], d_loss: 0.010019698180258274, g_loss: 6.156970977783203\n",
            "Epoch [521/2500], Step [0/17], d_loss: 0.015142330899834633, g_loss: 5.7750091552734375\n",
            "Epoch [522/2500], Step [0/17], d_loss: 0.017211489379405975, g_loss: 5.867892742156982\n",
            "Epoch [523/2500], Step [0/17], d_loss: 0.012087020091712475, g_loss: 6.313572883605957\n",
            "Epoch [524/2500], Step [0/17], d_loss: 0.012547576799988747, g_loss: 5.788812637329102\n",
            "Epoch [525/2500], Step [0/17], d_loss: 0.010592436417937279, g_loss: 5.872215747833252\n",
            "Epoch [526/2500], Step [0/17], d_loss: 0.011372387409210205, g_loss: 5.959359169006348\n",
            "Epoch [527/2500], Step [0/17], d_loss: 0.009018748998641968, g_loss: 5.971399307250977\n",
            "Epoch [528/2500], Step [0/17], d_loss: 0.015348506160080433, g_loss: 6.332643032073975\n",
            "Epoch [529/2500], Step [0/17], d_loss: 0.013932375237345695, g_loss: 5.819074630737305\n",
            "Epoch [530/2500], Step [0/17], d_loss: 0.011559564620256424, g_loss: 6.110559463500977\n",
            "Epoch [531/2500], Step [0/17], d_loss: 0.008363539353013039, g_loss: 6.4143476486206055\n",
            "Epoch [532/2500], Step [0/17], d_loss: 0.012618450447916985, g_loss: 5.748827934265137\n",
            "Epoch [533/2500], Step [0/17], d_loss: 0.013108620420098305, g_loss: 6.409597396850586\n",
            "Epoch [534/2500], Step [0/17], d_loss: 0.010590774938464165, g_loss: 6.179530143737793\n",
            "Epoch [535/2500], Step [0/17], d_loss: 0.008740554563701153, g_loss: 6.394223213195801\n",
            "Epoch [536/2500], Step [0/17], d_loss: 0.007906416431069374, g_loss: 6.348934173583984\n",
            "Epoch [537/2500], Step [0/17], d_loss: 0.010956810787320137, g_loss: 6.745388031005859\n",
            "Epoch [538/2500], Step [0/17], d_loss: 0.009874684736132622, g_loss: 6.175722122192383\n",
            "Epoch [539/2500], Step [0/17], d_loss: 0.008038087747991085, g_loss: 6.3898234367370605\n",
            "Epoch [540/2500], Step [0/17], d_loss: 0.007154369261115789, g_loss: 6.500787734985352\n",
            "Epoch [541/2500], Step [0/17], d_loss: 0.012936328537762165, g_loss: 5.725417137145996\n",
            "Epoch [542/2500], Step [0/17], d_loss: 0.008029034361243248, g_loss: 6.441503047943115\n",
            "Epoch [543/2500], Step [0/17], d_loss: 0.010300535708665848, g_loss: 6.907564163208008\n",
            "Epoch [544/2500], Step [0/17], d_loss: 0.010968906804919243, g_loss: 6.17081880569458\n",
            "Epoch [545/2500], Step [0/17], d_loss: 0.008418073877692223, g_loss: 6.3009867668151855\n",
            "Epoch [546/2500], Step [0/17], d_loss: 0.011168577708303928, g_loss: 6.671340465545654\n",
            "Epoch [547/2500], Step [0/17], d_loss: 0.0065808785147964954, g_loss: 7.174197196960449\n",
            "Epoch [548/2500], Step [0/17], d_loss: 0.007609665393829346, g_loss: 6.630178928375244\n",
            "Epoch [549/2500], Step [0/17], d_loss: 0.007061601150780916, g_loss: 7.106947898864746\n",
            "Epoch [550/2500], Step [0/17], d_loss: 0.009430693462491035, g_loss: 6.626828670501709\n",
            "Epoch [551/2500], Step [0/17], d_loss: 0.009428376331925392, g_loss: 6.459649085998535\n",
            "Epoch [552/2500], Step [0/17], d_loss: 0.012555545195937157, g_loss: 6.709185600280762\n",
            "Epoch [553/2500], Step [0/17], d_loss: 0.00432850606739521, g_loss: 6.838796138763428\n",
            "Epoch [554/2500], Step [0/17], d_loss: 0.008418940007686615, g_loss: 6.301804542541504\n",
            "Epoch [555/2500], Step [0/17], d_loss: 0.006560780107975006, g_loss: 6.7261505126953125\n",
            "Epoch [556/2500], Step [0/17], d_loss: 0.009401446208357811, g_loss: 6.410342216491699\n",
            "Epoch [557/2500], Step [0/17], d_loss: 0.006886329501867294, g_loss: 6.640685081481934\n",
            "Epoch [558/2500], Step [0/17], d_loss: 0.008324374444782734, g_loss: 6.3204522132873535\n",
            "Epoch [559/2500], Step [0/17], d_loss: 0.005482401233166456, g_loss: 6.613199234008789\n",
            "Epoch [560/2500], Step [0/17], d_loss: 0.0033277268521487713, g_loss: 7.218561172485352\n",
            "Epoch [561/2500], Step [0/17], d_loss: 0.004925009794533253, g_loss: 6.983797073364258\n",
            "Epoch [562/2500], Step [0/17], d_loss: 0.004880974534898996, g_loss: 6.851483345031738\n",
            "Epoch [563/2500], Step [0/17], d_loss: 0.008296862244606018, g_loss: 6.587306499481201\n",
            "Epoch [564/2500], Step [0/17], d_loss: 0.006527082994580269, g_loss: 6.882404327392578\n",
            "Epoch [565/2500], Step [0/17], d_loss: 0.005070343613624573, g_loss: 6.682627201080322\n",
            "Epoch [566/2500], Step [0/17], d_loss: 0.010427202098071575, g_loss: 7.147367000579834\n",
            "Epoch [567/2500], Step [0/17], d_loss: 0.003463499015197158, g_loss: 7.170271396636963\n",
            "Epoch [568/2500], Step [0/17], d_loss: 0.006039733532816172, g_loss: 5.8296308517456055\n",
            "Epoch [569/2500], Step [0/17], d_loss: 0.00430265162140131, g_loss: 6.875880241394043\n",
            "Epoch [570/2500], Step [0/17], d_loss: 0.00797320157289505, g_loss: 6.275546550750732\n",
            "Epoch [571/2500], Step [0/17], d_loss: 0.008399219252169132, g_loss: 6.469333648681641\n",
            "Epoch [572/2500], Step [0/17], d_loss: 0.005766683258116245, g_loss: 6.9911675453186035\n",
            "Epoch [573/2500], Step [0/17], d_loss: 0.00490811001509428, g_loss: 6.962248802185059\n",
            "Epoch [574/2500], Step [0/17], d_loss: 0.0037077374290674925, g_loss: 7.111875534057617\n",
            "Epoch [575/2500], Step [0/17], d_loss: 0.004614640027284622, g_loss: 6.702063083648682\n",
            "Epoch [576/2500], Step [0/17], d_loss: 0.0055915359407663345, g_loss: 6.964113235473633\n",
            "Epoch [577/2500], Step [0/17], d_loss: 0.004294395912438631, g_loss: 7.054365158081055\n",
            "Epoch [578/2500], Step [0/17], d_loss: 0.003549759741872549, g_loss: 7.264796733856201\n",
            "Epoch [579/2500], Step [0/17], d_loss: 0.005142077337950468, g_loss: 6.895215034484863\n",
            "Epoch [580/2500], Step [0/17], d_loss: 0.00374867906793952, g_loss: 6.936525344848633\n",
            "Epoch [581/2500], Step [0/17], d_loss: 0.004244631621986628, g_loss: 7.238375663757324\n",
            "Epoch [582/2500], Step [0/17], d_loss: 0.0035562324337661266, g_loss: 7.276721954345703\n",
            "Epoch [583/2500], Step [0/17], d_loss: 0.0032150493934750557, g_loss: 7.465879440307617\n",
            "Epoch [584/2500], Step [0/17], d_loss: 0.004673832096159458, g_loss: 7.0423102378845215\n",
            "Epoch [585/2500], Step [0/17], d_loss: 0.0041567301377654076, g_loss: 7.010148525238037\n",
            "Epoch [586/2500], Step [0/17], d_loss: 0.00604038592427969, g_loss: 6.766127586364746\n",
            "Epoch [587/2500], Step [0/17], d_loss: 0.005375942215323448, g_loss: 6.853757858276367\n",
            "Epoch [588/2500], Step [0/17], d_loss: 0.005724728107452393, g_loss: 6.691543102264404\n",
            "Epoch [589/2500], Step [0/17], d_loss: 0.007153055630624294, g_loss: 6.812152862548828\n",
            "Epoch [590/2500], Step [0/17], d_loss: 0.003787816036492586, g_loss: 7.24411153793335\n",
            "Epoch [591/2500], Step [0/17], d_loss: 0.007276922930032015, g_loss: 7.045103073120117\n",
            "Epoch [592/2500], Step [0/17], d_loss: 0.004409254994243383, g_loss: 7.434094429016113\n",
            "Epoch [593/2500], Step [0/17], d_loss: 0.004055800847709179, g_loss: 7.247049808502197\n",
            "Epoch [594/2500], Step [0/17], d_loss: 0.00324942241422832, g_loss: 7.340600967407227\n",
            "Epoch [595/2500], Step [0/17], d_loss: 0.002223917515948415, g_loss: 7.449643135070801\n",
            "Epoch [596/2500], Step [0/17], d_loss: 0.002746200654655695, g_loss: 7.452010154724121\n",
            "Epoch [597/2500], Step [0/17], d_loss: 0.003323789220303297, g_loss: 7.247707366943359\n",
            "Epoch [598/2500], Step [0/17], d_loss: 0.0028279784601181746, g_loss: 7.771415710449219\n",
            "Epoch [599/2500], Step [0/17], d_loss: 0.0023230588994920254, g_loss: 7.323007106781006\n",
            "Epoch [600/2500], Step [0/17], d_loss: 0.005742071196436882, g_loss: 6.871870994567871\n",
            "Epoch [601/2500], Step [0/17], d_loss: 0.0035082106478512287, g_loss: 6.925634384155273\n",
            "Epoch [602/2500], Step [0/17], d_loss: 0.00434735044836998, g_loss: 7.151420593261719\n",
            "Epoch [603/2500], Step [0/17], d_loss: 0.06465134024620056, g_loss: 0.5894801616668701\n",
            "Epoch [604/2500], Step [0/17], d_loss: 1.302014708518982, g_loss: 1.6163911819458008\n",
            "Epoch [605/2500], Step [0/17], d_loss: 0.9659355282783508, g_loss: 2.2850871086120605\n",
            "Epoch [606/2500], Step [0/17], d_loss: 0.6388895511627197, g_loss: 3.0610358715057373\n",
            "Epoch [607/2500], Step [0/17], d_loss: 0.8609185218811035, g_loss: 7.631416320800781\n",
            "Epoch [608/2500], Step [0/17], d_loss: 0.753016471862793, g_loss: 9.022268295288086\n",
            "Epoch [609/2500], Step [0/17], d_loss: 1.3258538246154785, g_loss: 7.469169616699219\n",
            "Epoch [610/2500], Step [0/17], d_loss: 0.6800971031188965, g_loss: 4.5999369621276855\n",
            "Epoch [611/2500], Step [0/17], d_loss: 0.32620275020599365, g_loss: 8.049176216125488\n",
            "Epoch [612/2500], Step [0/17], d_loss: 0.6442688703536987, g_loss: 7.466660499572754\n",
            "Epoch [613/2500], Step [0/17], d_loss: 0.18559837341308594, g_loss: 6.674014091491699\n",
            "Epoch [614/2500], Step [0/17], d_loss: 0.45551255345344543, g_loss: 5.760412216186523\n",
            "Epoch [615/2500], Step [0/17], d_loss: 0.21476325392723083, g_loss: 6.407327175140381\n",
            "Epoch [616/2500], Step [0/17], d_loss: 0.3135210871696472, g_loss: 8.52277946472168\n",
            "Epoch [617/2500], Step [0/17], d_loss: 0.20974572002887726, g_loss: 5.3039116859436035\n",
            "Epoch [618/2500], Step [0/17], d_loss: 0.5027793645858765, g_loss: 9.64474105834961\n",
            "Epoch [619/2500], Step [0/17], d_loss: 0.2458130568265915, g_loss: 7.063560962677002\n",
            "Epoch [620/2500], Step [0/17], d_loss: 0.20836536586284637, g_loss: 7.246239185333252\n",
            "Epoch [621/2500], Step [0/17], d_loss: 0.2916291356086731, g_loss: 6.354779243469238\n",
            "Epoch [622/2500], Step [0/17], d_loss: 0.27230873703956604, g_loss: 8.38671875\n",
            "Epoch [623/2500], Step [0/17], d_loss: 0.35341256856918335, g_loss: 8.546761512756348\n",
            "Epoch [624/2500], Step [0/17], d_loss: 0.178059384226799, g_loss: 6.295161724090576\n",
            "Epoch [625/2500], Step [0/17], d_loss: 0.2516137957572937, g_loss: 7.945994853973389\n",
            "Epoch [626/2500], Step [0/17], d_loss: 0.21128766238689423, g_loss: 6.755417823791504\n",
            "Epoch [627/2500], Step [0/17], d_loss: 0.2864370346069336, g_loss: 8.579551696777344\n",
            "Epoch [628/2500], Step [0/17], d_loss: 0.08300947397947311, g_loss: 8.653024673461914\n",
            "Epoch [629/2500], Step [0/17], d_loss: 0.14913630485534668, g_loss: 7.348648548126221\n",
            "Epoch [630/2500], Step [0/17], d_loss: 0.2071106731891632, g_loss: 7.859224796295166\n",
            "Epoch [631/2500], Step [0/17], d_loss: 0.2360258400440216, g_loss: 8.518041610717773\n",
            "Epoch [632/2500], Step [0/17], d_loss: 0.14933288097381592, g_loss: 7.199254512786865\n",
            "Epoch [633/2500], Step [0/17], d_loss: 0.10601139068603516, g_loss: 6.6469879150390625\n",
            "Epoch [634/2500], Step [0/17], d_loss: 0.1061868742108345, g_loss: 7.6299285888671875\n",
            "Epoch [635/2500], Step [0/17], d_loss: 0.07109445333480835, g_loss: 7.421306133270264\n",
            "Epoch [636/2500], Step [0/17], d_loss: 0.10517752170562744, g_loss: 6.741943359375\n",
            "Epoch [637/2500], Step [0/17], d_loss: 0.053212713450193405, g_loss: 6.514649391174316\n",
            "Epoch [638/2500], Step [0/17], d_loss: 0.06985846161842346, g_loss: 6.882321357727051\n",
            "Epoch [639/2500], Step [0/17], d_loss: 0.04326135665178299, g_loss: 5.982216835021973\n",
            "Epoch [640/2500], Step [0/17], d_loss: 0.06576351821422577, g_loss: 6.346746444702148\n",
            "Epoch [641/2500], Step [0/17], d_loss: 0.055885713547468185, g_loss: 5.949590682983398\n",
            "Epoch [642/2500], Step [0/17], d_loss: 0.04986552894115448, g_loss: 7.060937404632568\n",
            "Epoch [643/2500], Step [0/17], d_loss: 0.0467584952712059, g_loss: 5.966009140014648\n",
            "Epoch [644/2500], Step [0/17], d_loss: 0.05535389482975006, g_loss: 6.603342056274414\n",
            "Epoch [645/2500], Step [0/17], d_loss: 0.05313963443040848, g_loss: 6.270359516143799\n",
            "Epoch [646/2500], Step [0/17], d_loss: 0.046504005789756775, g_loss: 6.045902252197266\n",
            "Epoch [647/2500], Step [0/17], d_loss: 0.05822427198290825, g_loss: 5.46102237701416\n",
            "Epoch [648/2500], Step [0/17], d_loss: 0.02295328862965107, g_loss: 6.4001593589782715\n",
            "Epoch [649/2500], Step [0/17], d_loss: 0.056942135095596313, g_loss: 6.706364631652832\n",
            "Epoch [650/2500], Step [0/17], d_loss: 0.02874615415930748, g_loss: 6.224739074707031\n",
            "Epoch [651/2500], Step [0/17], d_loss: 0.040303658694028854, g_loss: 6.073063850402832\n",
            "Epoch [652/2500], Step [0/17], d_loss: 0.03699727728962898, g_loss: 5.70884895324707\n",
            "Epoch [653/2500], Step [0/17], d_loss: 0.030360445380210876, g_loss: 5.4522905349731445\n",
            "Epoch [654/2500], Step [0/17], d_loss: 0.02930428832769394, g_loss: 5.878904342651367\n",
            "Epoch [655/2500], Step [0/17], d_loss: 0.02625686675310135, g_loss: 6.034409999847412\n",
            "Epoch [656/2500], Step [0/17], d_loss: 0.02818683162331581, g_loss: 6.011301040649414\n",
            "Epoch [657/2500], Step [0/17], d_loss: 0.037235915660858154, g_loss: 5.9962477684021\n",
            "Epoch [658/2500], Step [0/17], d_loss: 0.030769050121307373, g_loss: 5.891848564147949\n",
            "Epoch [659/2500], Step [0/17], d_loss: 0.026328828185796738, g_loss: 5.673331260681152\n",
            "Epoch [660/2500], Step [0/17], d_loss: 0.02958342432975769, g_loss: 5.61309814453125\n",
            "Epoch [661/2500], Step [0/17], d_loss: 0.03286702185869217, g_loss: 5.545350074768066\n",
            "Epoch [662/2500], Step [0/17], d_loss: 0.02159678190946579, g_loss: 5.742730617523193\n",
            "Epoch [663/2500], Step [0/17], d_loss: 0.030867725610733032, g_loss: 5.601319789886475\n",
            "Epoch [664/2500], Step [0/17], d_loss: 0.02461836487054825, g_loss: 5.880516052246094\n",
            "Epoch [665/2500], Step [0/17], d_loss: 0.017355643212795258, g_loss: 6.4004411697387695\n",
            "Epoch [666/2500], Step [0/17], d_loss: 0.0210088100284338, g_loss: 6.180335521697998\n",
            "Epoch [667/2500], Step [0/17], d_loss: 0.025633351877331734, g_loss: 5.93962287902832\n",
            "Epoch [668/2500], Step [0/17], d_loss: 0.02221802994608879, g_loss: 5.439736366271973\n",
            "Epoch [669/2500], Step [0/17], d_loss: 0.017061997205018997, g_loss: 5.633342742919922\n",
            "Epoch [670/2500], Step [0/17], d_loss: 0.025735747069120407, g_loss: 5.933526515960693\n",
            "Epoch [671/2500], Step [0/17], d_loss: 0.02697737142443657, g_loss: 5.600113391876221\n",
            "Epoch [672/2500], Step [0/17], d_loss: 0.0217172522097826, g_loss: 5.967935562133789\n",
            "Epoch [673/2500], Step [0/17], d_loss: 0.01893528178334236, g_loss: 6.60069465637207\n",
            "Epoch [674/2500], Step [0/17], d_loss: 0.01831468567252159, g_loss: 6.029250144958496\n",
            "Epoch [675/2500], Step [0/17], d_loss: 0.021850161254405975, g_loss: 5.682565689086914\n",
            "Epoch [676/2500], Step [0/17], d_loss: 0.02604777365922928, g_loss: 5.631716251373291\n",
            "Epoch [677/2500], Step [0/17], d_loss: 0.0292750783264637, g_loss: 5.835265159606934\n",
            "Epoch [678/2500], Step [0/17], d_loss: 0.018501847982406616, g_loss: 5.915250778198242\n",
            "Epoch [679/2500], Step [0/17], d_loss: 0.021538609638810158, g_loss: 5.835720539093018\n",
            "Epoch [680/2500], Step [0/17], d_loss: 0.014012083411216736, g_loss: 5.73553466796875\n",
            "Epoch [681/2500], Step [0/17], d_loss: 0.0266933161765337, g_loss: 5.6055192947387695\n",
            "Epoch [682/2500], Step [0/17], d_loss: 0.014906318858265877, g_loss: 6.040353298187256\n",
            "Epoch [683/2500], Step [0/17], d_loss: 0.012846388854086399, g_loss: 6.119915008544922\n",
            "Epoch [684/2500], Step [0/17], d_loss: 0.01603197492659092, g_loss: 5.6975603103637695\n",
            "Epoch [685/2500], Step [0/17], d_loss: 0.011128799989819527, g_loss: 6.142355918884277\n",
            "Epoch [686/2500], Step [0/17], d_loss: 0.020376432687044144, g_loss: 5.937008857727051\n",
            "Epoch [687/2500], Step [0/17], d_loss: 0.019272254779934883, g_loss: 6.254268169403076\n",
            "Epoch [688/2500], Step [0/17], d_loss: 0.016629431396722794, g_loss: 5.797267913818359\n",
            "Epoch [689/2500], Step [0/17], d_loss: 0.015616687014698982, g_loss: 6.4710845947265625\n",
            "Epoch [690/2500], Step [0/17], d_loss: 0.008567297831177711, g_loss: 6.610104560852051\n",
            "Epoch [691/2500], Step [0/17], d_loss: 0.01408639457076788, g_loss: 6.22562313079834\n",
            "Epoch [692/2500], Step [0/17], d_loss: 0.011513331905007362, g_loss: 6.108990669250488\n",
            "Epoch [693/2500], Step [0/17], d_loss: 0.013644924387335777, g_loss: 5.969060897827148\n",
            "Epoch [694/2500], Step [0/17], d_loss: 0.011974044144153595, g_loss: 6.088871955871582\n",
            "Epoch [695/2500], Step [0/17], d_loss: 0.014417978003621101, g_loss: 6.232305526733398\n",
            "Epoch [696/2500], Step [0/17], d_loss: 0.013570436276495457, g_loss: 6.8657097816467285\n",
            "Epoch [697/2500], Step [0/17], d_loss: 0.010776210576295853, g_loss: 6.274505138397217\n",
            "Epoch [698/2500], Step [0/17], d_loss: 0.008866886608302593, g_loss: 5.986154556274414\n",
            "Epoch [699/2500], Step [0/17], d_loss: 0.020022202283143997, g_loss: 6.63043737411499\n",
            "Epoch [700/2500], Step [0/17], d_loss: 0.0166866984218359, g_loss: 6.077127933502197\n",
            "Epoch [701/2500], Step [0/17], d_loss: 0.011853726580739021, g_loss: 6.135650634765625\n",
            "Epoch [702/2500], Step [0/17], d_loss: 0.011112391948699951, g_loss: 6.035916805267334\n",
            "Epoch [703/2500], Step [0/17], d_loss: 0.014143465086817741, g_loss: 6.133522987365723\n",
            "Epoch [704/2500], Step [0/17], d_loss: 0.010409396141767502, g_loss: 6.168674468994141\n",
            "Epoch [705/2500], Step [0/17], d_loss: 0.012775236740708351, g_loss: 6.204709053039551\n",
            "Epoch [706/2500], Step [0/17], d_loss: 0.012950589880347252, g_loss: 6.180929183959961\n",
            "Epoch [707/2500], Step [0/17], d_loss: 0.008735323324799538, g_loss: 6.523879051208496\n",
            "Epoch [708/2500], Step [0/17], d_loss: 0.010512669570744038, g_loss: 6.340620040893555\n",
            "Epoch [709/2500], Step [0/17], d_loss: 0.00792635791003704, g_loss: 6.683542251586914\n",
            "Epoch [710/2500], Step [0/17], d_loss: 0.008184602484107018, g_loss: 6.438786506652832\n",
            "Epoch [711/2500], Step [0/17], d_loss: 0.008676597848534584, g_loss: 6.647278785705566\n",
            "Epoch [712/2500], Step [0/17], d_loss: 0.00925200805068016, g_loss: 6.146173000335693\n",
            "Epoch [713/2500], Step [0/17], d_loss: 0.013864532113075256, g_loss: 6.422723770141602\n",
            "Epoch [714/2500], Step [0/17], d_loss: 0.007719730027019978, g_loss: 6.468010425567627\n",
            "Epoch [715/2500], Step [0/17], d_loss: 0.0061560748144984245, g_loss: 6.681956768035889\n",
            "Epoch [716/2500], Step [0/17], d_loss: 0.008574464358389378, g_loss: 6.096118450164795\n",
            "Epoch [717/2500], Step [0/17], d_loss: 0.005741499364376068, g_loss: 6.359427452087402\n",
            "Epoch [718/2500], Step [0/17], d_loss: 0.00734364939853549, g_loss: 6.376031875610352\n",
            "Epoch [719/2500], Step [0/17], d_loss: 0.012788395397365093, g_loss: 6.315344333648682\n",
            "Epoch [720/2500], Step [0/17], d_loss: 0.0069655925035476685, g_loss: 6.238632678985596\n",
            "Epoch [721/2500], Step [0/17], d_loss: 0.00791594572365284, g_loss: 6.5090484619140625\n",
            "Epoch [722/2500], Step [0/17], d_loss: 0.0062646144069731236, g_loss: 6.675411224365234\n",
            "Epoch [723/2500], Step [0/17], d_loss: 0.007295520044863224, g_loss: 6.802511692047119\n",
            "Epoch [724/2500], Step [0/17], d_loss: 0.0077203563414514065, g_loss: 6.095158100128174\n",
            "Epoch [725/2500], Step [0/17], d_loss: 0.00809862557798624, g_loss: 6.0754475593566895\n",
            "Epoch [726/2500], Step [0/17], d_loss: 0.008119895122945309, g_loss: 6.291250228881836\n",
            "Epoch [727/2500], Step [0/17], d_loss: 0.006239069625735283, g_loss: 6.536732196807861\n",
            "Epoch [728/2500], Step [0/17], d_loss: 0.006912666838616133, g_loss: 6.761140823364258\n",
            "Epoch [729/2500], Step [0/17], d_loss: 0.0053600044921040535, g_loss: 6.622284889221191\n",
            "Epoch [730/2500], Step [0/17], d_loss: 0.010422702878713608, g_loss: 6.167763710021973\n",
            "Epoch [731/2500], Step [0/17], d_loss: 0.00842935498803854, g_loss: 6.361030578613281\n",
            "Epoch [732/2500], Step [0/17], d_loss: 0.010416056960821152, g_loss: 6.373682975769043\n",
            "Epoch [733/2500], Step [0/17], d_loss: 0.005396793596446514, g_loss: 6.7383856773376465\n",
            "Epoch [734/2500], Step [0/17], d_loss: 0.006587722338736057, g_loss: 6.452818870544434\n",
            "Epoch [735/2500], Step [0/17], d_loss: 0.006341252475976944, g_loss: 6.666818618774414\n",
            "Epoch [736/2500], Step [0/17], d_loss: 0.00576504273340106, g_loss: 6.729064464569092\n",
            "Epoch [737/2500], Step [0/17], d_loss: 0.0066399830393493176, g_loss: 6.930067539215088\n",
            "Epoch [738/2500], Step [0/17], d_loss: 0.0053967880085110664, g_loss: 6.761760711669922\n",
            "Epoch [739/2500], Step [0/17], d_loss: 0.005041779018938541, g_loss: 7.055018424987793\n",
            "Epoch [740/2500], Step [0/17], d_loss: 0.008574037812650204, g_loss: 6.9742817878723145\n",
            "Epoch [741/2500], Step [0/17], d_loss: 0.004536380525678396, g_loss: 6.838032245635986\n",
            "Epoch [742/2500], Step [0/17], d_loss: 0.007460608147084713, g_loss: 6.976864814758301\n",
            "Epoch [743/2500], Step [0/17], d_loss: 0.005874309688806534, g_loss: 6.711184501647949\n",
            "Epoch [744/2500], Step [0/17], d_loss: 0.005739256739616394, g_loss: 6.824412822723389\n",
            "Epoch [745/2500], Step [0/17], d_loss: 0.006267739459872246, g_loss: 6.552127838134766\n",
            "Epoch [746/2500], Step [0/17], d_loss: 0.0058105322532355785, g_loss: 6.63839864730835\n",
            "Epoch [747/2500], Step [0/17], d_loss: 0.004877406172454357, g_loss: 7.522360801696777\n",
            "Epoch [748/2500], Step [0/17], d_loss: 0.005173805169761181, g_loss: 6.521810531616211\n",
            "Epoch [749/2500], Step [0/17], d_loss: 0.0057166023179888725, g_loss: 6.892940521240234\n",
            "Epoch [750/2500], Step [0/17], d_loss: 0.0037539852783083916, g_loss: 7.199426651000977\n",
            "Epoch [751/2500], Step [0/17], d_loss: 0.003508226713165641, g_loss: 7.2043962478637695\n",
            "Epoch [752/2500], Step [0/17], d_loss: 0.006596804596483707, g_loss: 7.7287139892578125\n",
            "Epoch [753/2500], Step [0/17], d_loss: 0.005766735412180424, g_loss: 7.103856086730957\n",
            "Epoch [754/2500], Step [0/17], d_loss: 0.004793371073901653, g_loss: 6.990368366241455\n",
            "Epoch [755/2500], Step [0/17], d_loss: 0.007261399645358324, g_loss: 6.763893127441406\n",
            "Epoch [756/2500], Step [0/17], d_loss: 0.005295909009873867, g_loss: 6.792351722717285\n",
            "Epoch [757/2500], Step [0/17], d_loss: 0.003952529281377792, g_loss: 7.140832901000977\n",
            "Epoch [758/2500], Step [0/17], d_loss: 0.007868709042668343, g_loss: 6.821065902709961\n",
            "Epoch [759/2500], Step [0/17], d_loss: 22.1567440032959, g_loss: 6.845915794372559\n",
            "Epoch [760/2500], Step [0/17], d_loss: 0.8387027382850647, g_loss: 3.293259620666504\n",
            "Epoch [761/2500], Step [0/17], d_loss: 0.5009320378303528, g_loss: 4.9989800453186035\n",
            "Epoch [762/2500], Step [0/17], d_loss: 1.0199705362319946, g_loss: 9.389669418334961\n",
            "Epoch [763/2500], Step [0/17], d_loss: 0.16240158677101135, g_loss: 7.31278133392334\n",
            "Epoch [764/2500], Step [0/17], d_loss: 0.2079339325428009, g_loss: 5.932114124298096\n",
            "Epoch [765/2500], Step [0/17], d_loss: 0.3680858016014099, g_loss: 7.48867130279541\n",
            "Epoch [766/2500], Step [0/17], d_loss: 0.6373149156570435, g_loss: 8.42280101776123\n",
            "Epoch [767/2500], Step [0/17], d_loss: 0.37856101989746094, g_loss: 11.188091278076172\n",
            "Epoch [768/2500], Step [0/17], d_loss: 0.08666258305311203, g_loss: 8.891697883605957\n",
            "Epoch [769/2500], Step [0/17], d_loss: 0.14630119502544403, g_loss: 8.981901168823242\n",
            "Epoch [770/2500], Step [0/17], d_loss: 0.2651345133781433, g_loss: 10.456174850463867\n",
            "Epoch [771/2500], Step [0/17], d_loss: 0.14098681509494781, g_loss: 8.92857551574707\n",
            "Epoch [772/2500], Step [0/17], d_loss: 0.09198196977376938, g_loss: 7.1477274894714355\n",
            "Epoch [773/2500], Step [0/17], d_loss: 0.0633605346083641, g_loss: 7.2284321784973145\n",
            "Epoch [774/2500], Step [0/17], d_loss: 0.055856190621852875, g_loss: 8.478041648864746\n",
            "Epoch [775/2500], Step [0/17], d_loss: 0.028760291635990143, g_loss: 8.239338874816895\n",
            "Epoch [776/2500], Step [0/17], d_loss: 0.040568526834249496, g_loss: 6.850888252258301\n",
            "Epoch [777/2500], Step [0/17], d_loss: 0.021807176992297173, g_loss: 7.597148418426514\n",
            "Epoch [778/2500], Step [0/17], d_loss: 0.05355583131313324, g_loss: 7.517269134521484\n",
            "Epoch [779/2500], Step [0/17], d_loss: 0.03458697348833084, g_loss: 7.491597652435303\n",
            "Epoch [780/2500], Step [0/17], d_loss: 0.03430760279297829, g_loss: 7.035113334655762\n",
            "Epoch [781/2500], Step [0/17], d_loss: 0.04254014045000076, g_loss: 7.074680328369141\n",
            "Epoch [782/2500], Step [0/17], d_loss: 0.03231189399957657, g_loss: 6.414502143859863\n",
            "Epoch [783/2500], Step [0/17], d_loss: 0.019497906789183617, g_loss: 7.0103254318237305\n",
            "Epoch [784/2500], Step [0/17], d_loss: 0.032197050750255585, g_loss: 6.980948448181152\n",
            "Epoch [785/2500], Step [0/17], d_loss: 0.022871432825922966, g_loss: 6.821981430053711\n",
            "Epoch [786/2500], Step [0/17], d_loss: 0.013502661138772964, g_loss: 6.874629497528076\n",
            "Epoch [787/2500], Step [0/17], d_loss: 0.016697309911251068, g_loss: 6.513200283050537\n",
            "Epoch [788/2500], Step [0/17], d_loss: 0.045554302632808685, g_loss: 7.40805721282959\n",
            "Epoch [789/2500], Step [0/17], d_loss: 0.01601474918425083, g_loss: 6.462411880493164\n",
            "Epoch [790/2500], Step [0/17], d_loss: 0.024965714663267136, g_loss: 7.119633197784424\n",
            "Epoch [791/2500], Step [0/17], d_loss: 0.02027924172580242, g_loss: 6.630075454711914\n",
            "Epoch [792/2500], Step [0/17], d_loss: 0.01783936284482479, g_loss: 6.429523944854736\n",
            "Epoch [793/2500], Step [0/17], d_loss: 0.016110561788082123, g_loss: 6.490609169006348\n",
            "Epoch [794/2500], Step [0/17], d_loss: 0.018681155517697334, g_loss: 6.565255641937256\n",
            "Epoch [795/2500], Step [0/17], d_loss: 0.020922798663377762, g_loss: 6.257360458374023\n",
            "Epoch [796/2500], Step [0/17], d_loss: 0.015652088448405266, g_loss: 6.295732498168945\n",
            "Epoch [797/2500], Step [0/17], d_loss: 0.01230076514184475, g_loss: 6.61783504486084\n",
            "Epoch [798/2500], Step [0/17], d_loss: 0.016517559066414833, g_loss: 6.387226104736328\n",
            "Epoch [799/2500], Step [0/17], d_loss: 0.012119466438889503, g_loss: 6.743083953857422\n",
            "Epoch [800/2500], Step [0/17], d_loss: 0.013562783598899841, g_loss: 6.467777729034424\n",
            "Epoch [801/2500], Step [0/17], d_loss: 0.012316038832068443, g_loss: 6.562685489654541\n",
            "Epoch [802/2500], Step [0/17], d_loss: 0.016581853851675987, g_loss: 6.263922214508057\n",
            "Epoch [803/2500], Step [0/17], d_loss: 0.017516523599624634, g_loss: 6.770539283752441\n",
            "Epoch [804/2500], Step [0/17], d_loss: 0.013284939341247082, g_loss: 6.6377973556518555\n",
            "Epoch [805/2500], Step [0/17], d_loss: 0.005911838263273239, g_loss: 6.968334674835205\n",
            "Epoch [806/2500], Step [0/17], d_loss: 0.014695151709020138, g_loss: 6.098339557647705\n",
            "Epoch [807/2500], Step [0/17], d_loss: 0.013897056691348553, g_loss: 7.32344913482666\n",
            "Epoch [808/2500], Step [0/17], d_loss: 0.014184260740876198, g_loss: 6.653049468994141\n",
            "Epoch [809/2500], Step [0/17], d_loss: 0.00823277048766613, g_loss: 6.753267288208008\n",
            "Epoch [810/2500], Step [0/17], d_loss: 0.012031394056975842, g_loss: 6.471867561340332\n",
            "Epoch [811/2500], Step [0/17], d_loss: 0.0181148499250412, g_loss: 6.0673112869262695\n",
            "Epoch [812/2500], Step [0/17], d_loss: 0.010110422037541866, g_loss: 6.81844425201416\n",
            "Epoch [813/2500], Step [0/17], d_loss: 0.01653149165213108, g_loss: 6.186945915222168\n",
            "Epoch [814/2500], Step [0/17], d_loss: 0.007862811908125877, g_loss: 6.381310939788818\n",
            "Epoch [815/2500], Step [0/17], d_loss: 0.010562112554907799, g_loss: 6.300248146057129\n",
            "Epoch [816/2500], Step [0/17], d_loss: 0.01080801896750927, g_loss: 6.381370544433594\n",
            "Epoch [817/2500], Step [0/17], d_loss: 0.0073073552921414375, g_loss: 7.027466773986816\n",
            "Epoch [818/2500], Step [0/17], d_loss: 0.009897948242723942, g_loss: 6.676613807678223\n",
            "Epoch [819/2500], Step [0/17], d_loss: 0.01143842376768589, g_loss: 6.458028316497803\n",
            "Epoch [820/2500], Step [0/17], d_loss: 0.010711326263844967, g_loss: 6.860649585723877\n",
            "Epoch [821/2500], Step [0/17], d_loss: 0.009996013715863228, g_loss: 6.398127555847168\n",
            "Epoch [822/2500], Step [0/17], d_loss: 0.014548334293067455, g_loss: 6.674384593963623\n",
            "Epoch [823/2500], Step [0/17], d_loss: 0.008756045252084732, g_loss: 6.485607624053955\n",
            "Epoch [824/2500], Step [0/17], d_loss: 0.007791688200086355, g_loss: 6.778602123260498\n",
            "Epoch [825/2500], Step [0/17], d_loss: 0.0038496823981404305, g_loss: 7.557545185089111\n",
            "Epoch [826/2500], Step [0/17], d_loss: 0.006149019114673138, g_loss: 6.669920921325684\n",
            "Epoch [827/2500], Step [0/17], d_loss: 0.00794338807463646, g_loss: 7.16732931137085\n",
            "Epoch [828/2500], Step [0/17], d_loss: 0.004359138663858175, g_loss: 7.638051986694336\n",
            "Epoch [829/2500], Step [0/17], d_loss: 0.006338352337479591, g_loss: 6.901786804199219\n",
            "Epoch [830/2500], Step [0/17], d_loss: 0.006222044117748737, g_loss: 7.00730562210083\n",
            "Epoch [831/2500], Step [0/17], d_loss: 0.008699931204319, g_loss: 7.1857829093933105\n",
            "Epoch [832/2500], Step [0/17], d_loss: 0.007624608930200338, g_loss: 7.046616554260254\n",
            "Epoch [833/2500], Step [0/17], d_loss: 0.005689497105777264, g_loss: 7.099340438842773\n",
            "Epoch [834/2500], Step [0/17], d_loss: 0.00690295547246933, g_loss: 7.160379409790039\n",
            "Epoch [835/2500], Step [0/17], d_loss: 0.005775123834609985, g_loss: 6.908659934997559\n",
            "Epoch [836/2500], Step [0/17], d_loss: 0.0073288665153086185, g_loss: 6.879530906677246\n",
            "Epoch [837/2500], Step [0/17], d_loss: 0.0061013964004814625, g_loss: 6.835444450378418\n",
            "Epoch [838/2500], Step [0/17], d_loss: 0.005395311396569014, g_loss: 7.052113056182861\n",
            "Epoch [839/2500], Step [0/17], d_loss: 0.008123762905597687, g_loss: 6.870952606201172\n",
            "Epoch [840/2500], Step [0/17], d_loss: 0.007045049220323563, g_loss: 6.728772163391113\n",
            "Epoch [841/2500], Step [0/17], d_loss: 0.00747755216434598, g_loss: 7.084306716918945\n",
            "Epoch [842/2500], Step [0/17], d_loss: 0.00911659188568592, g_loss: 7.172513008117676\n",
            "Epoch [843/2500], Step [0/17], d_loss: 0.005916840396821499, g_loss: 6.835530757904053\n",
            "Epoch [844/2500], Step [0/17], d_loss: 0.0054292138665914536, g_loss: 6.846465110778809\n",
            "Epoch [845/2500], Step [0/17], d_loss: 0.007073169108480215, g_loss: 6.922249794006348\n",
            "Epoch [846/2500], Step [0/17], d_loss: 0.006527918856590986, g_loss: 6.998646259307861\n",
            "Epoch [847/2500], Step [0/17], d_loss: 0.002935128752142191, g_loss: 7.335996627807617\n",
            "Epoch [848/2500], Step [0/17], d_loss: 0.0038132346235215664, g_loss: 7.788522243499756\n",
            "Epoch [849/2500], Step [0/17], d_loss: 0.0060560391284525394, g_loss: 7.138131141662598\n",
            "Epoch [850/2500], Step [0/17], d_loss: 0.007088145706802607, g_loss: 7.583999156951904\n",
            "Epoch [851/2500], Step [0/17], d_loss: 0.004191265441477299, g_loss: 7.338207721710205\n",
            "Epoch [852/2500], Step [0/17], d_loss: 0.005874850321561098, g_loss: 7.178715705871582\n",
            "Epoch [853/2500], Step [0/17], d_loss: 0.006526146084070206, g_loss: 7.1147260665893555\n",
            "Epoch [854/2500], Step [0/17], d_loss: 0.004936584271490574, g_loss: 6.807175636291504\n",
            "Epoch [855/2500], Step [0/17], d_loss: 0.004452456720173359, g_loss: 7.820050239562988\n",
            "Epoch [856/2500], Step [0/17], d_loss: 0.00636780820786953, g_loss: 6.824536323547363\n",
            "Epoch [857/2500], Step [0/17], d_loss: 0.005513163283467293, g_loss: 7.429656982421875\n",
            "Epoch [858/2500], Step [0/17], d_loss: 0.006927846930921078, g_loss: 7.294546127319336\n",
            "Epoch [859/2500], Step [0/17], d_loss: 0.004649178124964237, g_loss: 7.4590301513671875\n",
            "Epoch [860/2500], Step [0/17], d_loss: 0.005911602638661861, g_loss: 7.6209516525268555\n",
            "Epoch [861/2500], Step [0/17], d_loss: 0.0021524117328226566, g_loss: 8.264822006225586\n",
            "Epoch [862/2500], Step [0/17], d_loss: 0.00611394876614213, g_loss: 7.558962821960449\n",
            "Epoch [863/2500], Step [0/17], d_loss: 0.004281527362763882, g_loss: 7.439257621765137\n",
            "Epoch [864/2500], Step [0/17], d_loss: 0.002865530550479889, g_loss: 8.344361305236816\n",
            "Epoch [865/2500], Step [0/17], d_loss: 0.0037918207235634327, g_loss: 7.849052429199219\n",
            "Epoch [866/2500], Step [0/17], d_loss: 0.0033079655840992928, g_loss: 7.996597766876221\n",
            "Epoch [867/2500], Step [0/17], d_loss: 0.0023720581084489822, g_loss: 8.27349853515625\n",
            "Epoch [868/2500], Step [0/17], d_loss: 0.0035610608756542206, g_loss: 8.43921184539795\n",
            "Epoch [869/2500], Step [0/17], d_loss: 0.0030493144877254963, g_loss: 8.663761138916016\n",
            "Epoch [870/2500], Step [0/17], d_loss: 0.0019373067189007998, g_loss: 9.143267631530762\n",
            "Epoch [871/2500], Step [0/17], d_loss: 0.005087182857096195, g_loss: 9.618733406066895\n",
            "Epoch [872/2500], Step [0/17], d_loss: 0.0017387884436175227, g_loss: 10.14154052734375\n",
            "Epoch [873/2500], Step [0/17], d_loss: 0.0003348938771523535, g_loss: 12.466991424560547\n",
            "Epoch [874/2500], Step [0/17], d_loss: 0.003173147328197956, g_loss: 11.345212936401367\n",
            "Epoch [875/2500], Step [0/17], d_loss: 0.0017917521763592958, g_loss: 11.751289367675781\n",
            "Epoch [876/2500], Step [0/17], d_loss: 0.00041855411836877465, g_loss: 16.1143856048584\n",
            "Epoch [877/2500], Step [0/17], d_loss: 0.00031807320192456245, g_loss: 29.276561737060547\n",
            "Epoch [878/2500], Step [0/17], d_loss: 0.0014046344440430403, g_loss: 10.561599731445312\n",
            "Epoch [879/2500], Step [0/17], d_loss: 0.0012861103750765324, g_loss: 11.323382377624512\n",
            "Epoch [880/2500], Step [0/17], d_loss: 0.0012085915077477694, g_loss: 11.339261054992676\n",
            "Epoch [881/2500], Step [0/17], d_loss: 0.0009590114932507277, g_loss: 11.414848327636719\n",
            "Epoch [882/2500], Step [0/17], d_loss: 0.0008729866240173578, g_loss: 10.684303283691406\n",
            "Epoch [883/2500], Step [0/17], d_loss: 0.0008023105910979211, g_loss: 10.813560485839844\n",
            "Epoch [884/2500], Step [0/17], d_loss: 0.0012734041083604097, g_loss: 10.713068962097168\n",
            "Epoch [885/2500], Step [0/17], d_loss: 0.0009650662541389465, g_loss: 11.051091194152832\n",
            "Epoch [886/2500], Step [0/17], d_loss: 0.0021160407923161983, g_loss: 11.470632553100586\n",
            "Epoch [887/2500], Step [0/17], d_loss: 0.0005291515844874084, g_loss: 10.974512100219727\n",
            "Epoch [888/2500], Step [0/17], d_loss: 0.000708180945366621, g_loss: 10.726245880126953\n",
            "Epoch [889/2500], Step [0/17], d_loss: 0.004660570994019508, g_loss: 16.556678771972656\n",
            "Epoch [890/2500], Step [0/17], d_loss: 0.00014336284948512912, g_loss: 42.602134704589844\n",
            "Epoch [891/2500], Step [0/17], d_loss: 2.5217654183506966e-05, g_loss: 42.0479621887207\n",
            "Epoch [892/2500], Step [0/17], d_loss: 8.916150363802444e-06, g_loss: 41.91035461425781\n",
            "Epoch [893/2500], Step [0/17], d_loss: 2.570484411990037e-06, g_loss: 41.99300765991211\n",
            "Epoch [894/2500], Step [0/17], d_loss: 1.0197714800597169e-05, g_loss: 41.95415496826172\n",
            "Epoch [895/2500], Step [0/17], d_loss: 1.419343448105792e-06, g_loss: 41.66130065917969\n",
            "Epoch [896/2500], Step [0/17], d_loss: 2.5071462914638687e-06, g_loss: 41.55973815917969\n",
            "Epoch [897/2500], Step [0/17], d_loss: 5.943938958807848e-06, g_loss: 41.51689910888672\n",
            "Epoch [898/2500], Step [0/17], d_loss: 2.805232270475244e-06, g_loss: 41.199893951416016\n",
            "Epoch [899/2500], Step [0/17], d_loss: 5.725933988287579e-06, g_loss: 41.01885223388672\n",
            "Epoch [900/2500], Step [0/17], d_loss: 1.9688311567733763e-06, g_loss: 40.73203659057617\n",
            "Epoch [901/2500], Step [0/17], d_loss: 5.096350378153147e-06, g_loss: 39.78020477294922\n",
            "Epoch [902/2500], Step [0/17], d_loss: 2.1327705326257274e-06, g_loss: 38.31100845336914\n",
            "Epoch [903/2500], Step [0/17], d_loss: 5.442420660983771e-05, g_loss: 36.990570068359375\n",
            "Epoch [904/2500], Step [0/17], d_loss: 7.627714512636885e-05, g_loss: 38.595149993896484\n",
            "Epoch [905/2500], Step [0/17], d_loss: 6.689115252811462e-05, g_loss: 37.8282470703125\n",
            "Epoch [906/2500], Step [0/17], d_loss: 1.494758362241555e-05, g_loss: 37.48876953125\n",
            "Epoch [907/2500], Step [0/17], d_loss: 7.204881512734573e-06, g_loss: 36.375328063964844\n",
            "Epoch [908/2500], Step [0/17], d_loss: 1.4289250430010725e-05, g_loss: 22.943737030029297\n",
            "Epoch [909/2500], Step [0/17], d_loss: 0.17516615986824036, g_loss: 44.79182434082031\n",
            "Epoch [910/2500], Step [0/17], d_loss: 0.0014912679325789213, g_loss: 35.76955795288086\n",
            "Epoch [911/2500], Step [0/17], d_loss: 0.7517603039741516, g_loss: 24.065757751464844\n",
            "Epoch [912/2500], Step [0/17], d_loss: 0.09565286338329315, g_loss: 8.454906463623047\n",
            "Epoch [913/2500], Step [0/17], d_loss: 0.29674607515335083, g_loss: 8.561203956604004\n",
            "Epoch [914/2500], Step [0/17], d_loss: 0.5507093071937561, g_loss: 12.141388893127441\n",
            "Epoch [915/2500], Step [0/17], d_loss: 0.029312174767255783, g_loss: 9.92833137512207\n",
            "Epoch [916/2500], Step [0/17], d_loss: 0.3615852892398834, g_loss: 12.143083572387695\n",
            "Epoch [917/2500], Step [0/17], d_loss: 0.06030889227986336, g_loss: 6.165623664855957\n",
            "Epoch [918/2500], Step [0/17], d_loss: 0.281882107257843, g_loss: 11.397979736328125\n",
            "Epoch [919/2500], Step [0/17], d_loss: 0.054839324206113815, g_loss: 7.354085922241211\n",
            "Epoch [920/2500], Step [0/17], d_loss: 0.1848136931657791, g_loss: 9.322586059570312\n",
            "Epoch [921/2500], Step [0/17], d_loss: 0.05798795446753502, g_loss: 6.379537582397461\n",
            "Epoch [922/2500], Step [0/17], d_loss: 0.22451065480709076, g_loss: 9.43156909942627\n",
            "Epoch [923/2500], Step [0/17], d_loss: 0.26583462953567505, g_loss: 6.664479732513428\n",
            "Epoch [924/2500], Step [0/17], d_loss: 0.1757742017507553, g_loss: 7.661748886108398\n",
            "Epoch [925/2500], Step [0/17], d_loss: 0.2139962911605835, g_loss: 8.030845642089844\n",
            "Epoch [926/2500], Step [0/17], d_loss: 0.11616642773151398, g_loss: 8.74080753326416\n",
            "Epoch [927/2500], Step [0/17], d_loss: 0.47844064235687256, g_loss: 12.804298400878906\n",
            "Epoch [928/2500], Step [0/17], d_loss: 0.32956594228744507, g_loss: 9.995990753173828\n",
            "Epoch [929/2500], Step [0/17], d_loss: 0.40113264322280884, g_loss: 10.648092269897461\n",
            "Epoch [930/2500], Step [0/17], d_loss: 0.02926519699394703, g_loss: 7.66300106048584\n",
            "Epoch [931/2500], Step [0/17], d_loss: 0.5876598358154297, g_loss: 9.346999168395996\n",
            "Epoch [932/2500], Step [0/17], d_loss: 0.11533886194229126, g_loss: 8.712118148803711\n",
            "Epoch [933/2500], Step [0/17], d_loss: 0.2654905617237091, g_loss: 9.463447570800781\n",
            "Epoch [934/2500], Step [0/17], d_loss: 0.09469236433506012, g_loss: 7.411831378936768\n",
            "Epoch [935/2500], Step [0/17], d_loss: 0.12664923071861267, g_loss: 6.0850725173950195\n",
            "Epoch [936/2500], Step [0/17], d_loss: 0.17564277350902557, g_loss: 7.928914546966553\n",
            "Epoch [937/2500], Step [0/17], d_loss: 0.0931980088353157, g_loss: 8.334612846374512\n",
            "Epoch [938/2500], Step [0/17], d_loss: 0.02394295111298561, g_loss: 8.397204399108887\n",
            "Epoch [939/2500], Step [0/17], d_loss: 0.09213868528604507, g_loss: 6.081325531005859\n",
            "Epoch [940/2500], Step [0/17], d_loss: 0.09764006733894348, g_loss: 10.938517570495605\n",
            "Epoch [941/2500], Step [0/17], d_loss: 0.0509001687169075, g_loss: 6.439670085906982\n",
            "Epoch [942/2500], Step [0/17], d_loss: 0.012791295535862446, g_loss: 7.60675573348999\n",
            "Epoch [943/2500], Step [0/17], d_loss: 0.037879593670368195, g_loss: 8.6638765335083\n",
            "Epoch [944/2500], Step [0/17], d_loss: 0.08247151225805283, g_loss: 7.850254058837891\n",
            "Epoch [945/2500], Step [0/17], d_loss: 0.07242894172668457, g_loss: 6.665351867675781\n",
            "Epoch [946/2500], Step [0/17], d_loss: 0.09379175305366516, g_loss: 9.044816970825195\n",
            "Epoch [947/2500], Step [0/17], d_loss: 0.08614236861467361, g_loss: 8.679544448852539\n",
            "Epoch [948/2500], Step [0/17], d_loss: 0.09265560656785965, g_loss: 8.070977210998535\n",
            "Epoch [949/2500], Step [0/17], d_loss: 0.03920862078666687, g_loss: 7.797901630401611\n",
            "Epoch [950/2500], Step [0/17], d_loss: 0.06467222422361374, g_loss: 7.5035505294799805\n",
            "Epoch [951/2500], Step [0/17], d_loss: 0.043428003787994385, g_loss: 7.105808258056641\n",
            "Epoch [952/2500], Step [0/17], d_loss: 0.01071886532008648, g_loss: 8.343832015991211\n",
            "Epoch [953/2500], Step [0/17], d_loss: 0.036452725529670715, g_loss: 7.649554252624512\n",
            "Epoch [954/2500], Step [0/17], d_loss: 0.1574210524559021, g_loss: 11.086356163024902\n",
            "Epoch [955/2500], Step [0/17], d_loss: 0.09288462996482849, g_loss: 7.0231804847717285\n",
            "Epoch [956/2500], Step [0/17], d_loss: 0.043313831090927124, g_loss: 7.372681617736816\n",
            "Epoch [957/2500], Step [0/17], d_loss: 0.018224596977233887, g_loss: 6.340236663818359\n",
            "Epoch [958/2500], Step [0/17], d_loss: 0.04626509174704552, g_loss: 6.8278303146362305\n",
            "Epoch [959/2500], Step [0/17], d_loss: 0.03165483474731445, g_loss: 6.645188331604004\n",
            "Epoch [960/2500], Step [0/17], d_loss: 0.03356323391199112, g_loss: 6.909296035766602\n",
            "Epoch [961/2500], Step [0/17], d_loss: 0.024069655686616898, g_loss: 6.965888023376465\n",
            "Epoch [962/2500], Step [0/17], d_loss: 0.014144863933324814, g_loss: 11.954360961914062\n",
            "Epoch [963/2500], Step [0/17], d_loss: 0.03429893031716347, g_loss: 7.239149570465088\n",
            "Epoch [964/2500], Step [0/17], d_loss: 0.02715305984020233, g_loss: 7.6160888671875\n",
            "Epoch [965/2500], Step [0/17], d_loss: 0.030227214097976685, g_loss: 7.727728843688965\n",
            "Epoch [966/2500], Step [0/17], d_loss: 0.03526079282164574, g_loss: 7.344893932342529\n",
            "Epoch [967/2500], Step [0/17], d_loss: 0.027706097811460495, g_loss: 7.140875816345215\n",
            "Epoch [968/2500], Step [0/17], d_loss: 0.031098315492272377, g_loss: 7.315391540527344\n",
            "Epoch [969/2500], Step [0/17], d_loss: 0.015699420124292374, g_loss: 6.819042682647705\n",
            "Epoch [970/2500], Step [0/17], d_loss: 0.01774158701300621, g_loss: 7.247808933258057\n",
            "Epoch [971/2500], Step [0/17], d_loss: 0.019477002322673798, g_loss: 7.495625972747803\n",
            "Epoch [972/2500], Step [0/17], d_loss: 0.0566185899078846, g_loss: 7.290499687194824\n",
            "Epoch [973/2500], Step [0/17], d_loss: 0.032152194529771805, g_loss: 10.722610473632812\n",
            "Epoch [974/2500], Step [0/17], d_loss: 0.01940249092876911, g_loss: 7.814373016357422\n",
            "Epoch [975/2500], Step [0/17], d_loss: 0.047847405076026917, g_loss: 7.854867935180664\n",
            "Epoch [976/2500], Step [0/17], d_loss: 0.03476633504033089, g_loss: 6.203420639038086\n",
            "Epoch [977/2500], Step [0/17], d_loss: 0.04936707392334938, g_loss: 7.099617004394531\n",
            "Epoch [978/2500], Step [0/17], d_loss: 0.06990371644496918, g_loss: 6.507596969604492\n",
            "Epoch [979/2500], Step [0/17], d_loss: 0.03745038062334061, g_loss: 6.871331214904785\n",
            "Epoch [980/2500], Step [0/17], d_loss: 0.030986158177256584, g_loss: 7.360887050628662\n",
            "Epoch [981/2500], Step [0/17], d_loss: 0.015028955414891243, g_loss: 7.162672519683838\n",
            "Epoch [982/2500], Step [0/17], d_loss: 0.017095403745770454, g_loss: 6.613310813903809\n",
            "Epoch [983/2500], Step [0/17], d_loss: 0.021901298314332962, g_loss: 7.189877510070801\n",
            "Epoch [984/2500], Step [0/17], d_loss: 0.050279662013053894, g_loss: 7.686282634735107\n",
            "Epoch [985/2500], Step [0/17], d_loss: 0.0172541756182909, g_loss: 7.869940757751465\n",
            "Epoch [986/2500], Step [0/17], d_loss: 0.015611817128956318, g_loss: 7.8581013679504395\n",
            "Epoch [987/2500], Step [0/17], d_loss: 0.021801553666591644, g_loss: 6.857851982116699\n",
            "Epoch [988/2500], Step [0/17], d_loss: 0.011605405248701572, g_loss: 7.2493438720703125\n",
            "Epoch [989/2500], Step [0/17], d_loss: 0.018925510346889496, g_loss: 7.526639461517334\n",
            "Epoch [990/2500], Step [0/17], d_loss: 0.017992550507187843, g_loss: 7.562281131744385\n",
            "Epoch [991/2500], Step [0/17], d_loss: 0.03536342829465866, g_loss: 6.451436996459961\n",
            "Epoch [992/2500], Step [0/17], d_loss: 0.03639861196279526, g_loss: 6.730611801147461\n",
            "Epoch [993/2500], Step [0/17], d_loss: 0.007974948734045029, g_loss: 7.807311534881592\n",
            "Epoch [994/2500], Step [0/17], d_loss: 0.021963827311992645, g_loss: 6.346913814544678\n",
            "Epoch [995/2500], Step [0/17], d_loss: 0.014362157322466373, g_loss: 7.485800743103027\n",
            "Epoch [996/2500], Step [0/17], d_loss: 0.012901894748210907, g_loss: 7.547386169433594\n",
            "Epoch [997/2500], Step [0/17], d_loss: 0.02379470318555832, g_loss: 7.377144813537598\n",
            "Epoch [998/2500], Step [0/17], d_loss: 0.021381042897701263, g_loss: 7.386873245239258\n",
            "Epoch [999/2500], Step [0/17], d_loss: 0.021480724215507507, g_loss: 7.715276718139648\n",
            "Epoch [1000/2500], Step [0/17], d_loss: 0.008615752682089806, g_loss: 8.513169288635254\n",
            "Epoch [1001/2500], Step [0/17], d_loss: 0.020541446283459663, g_loss: 7.648741722106934\n",
            "Epoch [1002/2500], Step [0/17], d_loss: 0.042522892355918884, g_loss: 6.883614540100098\n",
            "Epoch [1003/2500], Step [0/17], d_loss: 0.020289935171604156, g_loss: 7.7848734855651855\n",
            "Epoch [1004/2500], Step [0/17], d_loss: 0.02204721048474312, g_loss: 7.721378803253174\n",
            "Epoch [1005/2500], Step [0/17], d_loss: 0.018437275663018227, g_loss: 7.712371826171875\n",
            "Epoch [1006/2500], Step [0/17], d_loss: 0.006534476764500141, g_loss: 7.157817840576172\n",
            "Epoch [1007/2500], Step [0/17], d_loss: 0.031021127477288246, g_loss: 7.298830032348633\n",
            "Epoch [1008/2500], Step [0/17], d_loss: 0.015826504677534103, g_loss: 8.273176193237305\n",
            "Epoch [1009/2500], Step [0/17], d_loss: 0.010542557574808598, g_loss: 7.209563732147217\n",
            "Epoch [1010/2500], Step [0/17], d_loss: 0.015188726596534252, g_loss: 7.3731184005737305\n",
            "Epoch [1011/2500], Step [0/17], d_loss: 0.00904151238501072, g_loss: 8.218429565429688\n",
            "Epoch [1012/2500], Step [0/17], d_loss: 0.01025824062526226, g_loss: 7.801308631896973\n",
            "Epoch [1013/2500], Step [0/17], d_loss: 0.02204989641904831, g_loss: 8.20452880859375\n",
            "Epoch [1014/2500], Step [0/17], d_loss: 0.019307173788547516, g_loss: 8.107120513916016\n",
            "Epoch [1015/2500], Step [0/17], d_loss: 0.010082880035042763, g_loss: 7.51504373550415\n",
            "Epoch [1016/2500], Step [0/17], d_loss: 0.01021813228726387, g_loss: 8.306276321411133\n",
            "Epoch [1017/2500], Step [0/17], d_loss: 0.013845732435584068, g_loss: 7.342167854309082\n",
            "Epoch [1018/2500], Step [0/17], d_loss: 0.035041920840740204, g_loss: 6.245849609375\n",
            "Epoch [1019/2500], Step [0/17], d_loss: 0.022733695805072784, g_loss: 6.534952163696289\n",
            "Epoch [1020/2500], Step [0/17], d_loss: 0.011491790413856506, g_loss: 7.22755765914917\n",
            "Epoch [1021/2500], Step [0/17], d_loss: 0.009080984629690647, g_loss: 7.207651615142822\n",
            "Epoch [1022/2500], Step [0/17], d_loss: 0.014786011539399624, g_loss: 7.3627729415893555\n",
            "Epoch [1023/2500], Step [0/17], d_loss: 0.01531527191400528, g_loss: 7.856788635253906\n",
            "Epoch [1024/2500], Step [0/17], d_loss: 0.010088950395584106, g_loss: 7.324134826660156\n",
            "Epoch [1025/2500], Step [0/17], d_loss: 0.005136676132678986, g_loss: 7.613866329193115\n",
            "Epoch [1026/2500], Step [0/17], d_loss: 0.01104646921157837, g_loss: 7.975286483764648\n",
            "Epoch [1027/2500], Step [0/17], d_loss: 0.030870486050844193, g_loss: 9.198408126831055\n",
            "Epoch [1028/2500], Step [0/17], d_loss: 0.016991451382637024, g_loss: 7.161870956420898\n",
            "Epoch [1029/2500], Step [0/17], d_loss: 0.011032963171601295, g_loss: 8.06223201751709\n",
            "Epoch [1030/2500], Step [0/17], d_loss: 0.0125995809212327, g_loss: 7.342784881591797\n",
            "Epoch [1031/2500], Step [0/17], d_loss: 0.004807961173355579, g_loss: 8.102920532226562\n",
            "Epoch [1032/2500], Step [0/17], d_loss: 0.01572806015610695, g_loss: 7.977708339691162\n",
            "Epoch [1033/2500], Step [0/17], d_loss: 0.0094471899792552, g_loss: 7.886046886444092\n",
            "Epoch [1034/2500], Step [0/17], d_loss: 0.0069153388030827045, g_loss: 7.615994453430176\n",
            "Epoch [1035/2500], Step [0/17], d_loss: 0.004708647262305021, g_loss: 8.139481544494629\n",
            "Epoch [1036/2500], Step [0/17], d_loss: 0.029030287638306618, g_loss: 10.179280281066895\n",
            "Epoch [1037/2500], Step [0/17], d_loss: 0.010034589096903801, g_loss: 11.375133514404297\n",
            "Epoch [1038/2500], Step [0/17], d_loss: 0.013829698786139488, g_loss: 8.10545539855957\n",
            "Epoch [1039/2500], Step [0/17], d_loss: 0.00563490716740489, g_loss: 8.707538604736328\n",
            "Epoch [1040/2500], Step [0/17], d_loss: 0.011839385144412518, g_loss: 7.6460113525390625\n",
            "Epoch [1041/2500], Step [0/17], d_loss: 0.005938089918345213, g_loss: 8.302654266357422\n",
            "Epoch [1042/2500], Step [0/17], d_loss: 0.014918851666152477, g_loss: 7.334988594055176\n",
            "Epoch [1043/2500], Step [0/17], d_loss: 0.005592979956418276, g_loss: 8.822067260742188\n",
            "Epoch [1044/2500], Step [0/17], d_loss: 0.008769791573286057, g_loss: 8.00436782836914\n",
            "Epoch [1045/2500], Step [0/17], d_loss: 0.0036651312839239836, g_loss: 8.766883850097656\n",
            "Epoch [1046/2500], Step [0/17], d_loss: 0.003971349447965622, g_loss: 8.083456039428711\n",
            "Epoch [1047/2500], Step [0/17], d_loss: 0.018475361168384552, g_loss: 11.715755462646484\n",
            "Epoch [1048/2500], Step [0/17], d_loss: 0.00666790222749114, g_loss: 7.776602745056152\n",
            "Epoch [1049/2500], Step [0/17], d_loss: 0.013882618397474289, g_loss: 12.824484825134277\n",
            "Epoch [1050/2500], Step [0/17], d_loss: 0.38334593176841736, g_loss: 23.565603256225586\n",
            "Epoch [1051/2500], Step [0/17], d_loss: 0.615269124507904, g_loss: 29.173734664916992\n",
            "Epoch [1052/2500], Step [0/17], d_loss: 0.23552614450454712, g_loss: 21.059778213500977\n",
            "Epoch [1053/2500], Step [0/17], d_loss: 0.10409222543239594, g_loss: 17.679256439208984\n",
            "Epoch [1054/2500], Step [0/17], d_loss: 0.010355816222727299, g_loss: 11.227583885192871\n",
            "Epoch [1055/2500], Step [0/17], d_loss: 0.04748070612549782, g_loss: 12.711946487426758\n",
            "Epoch [1056/2500], Step [0/17], d_loss: 0.16901201009750366, g_loss: 20.62516212463379\n",
            "Epoch [1057/2500], Step [0/17], d_loss: 0.007569892331957817, g_loss: 11.135805130004883\n",
            "Epoch [1058/2500], Step [0/17], d_loss: 0.0045882295817136765, g_loss: 12.505956649780273\n",
            "Epoch [1059/2500], Step [0/17], d_loss: 0.01137472689151764, g_loss: 8.415752410888672\n",
            "Epoch [1060/2500], Step [0/17], d_loss: 0.12225309759378433, g_loss: 20.317716598510742\n",
            "Epoch [1061/2500], Step [0/17], d_loss: 0.011994565837085247, g_loss: 11.413461685180664\n",
            "Epoch [1062/2500], Step [0/17], d_loss: 0.012102743610739708, g_loss: 10.298583030700684\n",
            "Epoch [1063/2500], Step [0/17], d_loss: 0.08721567690372467, g_loss: 12.681571006774902\n",
            "Epoch [1064/2500], Step [0/17], d_loss: 0.01359685231000185, g_loss: 7.636790752410889\n",
            "Epoch [1065/2500], Step [0/17], d_loss: 0.010614773258566856, g_loss: 7.757302761077881\n",
            "Epoch [1066/2500], Step [0/17], d_loss: 0.0308912955224514, g_loss: 7.211441516876221\n",
            "Epoch [1067/2500], Step [0/17], d_loss: 0.027242502197623253, g_loss: 9.131840705871582\n",
            "Epoch [1068/2500], Step [0/17], d_loss: 0.017078887671232224, g_loss: 7.666866779327393\n",
            "Epoch [1069/2500], Step [0/17], d_loss: 0.008421104401350021, g_loss: 8.074475288391113\n",
            "Epoch [1070/2500], Step [0/17], d_loss: 0.008857806213200092, g_loss: 8.324825286865234\n",
            "Epoch [1071/2500], Step [0/17], d_loss: 0.018298987299203873, g_loss: 9.360332489013672\n",
            "Epoch [1072/2500], Step [0/17], d_loss: 0.00987943448126316, g_loss: 9.595447540283203\n",
            "Epoch [1073/2500], Step [0/17], d_loss: 0.006777082569897175, g_loss: 9.062106132507324\n",
            "Epoch [1074/2500], Step [0/17], d_loss: 0.018772423267364502, g_loss: 8.445606231689453\n",
            "Epoch [1075/2500], Step [0/17], d_loss: 0.061696212738752365, g_loss: 15.574186325073242\n",
            "Epoch [1076/2500], Step [0/17], d_loss: 0.008632130920886993, g_loss: 8.733587265014648\n",
            "Epoch [1077/2500], Step [0/17], d_loss: 0.0075567844323813915, g_loss: 7.90543270111084\n",
            "Epoch [1078/2500], Step [0/17], d_loss: 0.01685444265604019, g_loss: 8.003790855407715\n",
            "Epoch [1079/2500], Step [0/17], d_loss: 0.011030786670744419, g_loss: 8.330611228942871\n",
            "Epoch [1080/2500], Step [0/17], d_loss: 0.008450748398900032, g_loss: 8.71407413482666\n",
            "Epoch [1081/2500], Step [0/17], d_loss: 0.010467898100614548, g_loss: 8.307977676391602\n",
            "Epoch [1082/2500], Step [0/17], d_loss: 0.01266989391297102, g_loss: 7.792545318603516\n",
            "Epoch [1083/2500], Step [0/17], d_loss: 0.010770617984235287, g_loss: 7.358969688415527\n",
            "Epoch [1084/2500], Step [0/17], d_loss: 0.0031079400796443224, g_loss: 8.582351684570312\n",
            "Epoch [1085/2500], Step [0/17], d_loss: 0.009972700849175453, g_loss: 8.367219924926758\n",
            "Epoch [1086/2500], Step [0/17], d_loss: 0.016120189800858498, g_loss: 8.486288070678711\n",
            "Epoch [1087/2500], Step [0/17], d_loss: 0.02910337969660759, g_loss: 11.41713809967041\n",
            "Epoch [1088/2500], Step [0/17], d_loss: 0.0023988294415175915, g_loss: 8.41341781616211\n",
            "Epoch [1089/2500], Step [0/17], d_loss: 0.01314952690154314, g_loss: 7.640695571899414\n",
            "Epoch [1090/2500], Step [0/17], d_loss: 0.0037570574786514044, g_loss: 8.613481521606445\n",
            "Epoch [1091/2500], Step [0/17], d_loss: 0.009764918126165867, g_loss: 9.018630981445312\n",
            "Epoch [1092/2500], Step [0/17], d_loss: 0.010946806520223618, g_loss: 7.602488040924072\n",
            "Epoch [1093/2500], Step [0/17], d_loss: 0.003198990598320961, g_loss: 8.489730834960938\n",
            "Epoch [1094/2500], Step [0/17], d_loss: 0.01547295693308115, g_loss: 7.615539073944092\n",
            "Epoch [1095/2500], Step [0/17], d_loss: 0.006345516536384821, g_loss: 8.099286079406738\n",
            "Epoch [1096/2500], Step [0/17], d_loss: 0.009848159737884998, g_loss: 8.333988189697266\n",
            "Epoch [1097/2500], Step [0/17], d_loss: 0.008266869932413101, g_loss: 8.070240020751953\n",
            "Epoch [1098/2500], Step [0/17], d_loss: 0.003470826894044876, g_loss: 8.446382522583008\n",
            "Epoch [1099/2500], Step [0/17], d_loss: 0.0027247206307947636, g_loss: 9.504819869995117\n",
            "Epoch [1100/2500], Step [0/17], d_loss: 0.006636893376708031, g_loss: 13.72015380859375\n",
            "Epoch [1101/2500], Step [0/17], d_loss: 0.009499579668045044, g_loss: 8.378140449523926\n",
            "Epoch [1102/2500], Step [0/17], d_loss: 0.008500011637806892, g_loss: 8.016483306884766\n",
            "Epoch [1103/2500], Step [0/17], d_loss: 0.005322596523910761, g_loss: 8.377044677734375\n",
            "Epoch [1104/2500], Step [0/17], d_loss: 0.006303591188043356, g_loss: 7.95072078704834\n",
            "Epoch [1105/2500], Step [0/17], d_loss: 0.002690688008442521, g_loss: 9.042901992797852\n",
            "Epoch [1106/2500], Step [0/17], d_loss: 0.004471001215279102, g_loss: 8.287984848022461\n",
            "Epoch [1107/2500], Step [0/17], d_loss: 0.0044358475133776665, g_loss: 8.327041625976562\n",
            "Epoch [1108/2500], Step [0/17], d_loss: 0.0036883533466607332, g_loss: 8.788887023925781\n",
            "Epoch [1109/2500], Step [0/17], d_loss: 0.009133569896221161, g_loss: 8.341127395629883\n",
            "Epoch [1110/2500], Step [0/17], d_loss: 0.001588198123499751, g_loss: 9.741270065307617\n",
            "Epoch [1111/2500], Step [0/17], d_loss: 0.004396965727210045, g_loss: 8.369264602661133\n",
            "Epoch [1112/2500], Step [0/17], d_loss: 0.005118552129715681, g_loss: 11.65095329284668\n",
            "Epoch [1113/2500], Step [0/17], d_loss: 0.009576043114066124, g_loss: 9.705941200256348\n",
            "Epoch [1114/2500], Step [0/17], d_loss: 0.01070240419358015, g_loss: 8.661548614501953\n",
            "Epoch [1115/2500], Step [0/17], d_loss: 0.003160701831802726, g_loss: 12.370038986206055\n",
            "Epoch [1116/2500], Step [0/17], d_loss: 0.003636313369497657, g_loss: 8.096794128417969\n",
            "Epoch [1117/2500], Step [0/17], d_loss: 0.08593224734067917, g_loss: 28.145862579345703\n",
            "Epoch [1118/2500], Step [0/17], d_loss: 0.007500403095036745, g_loss: 25.644617080688477\n",
            "Epoch [1119/2500], Step [0/17], d_loss: 0.009482113644480705, g_loss: 13.78523063659668\n",
            "Epoch [1120/2500], Step [0/17], d_loss: 0.00434315949678421, g_loss: 12.792497634887695\n",
            "Epoch [1121/2500], Step [0/17], d_loss: 0.00232337461784482, g_loss: 10.394269943237305\n",
            "Epoch [1122/2500], Step [0/17], d_loss: 0.011917376890778542, g_loss: 9.278654098510742\n",
            "Epoch [1123/2500], Step [0/17], d_loss: 0.004475046414881945, g_loss: 8.26519775390625\n",
            "Epoch [1124/2500], Step [0/17], d_loss: 0.0020528710447251797, g_loss: 9.415788650512695\n",
            "Epoch [1125/2500], Step [0/17], d_loss: 0.0012634489685297012, g_loss: 13.957537651062012\n",
            "Epoch [1126/2500], Step [0/17], d_loss: 0.03143923357129097, g_loss: 16.521360397338867\n",
            "Epoch [1127/2500], Step [0/17], d_loss: 0.00859510712325573, g_loss: 8.795377731323242\n",
            "Epoch [1128/2500], Step [0/17], d_loss: 0.0016986692789942026, g_loss: 9.297317504882812\n",
            "Epoch [1129/2500], Step [0/17], d_loss: 0.007145971059799194, g_loss: 9.552000045776367\n",
            "Epoch [1130/2500], Step [0/17], d_loss: 0.04458842799067497, g_loss: 18.471343994140625\n",
            "Epoch [1131/2500], Step [0/17], d_loss: 0.003570685861632228, g_loss: 9.477521896362305\n",
            "Epoch [1132/2500], Step [0/17], d_loss: 0.004869114141911268, g_loss: 7.752161979675293\n",
            "Epoch [1133/2500], Step [0/17], d_loss: 0.005067128688097, g_loss: 8.776338577270508\n",
            "Epoch [1134/2500], Step [0/17], d_loss: 0.004068128298968077, g_loss: 9.82223892211914\n",
            "Epoch [1135/2500], Step [0/17], d_loss: 0.004938990343362093, g_loss: 8.231720924377441\n",
            "Epoch [1136/2500], Step [0/17], d_loss: 0.003426190232858062, g_loss: 8.722667694091797\n",
            "Epoch [1137/2500], Step [0/17], d_loss: 0.0034698504023253918, g_loss: 9.388068199157715\n",
            "Epoch [1138/2500], Step [0/17], d_loss: 0.003148005111142993, g_loss: 8.858884811401367\n",
            "Epoch [1139/2500], Step [0/17], d_loss: 0.0017492348561063409, g_loss: 9.381338119506836\n",
            "Epoch [1140/2500], Step [0/17], d_loss: 0.004170194733887911, g_loss: 9.436964988708496\n",
            "Epoch [1141/2500], Step [0/17], d_loss: 0.0026736119762063026, g_loss: 8.372489929199219\n",
            "Epoch [1142/2500], Step [0/17], d_loss: 0.00406687892973423, g_loss: 8.321044921875\n",
            "Epoch [1143/2500], Step [0/17], d_loss: 0.004775707609951496, g_loss: 8.355344772338867\n",
            "Epoch [1144/2500], Step [0/17], d_loss: 0.0027656182646751404, g_loss: 8.882671356201172\n",
            "Epoch [1145/2500], Step [0/17], d_loss: 0.0031384476460516453, g_loss: 8.526618957519531\n",
            "Epoch [1146/2500], Step [0/17], d_loss: 0.008295571431517601, g_loss: 10.958751678466797\n",
            "Epoch [1147/2500], Step [0/17], d_loss: 0.002120440360158682, g_loss: 14.01270866394043\n",
            "Epoch [1148/2500], Step [0/17], d_loss: 0.31243759393692017, g_loss: 31.15070152282715\n",
            "Epoch [1149/2500], Step [0/17], d_loss: 0.6980653405189514, g_loss: 12.423093795776367\n",
            "Epoch [1150/2500], Step [0/17], d_loss: 0.34507817029953003, g_loss: 13.566515922546387\n",
            "Epoch [1151/2500], Step [0/17], d_loss: 0.9388297200202942, g_loss: 17.833911895751953\n",
            "Epoch [1152/2500], Step [0/17], d_loss: 0.16382983326911926, g_loss: 11.265174865722656\n",
            "Epoch [1153/2500], Step [0/17], d_loss: 0.06285776197910309, g_loss: 16.486621856689453\n",
            "Epoch [1154/2500], Step [0/17], d_loss: 0.11582990735769272, g_loss: 15.903951644897461\n",
            "Epoch [1155/2500], Step [0/17], d_loss: 0.3009757995605469, g_loss: 12.533833503723145\n",
            "Epoch [1156/2500], Step [0/17], d_loss: 0.05766715854406357, g_loss: 11.823225021362305\n",
            "Epoch [1157/2500], Step [0/17], d_loss: 0.06911434978246689, g_loss: 26.29452896118164\n",
            "Epoch [1158/2500], Step [0/17], d_loss: 0.059089306741952896, g_loss: 11.156118392944336\n",
            "Epoch [1159/2500], Step [0/17], d_loss: 0.01339292898774147, g_loss: 11.487080574035645\n",
            "Epoch [1160/2500], Step [0/17], d_loss: 0.05427083373069763, g_loss: 10.7677001953125\n",
            "Epoch [1161/2500], Step [0/17], d_loss: 0.015382178127765656, g_loss: 10.95019817352295\n",
            "Epoch [1162/2500], Step [0/17], d_loss: 0.11714819818735123, g_loss: 9.486875534057617\n",
            "Epoch [1163/2500], Step [0/17], d_loss: 0.03745241463184357, g_loss: 9.872495651245117\n",
            "Epoch [1164/2500], Step [0/17], d_loss: 0.03533265367150307, g_loss: 13.256233215332031\n",
            "Epoch [1165/2500], Step [0/17], d_loss: 0.018801484256982803, g_loss: 12.896455764770508\n",
            "Epoch [1166/2500], Step [0/17], d_loss: 0.03947194665670395, g_loss: 8.434381484985352\n",
            "Epoch [1167/2500], Step [0/17], d_loss: 0.03442085534334183, g_loss: 7.957767486572266\n",
            "Epoch [1168/2500], Step [0/17], d_loss: 0.018948055803775787, g_loss: 8.16798210144043\n",
            "Epoch [1169/2500], Step [0/17], d_loss: 0.02360554039478302, g_loss: 7.121078014373779\n",
            "Epoch [1170/2500], Step [0/17], d_loss: 0.03213725984096527, g_loss: 8.057244300842285\n",
            "Epoch [1171/2500], Step [0/17], d_loss: 0.05012160912156105, g_loss: 8.44705581665039\n",
            "Epoch [1172/2500], Step [0/17], d_loss: 0.029159821569919586, g_loss: 8.105634689331055\n",
            "Epoch [1173/2500], Step [0/17], d_loss: 0.02100449800491333, g_loss: 7.699499607086182\n",
            "Epoch [1174/2500], Step [0/17], d_loss: 0.018221784383058548, g_loss: 8.006522178649902\n",
            "Epoch [1175/2500], Step [0/17], d_loss: 0.021967675536870956, g_loss: 7.444619655609131\n",
            "Epoch [1176/2500], Step [0/17], d_loss: 0.00890483520925045, g_loss: 7.969406604766846\n",
            "Epoch [1177/2500], Step [0/17], d_loss: 0.027606312185525894, g_loss: 7.483495712280273\n",
            "Epoch [1178/2500], Step [0/17], d_loss: 0.014024654403328896, g_loss: 9.374299049377441\n",
            "Epoch [1179/2500], Step [0/17], d_loss: 0.01055363193154335, g_loss: 6.950060844421387\n",
            "Epoch [1180/2500], Step [0/17], d_loss: 0.043103545904159546, g_loss: 8.576112747192383\n",
            "Epoch [1181/2500], Step [0/17], d_loss: 0.013402139768004417, g_loss: 8.375203132629395\n",
            "Epoch [1182/2500], Step [0/17], d_loss: 0.023860886693000793, g_loss: 7.260735511779785\n",
            "Epoch [1183/2500], Step [0/17], d_loss: 0.02464113011956215, g_loss: 8.38492488861084\n",
            "Epoch [1184/2500], Step [0/17], d_loss: 0.012019442394375801, g_loss: 7.6721601486206055\n",
            "Epoch [1185/2500], Step [0/17], d_loss: 0.028076719492673874, g_loss: 7.913943767547607\n",
            "Epoch [1186/2500], Step [0/17], d_loss: 0.01464755181223154, g_loss: 6.9414381980896\n",
            "Epoch [1187/2500], Step [0/17], d_loss: 0.024413779377937317, g_loss: 6.671551704406738\n",
            "Epoch [1188/2500], Step [0/17], d_loss: 0.020291756838560104, g_loss: 7.583474159240723\n",
            "Epoch [1189/2500], Step [0/17], d_loss: 0.04015311226248741, g_loss: 7.804988861083984\n",
            "Epoch [1190/2500], Step [0/17], d_loss: 0.015303630381822586, g_loss: 8.328438758850098\n",
            "Epoch [1191/2500], Step [0/17], d_loss: 0.023199260234832764, g_loss: 7.608567237854004\n",
            "Epoch [1192/2500], Step [0/17], d_loss: 0.020377373322844505, g_loss: 7.476137161254883\n",
            "Epoch [1193/2500], Step [0/17], d_loss: 0.007098151370882988, g_loss: 8.017411231994629\n",
            "Epoch [1194/2500], Step [0/17], d_loss: 0.012112344615161419, g_loss: 8.255884170532227\n",
            "Epoch [1195/2500], Step [0/17], d_loss: 0.008454271592199802, g_loss: 7.665076732635498\n",
            "Epoch [1196/2500], Step [0/17], d_loss: 0.003257769625633955, g_loss: 8.607166290283203\n",
            "Epoch [1197/2500], Step [0/17], d_loss: 0.004201932344585657, g_loss: 8.747403144836426\n",
            "Epoch [1198/2500], Step [0/17], d_loss: 0.01630689576268196, g_loss: 8.233259201049805\n",
            "Epoch [1199/2500], Step [0/17], d_loss: 0.031807154417037964, g_loss: 9.084253311157227\n",
            "Epoch [1200/2500], Step [0/17], d_loss: 0.014435414224863052, g_loss: 8.724213600158691\n",
            "Epoch [1201/2500], Step [0/17], d_loss: 0.011558939702808857, g_loss: 6.936569690704346\n",
            "Epoch [1202/2500], Step [0/17], d_loss: 0.006724034436047077, g_loss: 7.957503795623779\n",
            "Epoch [1203/2500], Step [0/17], d_loss: 0.009237175807356834, g_loss: 7.322576522827148\n",
            "Epoch [1204/2500], Step [0/17], d_loss: 0.011077109724283218, g_loss: 7.735983371734619\n",
            "Epoch [1205/2500], Step [0/17], d_loss: 0.04260195791721344, g_loss: 10.049081802368164\n",
            "Epoch [1206/2500], Step [0/17], d_loss: 0.007741857320070267, g_loss: 7.879300594329834\n",
            "Epoch [1207/2500], Step [0/17], d_loss: 0.005023969803005457, g_loss: 8.732110977172852\n",
            "Epoch [1208/2500], Step [0/17], d_loss: 0.011966967955231667, g_loss: 8.547121047973633\n",
            "Epoch [1209/2500], Step [0/17], d_loss: 0.015064457431435585, g_loss: 8.315339088439941\n",
            "Epoch [1210/2500], Step [0/17], d_loss: 0.007394406944513321, g_loss: 11.267739295959473\n",
            "Epoch [1211/2500], Step [0/17], d_loss: 0.013739603571593761, g_loss: 8.059391975402832\n",
            "Epoch [1212/2500], Step [0/17], d_loss: 0.00887843407690525, g_loss: 7.494851112365723\n",
            "Epoch [1213/2500], Step [0/17], d_loss: 0.006645842920988798, g_loss: 7.73946475982666\n",
            "Epoch [1214/2500], Step [0/17], d_loss: 0.006541446782648563, g_loss: 8.206542015075684\n",
            "Epoch [1215/2500], Step [0/17], d_loss: 0.008378805592656136, g_loss: 8.085142135620117\n",
            "Epoch [1216/2500], Step [0/17], d_loss: 0.003206636058166623, g_loss: 8.330873489379883\n",
            "Epoch [1217/2500], Step [0/17], d_loss: 0.012456728145480156, g_loss: 7.278374671936035\n",
            "Epoch [1218/2500], Step [0/17], d_loss: 0.015456855297088623, g_loss: 7.310563564300537\n",
            "Epoch [1219/2500], Step [0/17], d_loss: 0.024197423830628395, g_loss: 7.334720611572266\n",
            "Epoch [1220/2500], Step [0/17], d_loss: 0.007136598229408264, g_loss: 7.558858394622803\n",
            "Epoch [1221/2500], Step [0/17], d_loss: 0.004955318756401539, g_loss: 7.854033946990967\n",
            "Epoch [1222/2500], Step [0/17], d_loss: 0.012996290810406208, g_loss: 8.367883682250977\n",
            "Epoch [1223/2500], Step [0/17], d_loss: 0.004306754097342491, g_loss: 8.178930282592773\n",
            "Epoch [1224/2500], Step [0/17], d_loss: 0.01299112755805254, g_loss: 6.684574127197266\n",
            "Epoch [1225/2500], Step [0/17], d_loss: 0.020320681855082512, g_loss: 19.98077392578125\n",
            "Epoch [1226/2500], Step [0/17], d_loss: 0.010921422392129898, g_loss: 12.35395336151123\n",
            "Epoch [1227/2500], Step [0/17], d_loss: 0.015379775315523148, g_loss: 8.101167678833008\n",
            "Epoch [1228/2500], Step [0/17], d_loss: 0.020350608974695206, g_loss: 9.002603530883789\n",
            "Epoch [1229/2500], Step [0/17], d_loss: 0.26793140172958374, g_loss: 44.38775634765625\n",
            "Epoch [1230/2500], Step [0/17], d_loss: 0.007039344869554043, g_loss: 22.65961456298828\n",
            "Epoch [1231/2500], Step [0/17], d_loss: 0.01811128482222557, g_loss: 25.107807159423828\n",
            "Epoch [1232/2500], Step [0/17], d_loss: 0.0031655975617468357, g_loss: 11.884048461914062\n",
            "Epoch [1233/2500], Step [0/17], d_loss: 0.035674773156642914, g_loss: 9.584403991699219\n",
            "Epoch [1234/2500], Step [0/17], d_loss: 0.032260049134492874, g_loss: 8.42622184753418\n",
            "Epoch [1235/2500], Step [0/17], d_loss: 0.022477053105831146, g_loss: 7.844819068908691\n",
            "Epoch [1236/2500], Step [0/17], d_loss: 0.027196425944566727, g_loss: 9.74167251586914\n",
            "Epoch [1237/2500], Step [0/17], d_loss: 0.0025212462060153484, g_loss: 11.777606964111328\n",
            "Epoch [1238/2500], Step [0/17], d_loss: 0.003299286123365164, g_loss: 18.607194900512695\n",
            "Epoch [1239/2500], Step [0/17], d_loss: 0.0050532398745417595, g_loss: 11.548652648925781\n",
            "Epoch [1240/2500], Step [0/17], d_loss: 0.003906634636223316, g_loss: 15.15848159790039\n",
            "Epoch [1241/2500], Step [0/17], d_loss: 0.01330470759421587, g_loss: 8.873502731323242\n",
            "Epoch [1242/2500], Step [0/17], d_loss: 0.01069684512913227, g_loss: 22.63235855102539\n",
            "Epoch [1243/2500], Step [0/17], d_loss: 0.012569066137075424, g_loss: 10.415414810180664\n",
            "Epoch [1244/2500], Step [0/17], d_loss: 0.026644814759492874, g_loss: 9.156641006469727\n",
            "Epoch [1245/2500], Step [0/17], d_loss: 0.014478597790002823, g_loss: 7.658917427062988\n",
            "Epoch [1246/2500], Step [0/17], d_loss: 0.02991362288594246, g_loss: 10.724130630493164\n",
            "Epoch [1247/2500], Step [0/17], d_loss: 0.012526636011898518, g_loss: 7.763113021850586\n",
            "Epoch [1248/2500], Step [0/17], d_loss: 0.016970019787549973, g_loss: 8.12158203125\n",
            "Epoch [1249/2500], Step [0/17], d_loss: 0.004900879226624966, g_loss: 8.244796752929688\n",
            "Epoch [1250/2500], Step [0/17], d_loss: 0.00561491446569562, g_loss: 7.751011371612549\n",
            "Epoch [1251/2500], Step [0/17], d_loss: 0.012342988513410091, g_loss: 7.736886024475098\n",
            "Epoch [1252/2500], Step [0/17], d_loss: 0.004295097663998604, g_loss: 8.321022033691406\n",
            "Epoch [1253/2500], Step [0/17], d_loss: 0.0040580760687589645, g_loss: 7.975628852844238\n",
            "Epoch [1254/2500], Step [0/17], d_loss: 0.03326822444796562, g_loss: 14.512077331542969\n",
            "Epoch [1255/2500], Step [0/17], d_loss: 0.006260574795305729, g_loss: 12.886900901794434\n",
            "Epoch [1256/2500], Step [0/17], d_loss: 0.00375546352006495, g_loss: 29.76828956604004\n",
            "Epoch [1257/2500], Step [0/17], d_loss: 0.03455036133527756, g_loss: 14.942468643188477\n",
            "Epoch [1258/2500], Step [0/17], d_loss: 0.005602641962468624, g_loss: 12.33739948272705\n",
            "Epoch [1259/2500], Step [0/17], d_loss: 0.04286511242389679, g_loss: 14.41123104095459\n",
            "Epoch [1260/2500], Step [0/17], d_loss: 0.005747279617935419, g_loss: 14.285018920898438\n",
            "Epoch [1261/2500], Step [0/17], d_loss: 0.007822593674063683, g_loss: 18.14667510986328\n",
            "Epoch [1262/2500], Step [0/17], d_loss: 0.004462142009288073, g_loss: 26.062789916992188\n",
            "Epoch [1263/2500], Step [0/17], d_loss: 0.003790436778217554, g_loss: 13.183297157287598\n",
            "Epoch [1264/2500], Step [0/17], d_loss: 0.0047437530010938644, g_loss: 10.572835922241211\n",
            "Epoch [1265/2500], Step [0/17], d_loss: 0.03943275660276413, g_loss: 21.690677642822266\n",
            "Epoch [1266/2500], Step [0/17], d_loss: 0.0029987641610205173, g_loss: 16.215927124023438\n",
            "Epoch [1267/2500], Step [0/17], d_loss: 0.016064435243606567, g_loss: 9.338417053222656\n",
            "Epoch [1268/2500], Step [0/17], d_loss: 0.0022654449567198753, g_loss: 9.617349624633789\n",
            "Epoch [1269/2500], Step [0/17], d_loss: 0.006620148662477732, g_loss: 8.857475280761719\n",
            "Epoch [1270/2500], Step [0/17], d_loss: 0.007881704717874527, g_loss: 9.176225662231445\n",
            "Epoch [1271/2500], Step [0/17], d_loss: 0.01888427324593067, g_loss: 8.146446228027344\n",
            "Epoch [1272/2500], Step [0/17], d_loss: 0.003232535207644105, g_loss: 8.217727661132812\n",
            "Epoch [1273/2500], Step [0/17], d_loss: 0.006242185365408659, g_loss: 7.788501262664795\n",
            "Epoch [1274/2500], Step [0/17], d_loss: 0.00795686710625887, g_loss: 11.398656845092773\n",
            "Epoch [1275/2500], Step [0/17], d_loss: 0.006041203625500202, g_loss: 7.981757164001465\n",
            "Epoch [1276/2500], Step [0/17], d_loss: 0.004337332211434841, g_loss: 7.820319652557373\n",
            "Epoch [1277/2500], Step [0/17], d_loss: 0.002637971192598343, g_loss: 10.254039764404297\n",
            "Epoch [1278/2500], Step [0/17], d_loss: 0.006247684825211763, g_loss: 8.078859329223633\n",
            "Epoch [1279/2500], Step [0/17], d_loss: 0.003897636430338025, g_loss: 8.392341613769531\n",
            "Epoch [1280/2500], Step [0/17], d_loss: 0.008603805676102638, g_loss: 8.554986000061035\n",
            "Epoch [1281/2500], Step [0/17], d_loss: 0.014265468344092369, g_loss: 18.185285568237305\n",
            "Epoch [1282/2500], Step [0/17], d_loss: 0.0030158155132085085, g_loss: 9.42903995513916\n",
            "Epoch [1283/2500], Step [0/17], d_loss: 0.0019357858691364527, g_loss: 9.197736740112305\n",
            "Epoch [1284/2500], Step [0/17], d_loss: 0.004639646969735622, g_loss: 8.694219589233398\n",
            "Epoch [1285/2500], Step [0/17], d_loss: 0.0058598266914486885, g_loss: 8.096134185791016\n",
            "Epoch [1286/2500], Step [0/17], d_loss: 0.004662244580686092, g_loss: 7.87070894241333\n",
            "Epoch [1287/2500], Step [0/17], d_loss: 0.007025009021162987, g_loss: 8.18498420715332\n",
            "Epoch [1288/2500], Step [0/17], d_loss: 0.008894755505025387, g_loss: 8.685338973999023\n",
            "Epoch [1289/2500], Step [0/17], d_loss: 0.00550166517496109, g_loss: 8.717093467712402\n",
            "Epoch [1290/2500], Step [0/17], d_loss: 0.0013755855616182089, g_loss: 12.387715339660645\n",
            "Epoch [1291/2500], Step [0/17], d_loss: 0.3499023914337158, g_loss: 31.20937156677246\n",
            "Epoch [1292/2500], Step [0/17], d_loss: 0.4249839186668396, g_loss: 46.80001449584961\n",
            "Epoch [1293/2500], Step [0/17], d_loss: 1.4730205535888672, g_loss: 35.28721618652344\n",
            "Epoch [1294/2500], Step [0/17], d_loss: 3.1106760501861572, g_loss: 62.35472106933594\n",
            "Epoch [1295/2500], Step [0/17], d_loss: 0.3136872351169586, g_loss: 17.417091369628906\n",
            "Epoch [1296/2500], Step [0/17], d_loss: 0.3028952181339264, g_loss: 25.36890983581543\n",
            "Epoch [1297/2500], Step [0/17], d_loss: 0.12484899163246155, g_loss: 19.551441192626953\n",
            "Epoch [1298/2500], Step [0/17], d_loss: 0.11151429265737534, g_loss: 19.352310180664062\n",
            "Epoch [1299/2500], Step [0/17], d_loss: 0.29238516092300415, g_loss: 20.65691375732422\n",
            "Epoch [1300/2500], Step [0/17], d_loss: 1.3389803171157837, g_loss: 21.67752456665039\n",
            "Epoch [1301/2500], Step [0/17], d_loss: 0.5287495851516724, g_loss: 26.515186309814453\n",
            "Epoch [1302/2500], Step [0/17], d_loss: 0.09246505796909332, g_loss: 23.741165161132812\n",
            "Epoch [1303/2500], Step [0/17], d_loss: 0.2112724632024765, g_loss: 17.470699310302734\n",
            "Epoch [1304/2500], Step [0/17], d_loss: 0.05260152742266655, g_loss: 13.224149703979492\n",
            "Epoch [1305/2500], Step [0/17], d_loss: 0.010452755726873875, g_loss: 8.98497486114502\n",
            "Epoch [1306/2500], Step [0/17], d_loss: 0.40681928396224976, g_loss: 17.959407806396484\n",
            "Epoch [1307/2500], Step [0/17], d_loss: 0.03931840509176254, g_loss: 9.995928764343262\n",
            "Epoch [1308/2500], Step [0/17], d_loss: 0.033702824264764786, g_loss: 22.53404998779297\n",
            "Epoch [1309/2500], Step [0/17], d_loss: 0.012655653059482574, g_loss: 10.310138702392578\n",
            "Epoch [1310/2500], Step [0/17], d_loss: 0.02831973135471344, g_loss: 10.327682495117188\n",
            "Epoch [1311/2500], Step [0/17], d_loss: 0.03674173355102539, g_loss: 8.352884292602539\n",
            "Epoch [1312/2500], Step [0/17], d_loss: 0.011370419524610043, g_loss: 8.203173637390137\n",
            "Epoch [1313/2500], Step [0/17], d_loss: 0.02654978632926941, g_loss: 8.139598846435547\n",
            "Epoch [1314/2500], Step [0/17], d_loss: 0.06646007299423218, g_loss: 9.59029483795166\n",
            "Epoch [1315/2500], Step [0/17], d_loss: 0.049443796277046204, g_loss: 7.982509136199951\n",
            "Epoch [1316/2500], Step [0/17], d_loss: 0.07772889733314514, g_loss: 15.968957901000977\n",
            "Epoch [1317/2500], Step [0/17], d_loss: 0.033772677183151245, g_loss: 12.971794128417969\n",
            "Epoch [1318/2500], Step [0/17], d_loss: 0.017744198441505432, g_loss: 24.404491424560547\n",
            "Epoch [1319/2500], Step [0/17], d_loss: 0.03447529301047325, g_loss: 11.296529769897461\n",
            "Epoch [1320/2500], Step [0/17], d_loss: 0.04070020094513893, g_loss: 8.871331214904785\n",
            "Epoch [1321/2500], Step [0/17], d_loss: 0.016238873824477196, g_loss: 15.436805725097656\n",
            "Epoch [1322/2500], Step [0/17], d_loss: 0.050296660512685776, g_loss: 7.418537139892578\n",
            "Epoch [1323/2500], Step [0/17], d_loss: 0.019630275666713715, g_loss: 7.090002059936523\n",
            "Epoch [1324/2500], Step [0/17], d_loss: 0.030713997781276703, g_loss: 7.877695560455322\n",
            "Epoch [1325/2500], Step [0/17], d_loss: 0.012028357945382595, g_loss: 7.867627143859863\n",
            "Epoch [1326/2500], Step [0/17], d_loss: 0.006288886070251465, g_loss: 10.545343399047852\n",
            "Epoch [1327/2500], Step [0/17], d_loss: 0.01553531363606453, g_loss: 7.335141181945801\n",
            "Epoch [1328/2500], Step [0/17], d_loss: 0.012292216531932354, g_loss: 7.770471096038818\n",
            "Epoch [1329/2500], Step [0/17], d_loss: 0.018081899732351303, g_loss: 7.613786220550537\n",
            "Epoch [1330/2500], Step [0/17], d_loss: 0.0496026948094368, g_loss: 9.145694732666016\n",
            "Epoch [1331/2500], Step [0/17], d_loss: 0.02908753603696823, g_loss: 8.764808654785156\n",
            "Epoch [1332/2500], Step [0/17], d_loss: 0.0016166182467713952, g_loss: 10.042722702026367\n",
            "Epoch [1333/2500], Step [0/17], d_loss: 0.1041182354092598, g_loss: 18.624229431152344\n",
            "Epoch [1334/2500], Step [0/17], d_loss: 0.033875722438097, g_loss: 12.502115249633789\n",
            "Epoch [1335/2500], Step [0/17], d_loss: 0.006285893730819225, g_loss: 7.979867458343506\n",
            "Epoch [1336/2500], Step [0/17], d_loss: 0.01012678723782301, g_loss: 8.04902458190918\n",
            "Epoch [1337/2500], Step [0/17], d_loss: 0.02613450214266777, g_loss: 7.540773391723633\n",
            "Epoch [1338/2500], Step [0/17], d_loss: 0.012248927727341652, g_loss: 7.563096046447754\n",
            "Epoch [1339/2500], Step [0/17], d_loss: 0.00653902068734169, g_loss: 7.980039119720459\n",
            "Epoch [1340/2500], Step [0/17], d_loss: 0.0379066988825798, g_loss: 17.075237274169922\n",
            "Epoch [1341/2500], Step [0/17], d_loss: 0.010736076161265373, g_loss: 12.064848899841309\n",
            "Epoch [1342/2500], Step [0/17], d_loss: 0.009682697243988514, g_loss: 8.736571311950684\n",
            "Epoch [1343/2500], Step [0/17], d_loss: 0.010101343505084515, g_loss: 7.2829790115356445\n",
            "Epoch [1344/2500], Step [0/17], d_loss: 0.015789946541190147, g_loss: 7.353039741516113\n",
            "Epoch [1345/2500], Step [0/17], d_loss: 0.007016414776444435, g_loss: 7.130937576293945\n",
            "Epoch [1346/2500], Step [0/17], d_loss: 0.012045981362462044, g_loss: 7.549416542053223\n",
            "Epoch [1347/2500], Step [0/17], d_loss: 0.013032392598688602, g_loss: 6.646254539489746\n",
            "Epoch [1348/2500], Step [0/17], d_loss: 0.00921723060309887, g_loss: 7.350902557373047\n",
            "Epoch [1349/2500], Step [0/17], d_loss: 0.010382117703557014, g_loss: 7.562582969665527\n",
            "Epoch [1350/2500], Step [0/17], d_loss: 0.00255484483204782, g_loss: 29.19078254699707\n",
            "Epoch [1351/2500], Step [0/17], d_loss: 0.004809868056327105, g_loss: 15.828827857971191\n",
            "Epoch [1352/2500], Step [0/17], d_loss: 0.006713061593472958, g_loss: 7.609756946563721\n",
            "Epoch [1353/2500], Step [0/17], d_loss: 0.016605818644165993, g_loss: 7.6129655838012695\n",
            "Epoch [1354/2500], Step [0/17], d_loss: 0.0028625878039747477, g_loss: 9.589618682861328\n",
            "Epoch [1355/2500], Step [0/17], d_loss: 0.017624765634536743, g_loss: 8.403636932373047\n",
            "Epoch [1356/2500], Step [0/17], d_loss: 0.00742202065885067, g_loss: 8.163848876953125\n",
            "Epoch [1357/2500], Step [0/17], d_loss: 0.00446409173309803, g_loss: 9.144420623779297\n",
            "Epoch [1358/2500], Step [0/17], d_loss: 0.019902143627405167, g_loss: 8.521236419677734\n",
            "Epoch [1359/2500], Step [0/17], d_loss: 0.01482384279370308, g_loss: 10.216800689697266\n",
            "Epoch [1360/2500], Step [0/17], d_loss: 0.005242607556283474, g_loss: 7.325931549072266\n",
            "Epoch [1361/2500], Step [0/17], d_loss: 0.21003669500350952, g_loss: 32.09526062011719\n",
            "Epoch [1362/2500], Step [0/17], d_loss: 0.0012250563595443964, g_loss: 25.44275665283203\n",
            "Epoch [1363/2500], Step [0/17], d_loss: 0.03144378587603569, g_loss: 12.197669982910156\n",
            "Epoch [1364/2500], Step [0/17], d_loss: 0.009252899326384068, g_loss: 7.5064697265625\n",
            "Epoch [1365/2500], Step [0/17], d_loss: 0.023352107033133507, g_loss: 9.632859230041504\n",
            "Epoch [1366/2500], Step [0/17], d_loss: 0.005176148843020201, g_loss: 8.078060150146484\n",
            "Epoch [1367/2500], Step [0/17], d_loss: 0.008545983582735062, g_loss: 8.670605659484863\n",
            "Epoch [1368/2500], Step [0/17], d_loss: 0.002804338466376066, g_loss: 8.843818664550781\n",
            "Epoch [1369/2500], Step [0/17], d_loss: 0.01731710508465767, g_loss: 8.860889434814453\n",
            "Epoch [1370/2500], Step [0/17], d_loss: 0.008642531931400299, g_loss: 7.129774570465088\n",
            "Epoch [1371/2500], Step [0/17], d_loss: 0.016280241310596466, g_loss: 7.50314474105835\n",
            "Epoch [1372/2500], Step [0/17], d_loss: 0.005217716097831726, g_loss: 19.480085372924805\n",
            "Epoch [1373/2500], Step [0/17], d_loss: 0.016132019460201263, g_loss: 14.175773620605469\n",
            "Epoch [1374/2500], Step [0/17], d_loss: 0.011359026655554771, g_loss: 7.96588659286499\n",
            "Epoch [1375/2500], Step [0/17], d_loss: 0.010306627489626408, g_loss: 8.343472480773926\n",
            "Epoch [1376/2500], Step [0/17], d_loss: 0.006515144370496273, g_loss: 7.7910308837890625\n",
            "Epoch [1377/2500], Step [0/17], d_loss: 0.006768460385501385, g_loss: 7.668830871582031\n",
            "Epoch [1378/2500], Step [0/17], d_loss: 0.0013945659156888723, g_loss: 11.577713012695312\n",
            "Epoch [1379/2500], Step [0/17], d_loss: 0.0047120600938797, g_loss: 7.558929920196533\n",
            "Epoch [1380/2500], Step [0/17], d_loss: 0.011687525548040867, g_loss: 19.452144622802734\n",
            "Epoch [1381/2500], Step [0/17], d_loss: 0.010739975608885288, g_loss: 11.006537437438965\n",
            "Epoch [1382/2500], Step [0/17], d_loss: 0.017344946041703224, g_loss: 15.139056205749512\n",
            "Epoch [1383/2500], Step [0/17], d_loss: 0.018075769767165184, g_loss: 19.778636932373047\n",
            "Epoch [1384/2500], Step [0/17], d_loss: 0.014319594949483871, g_loss: 14.251752853393555\n",
            "Epoch [1385/2500], Step [0/17], d_loss: 0.002807450480759144, g_loss: 8.640950202941895\n",
            "Epoch [1386/2500], Step [0/17], d_loss: 0.02290646731853485, g_loss: 10.757562637329102\n",
            "Epoch [1387/2500], Step [0/17], d_loss: 0.004334918688982725, g_loss: 7.580225944519043\n",
            "Epoch [1388/2500], Step [0/17], d_loss: 0.01489928551018238, g_loss: 9.629497528076172\n",
            "Epoch [1389/2500], Step [0/17], d_loss: 0.010519994422793388, g_loss: 8.834711074829102\n",
            "Epoch [1390/2500], Step [0/17], d_loss: 0.004879292566329241, g_loss: 7.774442672729492\n",
            "Epoch [1391/2500], Step [0/17], d_loss: 0.006620914209634066, g_loss: 8.127731323242188\n",
            "Epoch [1392/2500], Step [0/17], d_loss: 0.02490828186273575, g_loss: 6.049314975738525\n",
            "Epoch [1393/2500], Step [0/17], d_loss: 0.023434074595570564, g_loss: 10.106034278869629\n",
            "Epoch [1394/2500], Step [0/17], d_loss: 0.005366708617657423, g_loss: 12.673942565917969\n",
            "Epoch [1395/2500], Step [0/17], d_loss: 0.00412595272064209, g_loss: 9.11621379852295\n",
            "Epoch [1396/2500], Step [0/17], d_loss: 0.00859168078750372, g_loss: 7.546411037445068\n",
            "Epoch [1397/2500], Step [0/17], d_loss: 0.004068371839821339, g_loss: 8.09805679321289\n",
            "Epoch [1398/2500], Step [0/17], d_loss: 0.005241894628852606, g_loss: 7.653014183044434\n",
            "Epoch [1399/2500], Step [0/17], d_loss: 0.005945215001702309, g_loss: 8.320405960083008\n",
            "Epoch [1400/2500], Step [0/17], d_loss: 0.006340259686112404, g_loss: 9.230599403381348\n",
            "Epoch [1401/2500], Step [0/17], d_loss: 0.022676048800349236, g_loss: 6.282909870147705\n",
            "Epoch [1402/2500], Step [0/17], d_loss: 0.007399809546768665, g_loss: 16.835405349731445\n",
            "Epoch [1403/2500], Step [0/17], d_loss: 0.0038341572508215904, g_loss: 8.349973678588867\n",
            "Epoch [1404/2500], Step [0/17], d_loss: 0.012616478838026524, g_loss: 7.80564022064209\n",
            "Epoch [1405/2500], Step [0/17], d_loss: 0.007046608254313469, g_loss: 9.758356094360352\n",
            "Epoch [1406/2500], Step [0/17], d_loss: 0.02533894218504429, g_loss: 22.205224990844727\n",
            "Epoch [1407/2500], Step [0/17], d_loss: 0.004160316661000252, g_loss: 15.587627410888672\n",
            "Epoch [1408/2500], Step [0/17], d_loss: 0.01098026242107153, g_loss: 8.652042388916016\n",
            "Epoch [1409/2500], Step [0/17], d_loss: 0.006542292423546314, g_loss: 9.42794132232666\n",
            "Epoch [1410/2500], Step [0/17], d_loss: 0.01906678080558777, g_loss: 34.69099426269531\n",
            "Epoch [1411/2500], Step [0/17], d_loss: 0.0078471340239048, g_loss: 13.20106315612793\n",
            "Epoch [1412/2500], Step [0/17], d_loss: 0.0019721905700862408, g_loss: 11.119232177734375\n",
            "Epoch [1413/2500], Step [0/17], d_loss: 0.004597010090947151, g_loss: 8.248786926269531\n",
            "Epoch [1414/2500], Step [0/17], d_loss: 0.006051246542483568, g_loss: 10.9596586227417\n",
            "Epoch [1415/2500], Step [0/17], d_loss: 0.021249903365969658, g_loss: 10.878326416015625\n",
            "Epoch [1416/2500], Step [0/17], d_loss: 0.0059687290340662, g_loss: 8.729938507080078\n",
            "Epoch [1417/2500], Step [0/17], d_loss: 0.003650092286989093, g_loss: 8.374040603637695\n",
            "Epoch [1418/2500], Step [0/17], d_loss: 0.005229644011706114, g_loss: 8.052928924560547\n",
            "Epoch [1419/2500], Step [0/17], d_loss: 0.00841775257140398, g_loss: 8.22553825378418\n",
            "Epoch [1420/2500], Step [0/17], d_loss: 0.0014636381529271603, g_loss: 13.862309455871582\n",
            "Epoch [1421/2500], Step [0/17], d_loss: 0.0013323018793016672, g_loss: 14.419474601745605\n",
            "Epoch [1422/2500], Step [0/17], d_loss: 0.023686233907938004, g_loss: 10.96560001373291\n",
            "Epoch [1423/2500], Step [0/17], d_loss: 0.011028259992599487, g_loss: 9.102834701538086\n",
            "Epoch [1424/2500], Step [0/17], d_loss: 0.007053134962916374, g_loss: 7.601490020751953\n",
            "Epoch [1425/2500], Step [0/17], d_loss: 0.004284783732146025, g_loss: 8.214372634887695\n",
            "Epoch [1426/2500], Step [0/17], d_loss: 0.003721429966390133, g_loss: 8.066338539123535\n",
            "Epoch [1427/2500], Step [0/17], d_loss: 0.017059914767742157, g_loss: 9.015695571899414\n",
            "Epoch [1428/2500], Step [0/17], d_loss: 0.004294819198548794, g_loss: 9.972589492797852\n",
            "Epoch [1429/2500], Step [0/17], d_loss: 0.009362412616610527, g_loss: 7.707472324371338\n",
            "Epoch [1430/2500], Step [0/17], d_loss: 0.004634754732251167, g_loss: 7.869175910949707\n",
            "Epoch [1431/2500], Step [0/17], d_loss: 0.002643638290464878, g_loss: 7.473907470703125\n",
            "Epoch [1432/2500], Step [0/17], d_loss: 0.010517314076423645, g_loss: 9.440065383911133\n",
            "Epoch [1433/2500], Step [0/17], d_loss: 0.007260548882186413, g_loss: 8.72571849822998\n",
            "Epoch [1434/2500], Step [0/17], d_loss: 0.00224194023758173, g_loss: 8.38974666595459\n",
            "Epoch [1435/2500], Step [0/17], d_loss: 0.0015616628807038069, g_loss: 8.53207778930664\n",
            "Epoch [1436/2500], Step [0/17], d_loss: 0.04435041546821594, g_loss: 38.14274597167969\n",
            "Epoch [1437/2500], Step [0/17], d_loss: 0.3438941240310669, g_loss: 30.797399520874023\n",
            "Epoch [1438/2500], Step [0/17], d_loss: 0.1862996220588684, g_loss: 23.427854537963867\n",
            "Epoch [1439/2500], Step [0/17], d_loss: 0.010909676551818848, g_loss: 24.274282455444336\n",
            "Epoch [1440/2500], Step [0/17], d_loss: 0.7989169359207153, g_loss: 24.326562881469727\n",
            "Epoch [1441/2500], Step [0/17], d_loss: 0.6380815505981445, g_loss: 28.81827735900879\n",
            "Epoch [1442/2500], Step [0/17], d_loss: 0.017909251153469086, g_loss: 18.546627044677734\n",
            "Epoch [1443/2500], Step [0/17], d_loss: 0.11462777853012085, g_loss: 13.529647827148438\n",
            "Epoch [1444/2500], Step [0/17], d_loss: 0.23720163106918335, g_loss: 16.970706939697266\n",
            "Epoch [1445/2500], Step [0/17], d_loss: 0.041817404329776764, g_loss: 8.86239242553711\n",
            "Epoch [1446/2500], Step [0/17], d_loss: 0.041362471878528595, g_loss: 9.944267272949219\n",
            "Epoch [1447/2500], Step [0/17], d_loss: 0.006179169751703739, g_loss: 16.938465118408203\n",
            "Epoch [1448/2500], Step [0/17], d_loss: 0.04905529320240021, g_loss: 9.131355285644531\n",
            "Epoch [1449/2500], Step [0/17], d_loss: 0.1205766424536705, g_loss: 9.880144119262695\n",
            "Epoch [1450/2500], Step [0/17], d_loss: 0.0533432774245739, g_loss: 11.038782119750977\n",
            "Epoch [1451/2500], Step [0/17], d_loss: 0.027780545875430107, g_loss: 7.529098987579346\n",
            "Epoch [1452/2500], Step [0/17], d_loss: 0.018812645226716995, g_loss: 16.260181427001953\n",
            "Epoch [1453/2500], Step [0/17], d_loss: 0.013436679728329182, g_loss: 19.83401107788086\n",
            "Epoch [1454/2500], Step [0/17], d_loss: 0.02324177324771881, g_loss: 8.552108764648438\n",
            "Epoch [1455/2500], Step [0/17], d_loss: 0.02569769136607647, g_loss: 7.761379241943359\n",
            "Epoch [1456/2500], Step [0/17], d_loss: 0.02359877899289131, g_loss: 7.892133712768555\n",
            "Epoch [1457/2500], Step [0/17], d_loss: 0.011249551549553871, g_loss: 7.5474700927734375\n",
            "Epoch [1458/2500], Step [0/17], d_loss: 0.00450288038700819, g_loss: 15.66301155090332\n",
            "Epoch [1459/2500], Step [0/17], d_loss: 0.019000699743628502, g_loss: 8.422393798828125\n",
            "Epoch [1460/2500], Step [0/17], d_loss: 0.008074040524661541, g_loss: 9.738510131835938\n",
            "Epoch [1461/2500], Step [0/17], d_loss: 0.014343940652906895, g_loss: 11.34097671508789\n",
            "Epoch [1462/2500], Step [0/17], d_loss: 0.022113550454378128, g_loss: 9.548643112182617\n",
            "Epoch [1463/2500], Step [0/17], d_loss: 0.026330076158046722, g_loss: 7.75812292098999\n",
            "Epoch [1464/2500], Step [0/17], d_loss: 0.008531434461474419, g_loss: 7.061978340148926\n",
            "Epoch [1465/2500], Step [0/17], d_loss: 0.03417714685201645, g_loss: 10.649725914001465\n",
            "Epoch [1466/2500], Step [0/17], d_loss: 0.010493157431483269, g_loss: 6.969825744628906\n",
            "Epoch [1467/2500], Step [0/17], d_loss: 0.03828246146440506, g_loss: 5.852538585662842\n",
            "Epoch [1468/2500], Step [0/17], d_loss: 0.04846161976456642, g_loss: 6.951451301574707\n",
            "Epoch [1469/2500], Step [0/17], d_loss: 0.011170933954417706, g_loss: 6.989838600158691\n",
            "Epoch [1470/2500], Step [0/17], d_loss: 0.011433063074946404, g_loss: 7.94138240814209\n",
            "Epoch [1471/2500], Step [0/17], d_loss: 0.01626284420490265, g_loss: 8.270458221435547\n",
            "Epoch [1472/2500], Step [0/17], d_loss: 0.061500903218984604, g_loss: 8.283418655395508\n",
            "Epoch [1473/2500], Step [0/17], d_loss: 0.025778673589229584, g_loss: 6.278572082519531\n",
            "Epoch [1474/2500], Step [0/17], d_loss: 0.022478971630334854, g_loss: 7.118133544921875\n",
            "Epoch [1475/2500], Step [0/17], d_loss: 0.05326090380549431, g_loss: 11.876474380493164\n",
            "Epoch [1476/2500], Step [0/17], d_loss: 0.010359663516283035, g_loss: 6.8972249031066895\n",
            "Epoch [1477/2500], Step [0/17], d_loss: 0.016493123024702072, g_loss: 7.4637956619262695\n",
            "Epoch [1478/2500], Step [0/17], d_loss: 0.021171195432543755, g_loss: 7.483826637268066\n",
            "Epoch [1479/2500], Step [0/17], d_loss: 0.03944934532046318, g_loss: 5.851397514343262\n",
            "Epoch [1480/2500], Step [0/17], d_loss: 0.021766306832432747, g_loss: 14.183184623718262\n",
            "Epoch [1481/2500], Step [0/17], d_loss: 0.006262717768549919, g_loss: 8.027619361877441\n",
            "Epoch [1482/2500], Step [0/17], d_loss: 0.04385809600353241, g_loss: 12.875049591064453\n",
            "Epoch [1483/2500], Step [0/17], d_loss: 0.008525701239705086, g_loss: 7.724143981933594\n",
            "Epoch [1484/2500], Step [0/17], d_loss: 0.010568123310804367, g_loss: 12.02497673034668\n",
            "Epoch [1485/2500], Step [0/17], d_loss: 0.0038651712238788605, g_loss: 7.893036842346191\n",
            "Epoch [1486/2500], Step [0/17], d_loss: 0.011947695165872574, g_loss: 7.370575904846191\n",
            "Epoch [1487/2500], Step [0/17], d_loss: 0.013133665546774864, g_loss: 7.3660502433776855\n",
            "Epoch [1488/2500], Step [0/17], d_loss: 0.004970190580934286, g_loss: 7.869473457336426\n",
            "Epoch [1489/2500], Step [0/17], d_loss: 0.0033116459380835295, g_loss: 7.99794864654541\n",
            "Epoch [1490/2500], Step [0/17], d_loss: 0.0052038803696632385, g_loss: 8.491551399230957\n",
            "Epoch [1491/2500], Step [0/17], d_loss: 0.021102257072925568, g_loss: 16.538448333740234\n",
            "Epoch [1492/2500], Step [0/17], d_loss: 0.015190353617072105, g_loss: 11.18923568725586\n",
            "Epoch [1493/2500], Step [0/17], d_loss: 0.03369689732789993, g_loss: 6.8453826904296875\n",
            "Epoch [1494/2500], Step [0/17], d_loss: 0.007113948930054903, g_loss: 22.161231994628906\n",
            "Epoch [1495/2500], Step [0/17], d_loss: 0.010339707136154175, g_loss: 16.263181686401367\n",
            "Epoch [1496/2500], Step [0/17], d_loss: 0.009153502993285656, g_loss: 8.528621673583984\n",
            "Epoch [1497/2500], Step [0/17], d_loss: 0.17082224786281586, g_loss: 38.05178451538086\n",
            "Epoch [1498/2500], Step [0/17], d_loss: 0.011252034455537796, g_loss: 32.67009735107422\n",
            "Epoch [1499/2500], Step [0/17], d_loss: 0.018096711486577988, g_loss: 22.117528915405273\n",
            "Epoch [1500/2500], Step [0/17], d_loss: 0.4906550347805023, g_loss: 31.51938247680664\n",
            "Epoch [1501/2500], Step [0/17], d_loss: 0.016314400359988213, g_loss: 24.227516174316406\n",
            "Epoch [1502/2500], Step [0/17], d_loss: 0.04013490304350853, g_loss: 8.5327730178833\n",
            "Epoch [1503/2500], Step [0/17], d_loss: 0.003646311117336154, g_loss: 14.65678596496582\n",
            "Epoch [1504/2500], Step [0/17], d_loss: 0.33859366178512573, g_loss: 39.09844970703125\n",
            "Epoch [1505/2500], Step [0/17], d_loss: 0.10344327986240387, g_loss: 19.75408935546875\n",
            "Epoch [1506/2500], Step [0/17], d_loss: 0.038576092571020126, g_loss: 8.362852096557617\n",
            "Epoch [1507/2500], Step [0/17], d_loss: 0.0018389987526461482, g_loss: 18.448633193969727\n",
            "Epoch [1508/2500], Step [0/17], d_loss: 0.023767758160829544, g_loss: 7.976491451263428\n",
            "Epoch [1509/2500], Step [0/17], d_loss: 0.005691871978342533, g_loss: 8.50514030456543\n",
            "Epoch [1510/2500], Step [0/17], d_loss: 0.021592525765299797, g_loss: 7.532825469970703\n",
            "Epoch [1511/2500], Step [0/17], d_loss: 0.010811212472617626, g_loss: 7.197474956512451\n",
            "Epoch [1512/2500], Step [0/17], d_loss: 0.003123223315924406, g_loss: 11.808520317077637\n",
            "Epoch [1513/2500], Step [0/17], d_loss: 0.015636984258890152, g_loss: 8.694122314453125\n",
            "Epoch [1514/2500], Step [0/17], d_loss: 0.04696018621325493, g_loss: 11.04997444152832\n",
            "Epoch [1515/2500], Step [0/17], d_loss: 0.013067014515399933, g_loss: 7.207690238952637\n",
            "Epoch [1516/2500], Step [0/17], d_loss: 0.005214560776948929, g_loss: 8.02332878112793\n",
            "Epoch [1517/2500], Step [0/17], d_loss: 0.011681679636240005, g_loss: 15.864873886108398\n",
            "Epoch [1518/2500], Step [0/17], d_loss: 0.012850929982960224, g_loss: 9.380271911621094\n",
            "Epoch [1519/2500], Step [0/17], d_loss: 0.018157605081796646, g_loss: 6.5344343185424805\n",
            "Epoch [1520/2500], Step [0/17], d_loss: 0.008975927717983723, g_loss: 32.339019775390625\n",
            "Epoch [1521/2500], Step [0/17], d_loss: 0.015586292371153831, g_loss: 18.94509506225586\n",
            "Epoch [1522/2500], Step [0/17], d_loss: 0.008134711533784866, g_loss: 9.79666519165039\n",
            "Epoch [1523/2500], Step [0/17], d_loss: 0.021359644830226898, g_loss: 8.101482391357422\n",
            "Epoch [1524/2500], Step [0/17], d_loss: 0.04554396867752075, g_loss: 8.156171798706055\n",
            "Epoch [1525/2500], Step [0/17], d_loss: 0.0031729191541671753, g_loss: 8.32430648803711\n",
            "Epoch [1526/2500], Step [0/17], d_loss: 0.004476030822843313, g_loss: 9.124473571777344\n",
            "Epoch [1527/2500], Step [0/17], d_loss: 0.012718076817691326, g_loss: 7.542923927307129\n",
            "Epoch [1528/2500], Step [0/17], d_loss: 0.007460757624357939, g_loss: 7.45457649230957\n",
            "Epoch [1529/2500], Step [0/17], d_loss: 0.0041617583483457565, g_loss: 7.2382354736328125\n",
            "Epoch [1530/2500], Step [0/17], d_loss: 0.014165898784995079, g_loss: 7.077801704406738\n",
            "Epoch [1531/2500], Step [0/17], d_loss: 0.008642807602882385, g_loss: 7.3954057693481445\n",
            "Epoch [1532/2500], Step [0/17], d_loss: 0.003886727849021554, g_loss: 7.6927947998046875\n",
            "Epoch [1533/2500], Step [0/17], d_loss: 0.009951433166861534, g_loss: 7.1059136390686035\n",
            "Epoch [1534/2500], Step [0/17], d_loss: 0.07090604305267334, g_loss: 23.08751106262207\n",
            "Epoch [1535/2500], Step [0/17], d_loss: 0.012785326689481735, g_loss: 16.039941787719727\n",
            "Epoch [1536/2500], Step [0/17], d_loss: 0.014228573068976402, g_loss: 9.584251403808594\n",
            "Epoch [1537/2500], Step [0/17], d_loss: 0.009293308481574059, g_loss: 7.224978446960449\n",
            "Epoch [1538/2500], Step [0/17], d_loss: 0.010809509083628654, g_loss: 6.827457904815674\n",
            "Epoch [1539/2500], Step [0/17], d_loss: 0.008777561597526073, g_loss: 7.396582126617432\n",
            "Epoch [1540/2500], Step [0/17], d_loss: 0.005616065114736557, g_loss: 7.41518497467041\n",
            "Epoch [1541/2500], Step [0/17], d_loss: 0.023467108607292175, g_loss: 7.311867713928223\n",
            "Epoch [1542/2500], Step [0/17], d_loss: 0.007842116057872772, g_loss: 7.531913757324219\n",
            "Epoch [1543/2500], Step [0/17], d_loss: 0.006307423114776611, g_loss: 7.965365886688232\n",
            "Epoch [1544/2500], Step [0/17], d_loss: 0.003373059444129467, g_loss: 19.922056198120117\n",
            "Epoch [1545/2500], Step [0/17], d_loss: 0.015840986743569374, g_loss: 11.206819534301758\n",
            "Epoch [1546/2500], Step [0/17], d_loss: 0.006691306829452515, g_loss: 8.10555648803711\n",
            "Epoch [1547/2500], Step [0/17], d_loss: 0.002890747506171465, g_loss: 9.453210830688477\n",
            "Epoch [1548/2500], Step [0/17], d_loss: 0.018611807376146317, g_loss: 10.637639045715332\n",
            "Epoch [1549/2500], Step [0/17], d_loss: 0.03621448576450348, g_loss: 6.244458198547363\n",
            "Epoch [1550/2500], Step [0/17], d_loss: 0.0048229144886136055, g_loss: 7.9346232414245605\n",
            "Epoch [1551/2500], Step [0/17], d_loss: 0.007226050831377506, g_loss: 7.502203941345215\n",
            "Epoch [1552/2500], Step [0/17], d_loss: 0.005921824369579554, g_loss: 8.344505310058594\n",
            "Epoch [1553/2500], Step [0/17], d_loss: 0.0017615231918171048, g_loss: 8.435436248779297\n",
            "Epoch [1554/2500], Step [0/17], d_loss: 0.0038047751877456903, g_loss: 9.428060531616211\n",
            "Epoch [1555/2500], Step [0/17], d_loss: 0.014423167333006859, g_loss: 32.10721969604492\n",
            "Epoch [1556/2500], Step [0/17], d_loss: 0.009705998003482819, g_loss: 32.061683654785156\n",
            "Epoch [1557/2500], Step [0/17], d_loss: 0.024453574791550636, g_loss: 12.941980361938477\n",
            "Epoch [1558/2500], Step [0/17], d_loss: 0.222873717546463, g_loss: 27.427051544189453\n",
            "Epoch [1559/2500], Step [0/17], d_loss: 0.0021950113587081432, g_loss: 43.57137680053711\n",
            "Epoch [1560/2500], Step [0/17], d_loss: 0.018731672316789627, g_loss: 13.033832550048828\n",
            "Epoch [1561/2500], Step [0/17], d_loss: 0.02146834321320057, g_loss: 10.179906845092773\n",
            "Epoch [1562/2500], Step [0/17], d_loss: 0.006199547089636326, g_loss: 15.637811660766602\n",
            "Epoch [1563/2500], Step [0/17], d_loss: 0.013759506866335869, g_loss: 8.878650665283203\n",
            "Epoch [1564/2500], Step [0/17], d_loss: 0.0039363340474665165, g_loss: 8.344095230102539\n",
            "Epoch [1565/2500], Step [0/17], d_loss: 0.00396367721259594, g_loss: 15.787059783935547\n",
            "Epoch [1566/2500], Step [0/17], d_loss: 0.007809286005795002, g_loss: 11.276908874511719\n",
            "Epoch [1567/2500], Step [0/17], d_loss: 0.008135087788105011, g_loss: 9.653509140014648\n",
            "Epoch [1568/2500], Step [0/17], d_loss: 0.018162887543439865, g_loss: 10.878843307495117\n",
            "Epoch [1569/2500], Step [0/17], d_loss: 0.024433545768260956, g_loss: 12.95598030090332\n",
            "Epoch [1570/2500], Step [0/17], d_loss: 0.01566227525472641, g_loss: 7.502235412597656\n",
            "Epoch [1571/2500], Step [0/17], d_loss: 0.007521556690335274, g_loss: 8.59618854522705\n",
            "Epoch [1572/2500], Step [0/17], d_loss: 0.004437834955751896, g_loss: 7.660888671875\n",
            "Epoch [1573/2500], Step [0/17], d_loss: 0.0061439042910933495, g_loss: 8.267276763916016\n",
            "Epoch [1574/2500], Step [0/17], d_loss: 0.898418128490448, g_loss: 39.077125549316406\n",
            "Epoch [1575/2500], Step [0/17], d_loss: 0.07659408450126648, g_loss: 27.797901153564453\n",
            "Epoch [1576/2500], Step [0/17], d_loss: 0.02380877546966076, g_loss: 35.79322814941406\n",
            "Epoch [1577/2500], Step [0/17], d_loss: 0.09821204841136932, g_loss: 23.40083122253418\n",
            "Epoch [1578/2500], Step [0/17], d_loss: 0.025947440415620804, g_loss: 14.17568588256836\n",
            "Epoch [1579/2500], Step [0/17], d_loss: 0.045354679226875305, g_loss: 42.3919677734375\n",
            "Epoch [1580/2500], Step [0/17], d_loss: 0.21845518052577972, g_loss: 23.296188354492188\n",
            "Epoch [1581/2500], Step [0/17], d_loss: 0.002840532688423991, g_loss: 14.662361145019531\n",
            "Epoch [1582/2500], Step [0/17], d_loss: 0.009722262620925903, g_loss: 7.571841239929199\n",
            "Epoch [1583/2500], Step [0/17], d_loss: 0.01516575738787651, g_loss: 9.624138832092285\n",
            "Epoch [1584/2500], Step [0/17], d_loss: 0.011872089467942715, g_loss: 7.539749622344971\n",
            "Epoch [1585/2500], Step [0/17], d_loss: 0.02848612144589424, g_loss: 7.902962684631348\n",
            "Epoch [1586/2500], Step [0/17], d_loss: 0.01196668017655611, g_loss: 7.641352653503418\n",
            "Epoch [1587/2500], Step [0/17], d_loss: 0.006780544295907021, g_loss: 10.35796070098877\n",
            "Epoch [1588/2500], Step [0/17], d_loss: 0.04218702018260956, g_loss: 11.023591041564941\n",
            "Epoch [1589/2500], Step [0/17], d_loss: 0.004366541747003794, g_loss: 11.572755813598633\n",
            "Epoch [1590/2500], Step [0/17], d_loss: 0.018895646557211876, g_loss: 7.613919258117676\n",
            "Epoch [1591/2500], Step [0/17], d_loss: 0.014757993631064892, g_loss: 7.4000244140625\n",
            "Epoch [1592/2500], Step [0/17], d_loss: 0.007386312820017338, g_loss: 8.341314315795898\n",
            "Epoch [1593/2500], Step [0/17], d_loss: 0.008263878524303436, g_loss: 16.029556274414062\n",
            "Epoch [1594/2500], Step [0/17], d_loss: 0.00478096678853035, g_loss: 9.09659194946289\n",
            "Epoch [1595/2500], Step [0/17], d_loss: 0.005735034588724375, g_loss: 8.337246894836426\n",
            "Epoch [1596/2500], Step [0/17], d_loss: 0.012266810983419418, g_loss: 7.213367462158203\n",
            "Epoch [1597/2500], Step [0/17], d_loss: 0.04218422248959541, g_loss: 8.922178268432617\n",
            "Epoch [1598/2500], Step [0/17], d_loss: 0.0032686204649508, g_loss: 24.278778076171875\n",
            "Epoch [1599/2500], Step [0/17], d_loss: 0.051119640469551086, g_loss: 13.108585357666016\n",
            "Epoch [1600/2500], Step [0/17], d_loss: 0.009855746291577816, g_loss: 7.520179748535156\n",
            "Epoch [1601/2500], Step [0/17], d_loss: 0.08939433097839355, g_loss: 13.649923324584961\n",
            "Epoch [1602/2500], Step [0/17], d_loss: 0.01102052815258503, g_loss: 7.300453186035156\n",
            "Epoch [1603/2500], Step [0/17], d_loss: 0.005207308102399111, g_loss: 7.950144290924072\n",
            "Epoch [1604/2500], Step [0/17], d_loss: 0.01642998494207859, g_loss: 6.54299259185791\n",
            "Epoch [1605/2500], Step [0/17], d_loss: 0.029190264642238617, g_loss: 7.500823974609375\n",
            "Epoch [1606/2500], Step [0/17], d_loss: 0.02081206627190113, g_loss: 7.291975021362305\n",
            "Epoch [1607/2500], Step [0/17], d_loss: 0.004038780461996794, g_loss: 8.08565902709961\n",
            "Epoch [1608/2500], Step [0/17], d_loss: 0.022748347371816635, g_loss: 6.8632659912109375\n",
            "Epoch [1609/2500], Step [0/17], d_loss: 0.007008021697402, g_loss: 7.592965602874756\n",
            "Epoch [1610/2500], Step [0/17], d_loss: 0.00793082918971777, g_loss: 8.924840927124023\n",
            "Epoch [1611/2500], Step [0/17], d_loss: 0.008669912815093994, g_loss: 19.214200973510742\n",
            "Epoch [1612/2500], Step [0/17], d_loss: 0.0050306133925914764, g_loss: 8.565084457397461\n",
            "Epoch [1613/2500], Step [0/17], d_loss: 0.006359998136758804, g_loss: 7.5576372146606445\n",
            "Epoch [1614/2500], Step [0/17], d_loss: 0.0027693130541592836, g_loss: 10.69168472290039\n",
            "Epoch [1615/2500], Step [0/17], d_loss: 0.009337838739156723, g_loss: 7.198349952697754\n",
            "Epoch [1616/2500], Step [0/17], d_loss: 0.053273506462574005, g_loss: 10.35905647277832\n",
            "Epoch [1617/2500], Step [0/17], d_loss: 0.009137973189353943, g_loss: 10.687393188476562\n",
            "Epoch [1618/2500], Step [0/17], d_loss: 0.02575131319463253, g_loss: 7.522017002105713\n",
            "Epoch [1619/2500], Step [0/17], d_loss: 0.004374051466584206, g_loss: 12.323423385620117\n",
            "Epoch [1620/2500], Step [0/17], d_loss: 0.006608686875551939, g_loss: 6.87913179397583\n",
            "Epoch [1621/2500], Step [0/17], d_loss: 0.019638091325759888, g_loss: 6.963289260864258\n",
            "Epoch [1622/2500], Step [0/17], d_loss: 0.01236322708427906, g_loss: 7.75206184387207\n",
            "Epoch [1623/2500], Step [0/17], d_loss: 0.02143876813352108, g_loss: 8.561444282531738\n",
            "Epoch [1624/2500], Step [0/17], d_loss: 0.025375768542289734, g_loss: 6.823762893676758\n",
            "Epoch [1625/2500], Step [0/17], d_loss: 0.003429089207202196, g_loss: 9.272436141967773\n",
            "Epoch [1626/2500], Step [0/17], d_loss: 0.00790070928633213, g_loss: 7.029903888702393\n",
            "Epoch [1627/2500], Step [0/17], d_loss: 0.0052475882694125175, g_loss: 10.464759826660156\n",
            "Epoch [1628/2500], Step [0/17], d_loss: 0.01752088963985443, g_loss: 7.684061050415039\n",
            "Epoch [1629/2500], Step [0/17], d_loss: 0.008795835077762604, g_loss: 7.387401580810547\n",
            "Epoch [1630/2500], Step [0/17], d_loss: 0.035519376397132874, g_loss: 7.700876712799072\n",
            "Epoch [1631/2500], Step [0/17], d_loss: 0.007433827966451645, g_loss: 8.269834518432617\n",
            "Epoch [1632/2500], Step [0/17], d_loss: 0.012634413316845894, g_loss: 7.26737117767334\n",
            "Epoch [1633/2500], Step [0/17], d_loss: 0.0035028005950152874, g_loss: 8.023300170898438\n",
            "Epoch [1634/2500], Step [0/17], d_loss: 0.007557683624327183, g_loss: 7.114205837249756\n",
            "Epoch [1635/2500], Step [0/17], d_loss: 0.05400121584534645, g_loss: 16.435747146606445\n",
            "Epoch [1636/2500], Step [0/17], d_loss: 0.015547647140920162, g_loss: 17.150585174560547\n",
            "Epoch [1637/2500], Step [0/17], d_loss: 0.009703722782433033, g_loss: 10.308110237121582\n",
            "Epoch [1638/2500], Step [0/17], d_loss: 0.010643020272254944, g_loss: 7.024604797363281\n",
            "Epoch [1639/2500], Step [0/17], d_loss: 0.005183430854231119, g_loss: 7.501928329467773\n",
            "Epoch [1640/2500], Step [0/17], d_loss: 0.004705516155809164, g_loss: 8.521310806274414\n",
            "Epoch [1641/2500], Step [0/17], d_loss: 0.00801143143326044, g_loss: 7.683375358581543\n",
            "Epoch [1642/2500], Step [0/17], d_loss: 0.006219414062798023, g_loss: 7.6038737297058105\n",
            "Epoch [1643/2500], Step [0/17], d_loss: 0.011255936697125435, g_loss: 7.470220565795898\n",
            "Epoch [1644/2500], Step [0/17], d_loss: 0.8708441257476807, g_loss: 46.17770767211914\n",
            "Epoch [1645/2500], Step [0/17], d_loss: 0.03747859224677086, g_loss: 38.7899055480957\n",
            "Epoch [1646/2500], Step [0/17], d_loss: 0.5535234212875366, g_loss: 42.34190368652344\n",
            "Epoch [1647/2500], Step [0/17], d_loss: 0.02462601847946644, g_loss: 25.33835220336914\n",
            "Epoch [1648/2500], Step [0/17], d_loss: 0.007200909778475761, g_loss: 18.95134735107422\n",
            "Epoch [1649/2500], Step [0/17], d_loss: 0.23231562972068787, g_loss: 32.88140106201172\n",
            "Epoch [1650/2500], Step [0/17], d_loss: 0.012918788008391857, g_loss: 26.838010787963867\n",
            "Epoch [1651/2500], Step [0/17], d_loss: 0.2468080371618271, g_loss: 28.969985961914062\n",
            "Epoch [1652/2500], Step [0/17], d_loss: 0.021482795476913452, g_loss: 21.001541137695312\n",
            "Epoch [1653/2500], Step [0/17], d_loss: 0.0036147627979516983, g_loss: 29.454565048217773\n",
            "Epoch [1654/2500], Step [0/17], d_loss: 0.0707009881734848, g_loss: 12.99468994140625\n",
            "Epoch [1655/2500], Step [0/17], d_loss: 0.0028387033380568027, g_loss: 16.676410675048828\n",
            "Epoch [1656/2500], Step [0/17], d_loss: 0.061372920870780945, g_loss: 11.895062446594238\n",
            "Epoch [1657/2500], Step [0/17], d_loss: 0.027880672365427017, g_loss: 7.775238513946533\n",
            "Epoch [1658/2500], Step [0/17], d_loss: 0.0567220114171505, g_loss: 11.707856178283691\n",
            "Epoch [1659/2500], Step [0/17], d_loss: 0.020318523049354553, g_loss: 40.39525604248047\n",
            "Epoch [1660/2500], Step [0/17], d_loss: 0.0022579138167202473, g_loss: 28.459129333496094\n",
            "Epoch [1661/2500], Step [0/17], d_loss: 0.04476271569728851, g_loss: 13.22126579284668\n",
            "Epoch [1662/2500], Step [0/17], d_loss: 0.012783276848495007, g_loss: 8.815837860107422\n",
            "Epoch [1663/2500], Step [0/17], d_loss: 0.013662992045283318, g_loss: 12.376291275024414\n",
            "Epoch [1664/2500], Step [0/17], d_loss: 0.021742114797234535, g_loss: 8.558947563171387\n",
            "Epoch [1665/2500], Step [0/17], d_loss: 0.04167763143777847, g_loss: 9.394338607788086\n",
            "Epoch [1666/2500], Step [0/17], d_loss: 0.05280430614948273, g_loss: 20.17607879638672\n",
            "Epoch [1667/2500], Step [0/17], d_loss: 0.004421486519277096, g_loss: 12.9754638671875\n",
            "Epoch [1668/2500], Step [0/17], d_loss: 0.00834911223500967, g_loss: 9.266860961914062\n",
            "Epoch [1669/2500], Step [0/17], d_loss: 0.006146955769509077, g_loss: 7.872800827026367\n",
            "Epoch [1670/2500], Step [0/17], d_loss: 0.014146197587251663, g_loss: 7.471341133117676\n",
            "Epoch [1671/2500], Step [0/17], d_loss: 0.0037549794651567936, g_loss: 8.24659538269043\n",
            "Epoch [1672/2500], Step [0/17], d_loss: 0.015138383023440838, g_loss: 9.349899291992188\n",
            "Epoch [1673/2500], Step [0/17], d_loss: 0.005202373489737511, g_loss: 7.940045356750488\n",
            "Epoch [1674/2500], Step [0/17], d_loss: 0.017112253233790398, g_loss: 8.682794570922852\n",
            "Epoch [1675/2500], Step [0/17], d_loss: 0.007013360969722271, g_loss: 8.725503921508789\n",
            "Epoch [1676/2500], Step [0/17], d_loss: 0.024586912244558334, g_loss: 7.751540660858154\n",
            "Epoch [1677/2500], Step [0/17], d_loss: 0.012661040760576725, g_loss: 7.127129077911377\n",
            "Epoch [1678/2500], Step [0/17], d_loss: 0.008719021454453468, g_loss: 9.439510345458984\n",
            "Epoch [1679/2500], Step [0/17], d_loss: 0.7121495604515076, g_loss: 52.12449645996094\n",
            "Epoch [1680/2500], Step [0/17], d_loss: 0.04603256657719612, g_loss: 23.55350685119629\n",
            "Epoch [1681/2500], Step [0/17], d_loss: 0.010878336615860462, g_loss: 18.13242530822754\n",
            "Epoch [1682/2500], Step [0/17], d_loss: 0.03310782462358475, g_loss: 8.531044960021973\n",
            "Epoch [1683/2500], Step [0/17], d_loss: 0.015197498723864555, g_loss: 10.190601348876953\n",
            "Epoch [1684/2500], Step [0/17], d_loss: 0.005341383628547192, g_loss: 9.282155990600586\n",
            "Epoch [1685/2500], Step [0/17], d_loss: 0.007099058013409376, g_loss: 7.89755916595459\n",
            "Epoch [1686/2500], Step [0/17], d_loss: 0.010596917942166328, g_loss: 8.350662231445312\n",
            "Epoch [1687/2500], Step [0/17], d_loss: 0.01290874183177948, g_loss: 8.13432788848877\n",
            "Epoch [1688/2500], Step [0/17], d_loss: 0.00646235654130578, g_loss: 7.548140525817871\n",
            "Epoch [1689/2500], Step [0/17], d_loss: 0.0028705820441246033, g_loss: 10.328433990478516\n",
            "Epoch [1690/2500], Step [0/17], d_loss: 0.00641242228448391, g_loss: 7.302926540374756\n",
            "Epoch [1691/2500], Step [0/17], d_loss: 0.017192117869853973, g_loss: 7.687095642089844\n",
            "Epoch [1692/2500], Step [0/17], d_loss: 0.007008787244558334, g_loss: 8.83167839050293\n",
            "Epoch [1693/2500], Step [0/17], d_loss: 0.005530843045562506, g_loss: 8.323473930358887\n",
            "Epoch [1694/2500], Step [0/17], d_loss: 0.009229864925146103, g_loss: 7.4332404136657715\n",
            "Epoch [1695/2500], Step [0/17], d_loss: 0.020849596709012985, g_loss: 6.772780418395996\n",
            "Epoch [1696/2500], Step [0/17], d_loss: 0.010626467876136303, g_loss: 6.9495038986206055\n",
            "Epoch [1697/2500], Step [0/17], d_loss: 0.008350235410034657, g_loss: 9.358196258544922\n",
            "Epoch [1698/2500], Step [0/17], d_loss: 0.020539861172437668, g_loss: 6.780971050262451\n",
            "Epoch [1699/2500], Step [0/17], d_loss: 0.004891559015959501, g_loss: 9.847335815429688\n",
            "Epoch [1700/2500], Step [0/17], d_loss: 0.005506127141416073, g_loss: 8.454182624816895\n",
            "Epoch [1701/2500], Step [0/17], d_loss: 0.013438962399959564, g_loss: 10.985332489013672\n",
            "Epoch [1702/2500], Step [0/17], d_loss: 0.007507124450057745, g_loss: 15.930523872375488\n",
            "Epoch [1703/2500], Step [0/17], d_loss: 0.00572480633854866, g_loss: 7.875335216522217\n",
            "Epoch [1704/2500], Step [0/17], d_loss: 0.03791090473532677, g_loss: 11.25047492980957\n",
            "Epoch [1705/2500], Step [0/17], d_loss: 0.010253134183585644, g_loss: 9.366117477416992\n",
            "Epoch [1706/2500], Step [0/17], d_loss: 0.01197802647948265, g_loss: 8.29293441772461\n",
            "Epoch [1707/2500], Step [0/17], d_loss: 0.004766765516251326, g_loss: 6.663907051086426\n",
            "Epoch [1708/2500], Step [0/17], d_loss: 0.0081649050116539, g_loss: 7.471268653869629\n",
            "Epoch [1709/2500], Step [0/17], d_loss: 0.006354330573230982, g_loss: 30.383697509765625\n",
            "Epoch [1710/2500], Step [0/17], d_loss: 0.037193525582551956, g_loss: 20.877227783203125\n",
            "Epoch [1711/2500], Step [0/17], d_loss: 0.01571127027273178, g_loss: 11.970176696777344\n",
            "Epoch [1712/2500], Step [0/17], d_loss: 0.00958932749927044, g_loss: 7.634161949157715\n",
            "Epoch [1713/2500], Step [0/17], d_loss: 0.002485670382156968, g_loss: 10.003023147583008\n",
            "Epoch [1714/2500], Step [0/17], d_loss: 0.00519489124417305, g_loss: 7.548768997192383\n",
            "Epoch [1715/2500], Step [0/17], d_loss: 0.0035069165751338005, g_loss: 19.069820404052734\n",
            "Epoch [1716/2500], Step [0/17], d_loss: 0.0579056590795517, g_loss: 25.335351943969727\n",
            "Epoch [1717/2500], Step [0/17], d_loss: 0.001547637628391385, g_loss: 21.369789123535156\n",
            "Epoch [1718/2500], Step [0/17], d_loss: 0.012137409299612045, g_loss: 10.88553237915039\n",
            "Epoch [1719/2500], Step [0/17], d_loss: 0.007188813760876656, g_loss: 8.167074203491211\n",
            "Epoch [1720/2500], Step [0/17], d_loss: 0.004304839298129082, g_loss: 9.22104549407959\n",
            "Epoch [1721/2500], Step [0/17], d_loss: 0.009592090733349323, g_loss: 7.730849742889404\n",
            "Epoch [1722/2500], Step [0/17], d_loss: 0.02515505626797676, g_loss: 7.0106096267700195\n",
            "Epoch [1723/2500], Step [0/17], d_loss: 0.007089084945619106, g_loss: 7.978387832641602\n",
            "Epoch [1724/2500], Step [0/17], d_loss: 0.003600138472393155, g_loss: 9.648520469665527\n",
            "Epoch [1725/2500], Step [0/17], d_loss: 0.005407685413956642, g_loss: 7.46693754196167\n",
            "Epoch [1726/2500], Step [0/17], d_loss: 0.003082481911405921, g_loss: 9.325021743774414\n",
            "Epoch [1727/2500], Step [0/17], d_loss: 0.030728254467248917, g_loss: 10.957995414733887\n",
            "Epoch [1728/2500], Step [0/17], d_loss: 0.0027061712462455034, g_loss: 25.354393005371094\n",
            "Epoch [1729/2500], Step [0/17], d_loss: 0.003049262333661318, g_loss: 16.30677032470703\n",
            "Epoch [1730/2500], Step [0/17], d_loss: 0.003884335979819298, g_loss: 11.762704849243164\n",
            "Epoch [1731/2500], Step [0/17], d_loss: 0.006513860076665878, g_loss: 9.246307373046875\n",
            "Epoch [1732/2500], Step [0/17], d_loss: 0.004167876671999693, g_loss: 8.00773811340332\n",
            "Epoch [1733/2500], Step [0/17], d_loss: 0.005999930202960968, g_loss: 7.818548202514648\n",
            "Epoch [1734/2500], Step [0/17], d_loss: 0.00953108910471201, g_loss: 9.386014938354492\n",
            "Epoch [1735/2500], Step [0/17], d_loss: 0.004684749059379101, g_loss: 7.741413116455078\n",
            "Epoch [1736/2500], Step [0/17], d_loss: 0.005344856530427933, g_loss: 8.274757385253906\n",
            "Epoch [1737/2500], Step [0/17], d_loss: 0.0020268834196031094, g_loss: 9.317315101623535\n",
            "Epoch [1738/2500], Step [0/17], d_loss: 0.0017154570668935776, g_loss: 27.09174346923828\n",
            "Epoch [1739/2500], Step [0/17], d_loss: 0.009465026669204235, g_loss: 19.434900283813477\n",
            "Epoch [1740/2500], Step [0/17], d_loss: 0.0017308613751083612, g_loss: 10.521978378295898\n",
            "Epoch [1741/2500], Step [0/17], d_loss: 0.003165454836562276, g_loss: 9.443032264709473\n",
            "Epoch [1742/2500], Step [0/17], d_loss: 0.0007557490607723594, g_loss: 23.20814323425293\n",
            "Epoch [1743/2500], Step [0/17], d_loss: 0.005472091026604176, g_loss: 16.139036178588867\n",
            "Epoch [1744/2500], Step [0/17], d_loss: 0.008043896406888962, g_loss: 14.33735466003418\n",
            "Epoch [1745/2500], Step [0/17], d_loss: 0.005740606226027012, g_loss: 7.479538440704346\n",
            "Epoch [1746/2500], Step [0/17], d_loss: 0.0052943481132388115, g_loss: 10.704935073852539\n",
            "Epoch [1747/2500], Step [0/17], d_loss: 0.003877954790368676, g_loss: 7.709742546081543\n",
            "Epoch [1748/2500], Step [0/17], d_loss: 0.002567039802670479, g_loss: 8.129961013793945\n",
            "Epoch [1749/2500], Step [0/17], d_loss: 0.03478282690048218, g_loss: 22.828933715820312\n",
            "Epoch [1750/2500], Step [0/17], d_loss: 0.003556571900844574, g_loss: 13.696625709533691\n",
            "Epoch [1751/2500], Step [0/17], d_loss: 0.005358131602406502, g_loss: 7.408967971801758\n",
            "Epoch [1752/2500], Step [0/17], d_loss: 0.017617560923099518, g_loss: 7.707463264465332\n",
            "Epoch [1753/2500], Step [0/17], d_loss: 0.0017650218214839697, g_loss: 8.89808177947998\n",
            "Epoch [1754/2500], Step [0/17], d_loss: 0.00702326837927103, g_loss: 7.369412422180176\n",
            "Epoch [1755/2500], Step [0/17], d_loss: 0.0049207936972379684, g_loss: 9.47523307800293\n",
            "Epoch [1756/2500], Step [0/17], d_loss: 0.004495469853281975, g_loss: 7.910489082336426\n",
            "Epoch [1757/2500], Step [0/17], d_loss: 0.002308850409463048, g_loss: 9.901948928833008\n",
            "Epoch [1758/2500], Step [0/17], d_loss: 0.027742713689804077, g_loss: 26.53677749633789\n",
            "Epoch [1759/2500], Step [0/17], d_loss: 0.002074203686788678, g_loss: 32.504276275634766\n",
            "Epoch [1760/2500], Step [0/17], d_loss: 0.0002499367401469499, g_loss: 18.378643035888672\n",
            "Epoch [1761/2500], Step [0/17], d_loss: 0.025207282975316048, g_loss: 18.477672576904297\n",
            "Epoch [1762/2500], Step [0/17], d_loss: 0.006197867915034294, g_loss: 18.046987533569336\n",
            "Epoch [1763/2500], Step [0/17], d_loss: 0.002000262029469013, g_loss: 9.759878158569336\n",
            "Epoch [1764/2500], Step [0/17], d_loss: 0.008834796026349068, g_loss: 8.180379867553711\n",
            "Epoch [1765/2500], Step [0/17], d_loss: 0.003238399513065815, g_loss: 8.788833618164062\n",
            "Epoch [1766/2500], Step [0/17], d_loss: 0.002851185854524374, g_loss: 9.959053039550781\n",
            "Epoch [1767/2500], Step [0/17], d_loss: 0.006618903949856758, g_loss: 8.581327438354492\n",
            "Epoch [1768/2500], Step [0/17], d_loss: 0.003734935075044632, g_loss: 7.722018241882324\n",
            "Epoch [1769/2500], Step [0/17], d_loss: 0.004416625015437603, g_loss: 8.308500289916992\n",
            "Epoch [1770/2500], Step [0/17], d_loss: 0.007395446300506592, g_loss: 9.73270034790039\n",
            "Epoch [1771/2500], Step [0/17], d_loss: 0.002747918711975217, g_loss: 15.874224662780762\n",
            "Epoch [1772/2500], Step [0/17], d_loss: 0.0014389718417078257, g_loss: 14.809377670288086\n",
            "Epoch [1773/2500], Step [0/17], d_loss: 0.013731667771935463, g_loss: 10.816290855407715\n",
            "Epoch [1774/2500], Step [0/17], d_loss: 0.03263459354639053, g_loss: 12.903517723083496\n",
            "Epoch [1775/2500], Step [0/17], d_loss: 0.006048804149031639, g_loss: 10.468159675598145\n",
            "Epoch [1776/2500], Step [0/17], d_loss: 0.0037225910928100348, g_loss: 11.659761428833008\n",
            "Epoch [1777/2500], Step [0/17], d_loss: 0.0030209338292479515, g_loss: 8.077964782714844\n",
            "Epoch [1778/2500], Step [0/17], d_loss: 0.00368387158960104, g_loss: 8.179285049438477\n",
            "Epoch [1779/2500], Step [0/17], d_loss: 0.003787423949688673, g_loss: 8.007427215576172\n",
            "Epoch [1780/2500], Step [0/17], d_loss: 0.002216567751020193, g_loss: 8.95993423461914\n",
            "Epoch [1781/2500], Step [0/17], d_loss: 0.018596649169921875, g_loss: 9.807695388793945\n",
            "Epoch [1782/2500], Step [0/17], d_loss: 0.001999604981392622, g_loss: 12.421213150024414\n",
            "Epoch [1783/2500], Step [0/17], d_loss: 0.0005728608812205493, g_loss: 19.80758285522461\n",
            "Epoch [1784/2500], Step [0/17], d_loss: 0.006498576607555151, g_loss: 8.58348560333252\n",
            "Epoch [1785/2500], Step [0/17], d_loss: 0.008538237772881985, g_loss: 10.79830265045166\n",
            "Epoch [1786/2500], Step [0/17], d_loss: 0.006635691970586777, g_loss: 13.796168327331543\n",
            "Epoch [1787/2500], Step [0/17], d_loss: 0.002319764345884323, g_loss: 10.157567977905273\n",
            "Epoch [1788/2500], Step [0/17], d_loss: 0.00354457413777709, g_loss: 9.852958679199219\n",
            "Epoch [1789/2500], Step [0/17], d_loss: 0.07491587847471237, g_loss: 35.174400329589844\n",
            "Epoch [1790/2500], Step [0/17], d_loss: 0.04834958165884018, g_loss: 40.624176025390625\n",
            "Epoch [1791/2500], Step [0/17], d_loss: 0.01800692453980446, g_loss: 25.659862518310547\n",
            "Epoch [1792/2500], Step [0/17], d_loss: 0.00273478333838284, g_loss: 35.48069763183594\n",
            "Epoch [1793/2500], Step [0/17], d_loss: 0.007609283551573753, g_loss: 35.77880859375\n",
            "Epoch [1794/2500], Step [0/17], d_loss: 0.7210780382156372, g_loss: 48.061241149902344\n",
            "Epoch [1795/2500], Step [0/17], d_loss: 0.09591710567474365, g_loss: 34.56757354736328\n",
            "Epoch [1796/2500], Step [0/17], d_loss: 0.20687973499298096, g_loss: 18.54108428955078\n",
            "Epoch [1797/2500], Step [0/17], d_loss: 0.02013067528605461, g_loss: 27.121322631835938\n",
            "Epoch [1798/2500], Step [0/17], d_loss: 0.07190002501010895, g_loss: 25.38509750366211\n",
            "Epoch [1799/2500], Step [0/17], d_loss: 0.06847956031560898, g_loss: 18.701114654541016\n",
            "Epoch [1800/2500], Step [0/17], d_loss: 0.0011955640511587262, g_loss: 19.653289794921875\n",
            "Epoch [1801/2500], Step [0/17], d_loss: 0.028662273660302162, g_loss: 10.8317289352417\n",
            "Epoch [1802/2500], Step [0/17], d_loss: 0.01189870573580265, g_loss: 13.301169395446777\n",
            "Epoch [1803/2500], Step [0/17], d_loss: 0.010702058672904968, g_loss: 16.057594299316406\n",
            "Epoch [1804/2500], Step [0/17], d_loss: 0.027271389961242676, g_loss: 11.373748779296875\n",
            "Epoch [1805/2500], Step [0/17], d_loss: 0.005053277127444744, g_loss: 29.00174331665039\n",
            "Epoch [1806/2500], Step [0/17], d_loss: 0.010520531795918941, g_loss: 13.76597785949707\n",
            "Epoch [1807/2500], Step [0/17], d_loss: 0.010959488339722157, g_loss: 9.693323135375977\n",
            "Epoch [1808/2500], Step [0/17], d_loss: 0.06459072977304459, g_loss: 13.223655700683594\n",
            "Epoch [1809/2500], Step [0/17], d_loss: 0.0425218790769577, g_loss: 14.695152282714844\n",
            "Epoch [1810/2500], Step [0/17], d_loss: 0.008124547079205513, g_loss: 10.905401229858398\n",
            "Epoch [1811/2500], Step [0/17], d_loss: 0.02356380596756935, g_loss: 8.33326530456543\n",
            "Epoch [1812/2500], Step [0/17], d_loss: 0.012003578245639801, g_loss: 8.046009063720703\n",
            "Epoch [1813/2500], Step [0/17], d_loss: 0.011808945797383785, g_loss: 7.686081886291504\n",
            "Epoch [1814/2500], Step [0/17], d_loss: 0.013606544584035873, g_loss: 6.8813276290893555\n",
            "Epoch [1815/2500], Step [0/17], d_loss: 0.012188450433313847, g_loss: 7.620789527893066\n",
            "Epoch [1816/2500], Step [0/17], d_loss: 0.013259594328701496, g_loss: 7.245203495025635\n",
            "Epoch [1817/2500], Step [0/17], d_loss: 0.027018945664167404, g_loss: 8.281049728393555\n",
            "Epoch [1818/2500], Step [0/17], d_loss: 0.029981864616274834, g_loss: 9.231962203979492\n",
            "Epoch [1819/2500], Step [0/17], d_loss: 0.009707579389214516, g_loss: 8.930730819702148\n",
            "Epoch [1820/2500], Step [0/17], d_loss: 0.006411734037101269, g_loss: 17.053340911865234\n",
            "Epoch [1821/2500], Step [0/17], d_loss: 0.004106955137103796, g_loss: 8.877547264099121\n",
            "Epoch [1822/2500], Step [0/17], d_loss: 0.03586673364043236, g_loss: 7.76153564453125\n",
            "Epoch [1823/2500], Step [0/17], d_loss: 0.004530626814812422, g_loss: 8.622127532958984\n",
            "Epoch [1824/2500], Step [0/17], d_loss: 0.012540842406451702, g_loss: 8.289037704467773\n",
            "Epoch [1825/2500], Step [0/17], d_loss: 0.0036000357940793037, g_loss: 7.824977874755859\n",
            "Epoch [1826/2500], Step [0/17], d_loss: 0.011944119818508625, g_loss: 7.569464683532715\n",
            "Epoch [1827/2500], Step [0/17], d_loss: 0.007189914118498564, g_loss: 8.087458610534668\n",
            "Epoch [1828/2500], Step [0/17], d_loss: 0.0077920230105519295, g_loss: 8.409859657287598\n",
            "Epoch [1829/2500], Step [0/17], d_loss: 0.00866895541548729, g_loss: 14.089844703674316\n",
            "Epoch [1830/2500], Step [0/17], d_loss: 0.060812462121248245, g_loss: 16.93695831298828\n",
            "Epoch [1831/2500], Step [0/17], d_loss: 0.0072356658056378365, g_loss: 9.039006233215332\n",
            "Epoch [1832/2500], Step [0/17], d_loss: 0.008142128586769104, g_loss: 8.735027313232422\n",
            "Epoch [1833/2500], Step [0/17], d_loss: 0.0070127565413713455, g_loss: 7.745511054992676\n",
            "Epoch [1834/2500], Step [0/17], d_loss: 0.002306386362761259, g_loss: 8.574260711669922\n",
            "Epoch [1835/2500], Step [0/17], d_loss: 0.011560263112187386, g_loss: 6.5853986740112305\n",
            "Epoch [1836/2500], Step [0/17], d_loss: 0.005497886799275875, g_loss: 7.9587531089782715\n",
            "Epoch [1837/2500], Step [0/17], d_loss: 0.018468279391527176, g_loss: 7.374420166015625\n",
            "Epoch [1838/2500], Step [0/17], d_loss: 0.01778576895594597, g_loss: 7.599273681640625\n",
            "Epoch [1839/2500], Step [0/17], d_loss: 0.04852350428700447, g_loss: 42.32417297363281\n",
            "Epoch [1840/2500], Step [0/17], d_loss: 0.008296196348965168, g_loss: 28.253530502319336\n",
            "Epoch [1841/2500], Step [0/17], d_loss: 0.004800703842192888, g_loss: 13.821821212768555\n",
            "Epoch [1842/2500], Step [0/17], d_loss: 0.006258767563849688, g_loss: 7.625641345977783\n",
            "Epoch [1843/2500], Step [0/17], d_loss: 0.003681835485622287, g_loss: 16.118392944335938\n",
            "Epoch [1844/2500], Step [0/17], d_loss: 0.009266853332519531, g_loss: 10.976263999938965\n",
            "Epoch [1845/2500], Step [0/17], d_loss: 0.006054938305169344, g_loss: 12.403902053833008\n",
            "Epoch [1846/2500], Step [0/17], d_loss: 0.017779819667339325, g_loss: 7.77717399597168\n",
            "Epoch [1847/2500], Step [0/17], d_loss: 0.010336586274206638, g_loss: 7.96586799621582\n",
            "Epoch [1848/2500], Step [0/17], d_loss: 0.005075247958302498, g_loss: 20.07658576965332\n",
            "Epoch [1849/2500], Step [0/17], d_loss: 0.008568336255848408, g_loss: 11.745729446411133\n",
            "Epoch [1850/2500], Step [0/17], d_loss: 0.01320318877696991, g_loss: 7.602487087249756\n",
            "Epoch [1851/2500], Step [0/17], d_loss: 0.006820977665483952, g_loss: 15.423377990722656\n",
            "Epoch [1852/2500], Step [0/17], d_loss: 0.025606771931052208, g_loss: 8.75722885131836\n",
            "Epoch [1853/2500], Step [0/17], d_loss: 0.0023012899328023195, g_loss: 9.426631927490234\n",
            "Epoch [1854/2500], Step [0/17], d_loss: 0.02990001067519188, g_loss: 6.917862892150879\n",
            "Epoch [1855/2500], Step [0/17], d_loss: 0.009495224803686142, g_loss: 8.02120590209961\n",
            "Epoch [1856/2500], Step [0/17], d_loss: 0.008984687738120556, g_loss: 7.559633255004883\n",
            "Epoch [1857/2500], Step [0/17], d_loss: 0.005246907472610474, g_loss: 10.266305923461914\n",
            "Epoch [1858/2500], Step [0/17], d_loss: 0.0061357454396784306, g_loss: 8.634313583374023\n",
            "Epoch [1859/2500], Step [0/17], d_loss: 0.0027172155678272247, g_loss: 11.908052444458008\n",
            "Epoch [1860/2500], Step [0/17], d_loss: 0.03452688083052635, g_loss: 10.444123268127441\n",
            "Epoch [1861/2500], Step [0/17], d_loss: 0.006211132742464542, g_loss: 14.482344627380371\n",
            "Epoch [1862/2500], Step [0/17], d_loss: 0.017804181203246117, g_loss: 7.845258712768555\n",
            "Epoch [1863/2500], Step [0/17], d_loss: 0.012806633487343788, g_loss: 7.479168891906738\n",
            "Epoch [1864/2500], Step [0/17], d_loss: 0.007418051362037659, g_loss: 7.184322357177734\n",
            "Epoch [1865/2500], Step [0/17], d_loss: 0.01205600518733263, g_loss: 9.047175407409668\n",
            "Epoch [1866/2500], Step [0/17], d_loss: 0.006025071255862713, g_loss: 8.268228530883789\n",
            "Epoch [1867/2500], Step [0/17], d_loss: 0.0015883621526882052, g_loss: 8.590265274047852\n",
            "Epoch [1868/2500], Step [0/17], d_loss: 0.013032217510044575, g_loss: 7.309088706970215\n",
            "Epoch [1869/2500], Step [0/17], d_loss: 0.009840494021773338, g_loss: 7.333971977233887\n",
            "Epoch [1870/2500], Step [0/17], d_loss: 0.002937635174021125, g_loss: 7.850247859954834\n",
            "Epoch [1871/2500], Step [0/17], d_loss: 0.0586417019367218, g_loss: 12.44448184967041\n",
            "Epoch [1872/2500], Step [0/17], d_loss: 0.009979000315070152, g_loss: 8.391342163085938\n",
            "Epoch [1873/2500], Step [0/17], d_loss: 0.020984504371881485, g_loss: 6.755070209503174\n",
            "Epoch [1874/2500], Step [0/17], d_loss: 0.005244184751063585, g_loss: 7.535531044006348\n",
            "Epoch [1875/2500], Step [0/17], d_loss: 0.006306396797299385, g_loss: 8.321836471557617\n",
            "Epoch [1876/2500], Step [0/17], d_loss: 0.009293139912188053, g_loss: 6.9846882820129395\n",
            "Epoch [1877/2500], Step [0/17], d_loss: 0.010013760067522526, g_loss: 10.27886962890625\n",
            "Epoch [1878/2500], Step [0/17], d_loss: 0.003389133373275399, g_loss: 7.635364532470703\n",
            "Epoch [1879/2500], Step [0/17], d_loss: 0.008725181221961975, g_loss: 48.84519577026367\n",
            "Epoch [1880/2500], Step [0/17], d_loss: 0.039812423288822174, g_loss: 39.49755096435547\n",
            "Epoch [1881/2500], Step [0/17], d_loss: 0.027907544746994972, g_loss: 32.9234733581543\n",
            "Epoch [1882/2500], Step [0/17], d_loss: 0.007272065617144108, g_loss: 18.685710906982422\n",
            "Epoch [1883/2500], Step [0/17], d_loss: 0.013506736606359482, g_loss: 27.30803871154785\n",
            "Epoch [1884/2500], Step [0/17], d_loss: 0.0041839987970888615, g_loss: 17.177669525146484\n",
            "Epoch [1885/2500], Step [0/17], d_loss: 0.003953819163143635, g_loss: 9.469470977783203\n",
            "Epoch [1886/2500], Step [0/17], d_loss: 0.014984242618083954, g_loss: 8.740938186645508\n",
            "Epoch [1887/2500], Step [0/17], d_loss: 0.06201181933283806, g_loss: 13.678032875061035\n",
            "Epoch [1888/2500], Step [0/17], d_loss: 0.01772703044116497, g_loss: 11.069497108459473\n",
            "Epoch [1889/2500], Step [0/17], d_loss: 0.0031047258526086807, g_loss: 11.920910835266113\n",
            "Epoch [1890/2500], Step [0/17], d_loss: 0.005032756831496954, g_loss: 7.787339687347412\n",
            "Epoch [1891/2500], Step [0/17], d_loss: 0.17411141097545624, g_loss: 47.97473907470703\n",
            "Epoch [1892/2500], Step [0/17], d_loss: 0.00025920342886820436, g_loss: 43.315467834472656\n",
            "Epoch [1893/2500], Step [0/17], d_loss: 0.0016245042206719518, g_loss: 37.15479278564453\n",
            "Epoch [1894/2500], Step [0/17], d_loss: 0.023486822843551636, g_loss: 20.866043090820312\n",
            "Epoch [1895/2500], Step [0/17], d_loss: 0.008068022318184376, g_loss: 9.164894104003906\n",
            "Epoch [1896/2500], Step [0/17], d_loss: 0.022620515897870064, g_loss: 9.201620101928711\n",
            "Epoch [1897/2500], Step [0/17], d_loss: 0.013653287664055824, g_loss: 8.855732917785645\n",
            "Epoch [1898/2500], Step [0/17], d_loss: 0.010929765179753304, g_loss: 7.6363935470581055\n",
            "Epoch [1899/2500], Step [0/17], d_loss: 0.016291271895170212, g_loss: 7.535477161407471\n",
            "Epoch [1900/2500], Step [0/17], d_loss: 0.00809646025300026, g_loss: 7.798731803894043\n",
            "Epoch [1901/2500], Step [0/17], d_loss: 0.017059559002518654, g_loss: 7.872308731079102\n",
            "Epoch [1902/2500], Step [0/17], d_loss: 0.018252121284604073, g_loss: 8.488460540771484\n",
            "Epoch [1903/2500], Step [0/17], d_loss: 0.0012502714525908232, g_loss: 17.06230926513672\n",
            "Epoch [1904/2500], Step [0/17], d_loss: 0.0268924031406641, g_loss: 12.266273498535156\n",
            "Epoch [1905/2500], Step [0/17], d_loss: 0.01217719353735447, g_loss: 7.471131801605225\n",
            "Epoch [1906/2500], Step [0/17], d_loss: 0.012150653637945652, g_loss: 8.864608764648438\n",
            "Epoch [1907/2500], Step [0/17], d_loss: 0.02066482976078987, g_loss: 8.809191703796387\n",
            "Epoch [1908/2500], Step [0/17], d_loss: 0.025801345705986023, g_loss: 8.456781387329102\n",
            "Epoch [1909/2500], Step [0/17], d_loss: 0.014217722229659557, g_loss: 7.362916946411133\n",
            "Epoch [1910/2500], Step [0/17], d_loss: 0.007593653630465269, g_loss: 7.2024993896484375\n",
            "Epoch [1911/2500], Step [0/17], d_loss: 0.0026053395122289658, g_loss: 9.173908233642578\n",
            "Epoch [1912/2500], Step [0/17], d_loss: 0.0061422535218298435, g_loss: 10.314722061157227\n",
            "Epoch [1913/2500], Step [0/17], d_loss: 0.003471757285296917, g_loss: 8.297164916992188\n",
            "Epoch [1914/2500], Step [0/17], d_loss: 0.012120471335947514, g_loss: 7.668874263763428\n",
            "Epoch [1915/2500], Step [0/17], d_loss: 0.006966229062527418, g_loss: 7.962216377258301\n",
            "Epoch [1916/2500], Step [0/17], d_loss: 0.0037216409109532833, g_loss: 10.400449752807617\n",
            "Epoch [1917/2500], Step [0/17], d_loss: 0.002041172469034791, g_loss: 10.792318344116211\n",
            "Epoch [1918/2500], Step [0/17], d_loss: 0.01695011928677559, g_loss: 7.987588405609131\n",
            "Epoch [1919/2500], Step [0/17], d_loss: 0.04834505915641785, g_loss: 5.105138778686523\n",
            "Epoch [1920/2500], Step [0/17], d_loss: 0.004296496510505676, g_loss: 8.702491760253906\n",
            "Epoch [1921/2500], Step [0/17], d_loss: 0.002005279064178467, g_loss: 8.598882675170898\n",
            "Epoch [1922/2500], Step [0/17], d_loss: 0.012161802500486374, g_loss: 6.606384754180908\n",
            "Epoch [1923/2500], Step [0/17], d_loss: 0.0020056073553860188, g_loss: 7.736433982849121\n",
            "Epoch [1924/2500], Step [0/17], d_loss: 0.05884743481874466, g_loss: 52.00758361816406\n",
            "Epoch [1925/2500], Step [0/17], d_loss: 0.03163817897439003, g_loss: 35.76485061645508\n",
            "Epoch [1926/2500], Step [0/17], d_loss: 0.06508201360702515, g_loss: 14.991605758666992\n",
            "Epoch [1927/2500], Step [0/17], d_loss: 0.008423796854913235, g_loss: 44.17216873168945\n",
            "Epoch [1928/2500], Step [0/17], d_loss: 0.046795304864645004, g_loss: 26.898862838745117\n",
            "Epoch [1929/2500], Step [0/17], d_loss: 0.022618329152464867, g_loss: 10.966146469116211\n",
            "Epoch [1930/2500], Step [0/17], d_loss: 0.0014601489529013634, g_loss: 15.615522384643555\n",
            "Epoch [1931/2500], Step [0/17], d_loss: 0.007142084185034037, g_loss: 9.843839645385742\n",
            "Epoch [1932/2500], Step [0/17], d_loss: 0.10985156893730164, g_loss: 46.904205322265625\n",
            "Epoch [1933/2500], Step [0/17], d_loss: 0.0020095747895538807, g_loss: 32.31529235839844\n",
            "Epoch [1934/2500], Step [0/17], d_loss: 0.060369379818439484, g_loss: 46.110740661621094\n",
            "Epoch [1935/2500], Step [0/17], d_loss: 0.043753623962402344, g_loss: 24.327808380126953\n",
            "Epoch [1936/2500], Step [0/17], d_loss: 0.02115488238632679, g_loss: 17.698287963867188\n",
            "Epoch [1937/2500], Step [0/17], d_loss: 8.148472988978028e-05, g_loss: 37.792816162109375\n",
            "Epoch [1938/2500], Step [0/17], d_loss: 0.0016498088371008635, g_loss: 33.69718933105469\n",
            "Epoch [1939/2500], Step [0/17], d_loss: 0.06347079575061798, g_loss: 27.56072998046875\n",
            "Epoch [1940/2500], Step [0/17], d_loss: 0.023739365860819817, g_loss: 16.251605987548828\n",
            "Epoch [1941/2500], Step [0/17], d_loss: 0.004559587221592665, g_loss: 18.93093490600586\n",
            "Epoch [1942/2500], Step [0/17], d_loss: 0.006064923945814371, g_loss: 8.753625869750977\n",
            "Epoch [1943/2500], Step [0/17], d_loss: 0.01054704375565052, g_loss: 8.249631881713867\n",
            "Epoch [1944/2500], Step [0/17], d_loss: 0.01140423770993948, g_loss: 10.841741561889648\n",
            "Epoch [1945/2500], Step [0/17], d_loss: 0.03864932805299759, g_loss: 11.346752166748047\n",
            "Epoch [1946/2500], Step [0/17], d_loss: 0.0016243777936324477, g_loss: 13.464550018310547\n",
            "Epoch [1947/2500], Step [0/17], d_loss: 0.0387740284204483, g_loss: 7.194089889526367\n",
            "Epoch [1948/2500], Step [0/17], d_loss: 0.017371466383337975, g_loss: 7.84133243560791\n",
            "Epoch [1949/2500], Step [0/17], d_loss: 0.009691662155091763, g_loss: 14.561932563781738\n",
            "Epoch [1950/2500], Step [0/17], d_loss: 0.011485250666737556, g_loss: 7.597487449645996\n",
            "Epoch [1951/2500], Step [0/17], d_loss: 0.001736559090204537, g_loss: 9.625957489013672\n",
            "Epoch [1952/2500], Step [0/17], d_loss: 0.0013590799644589424, g_loss: 8.666973114013672\n",
            "Epoch [1953/2500], Step [0/17], d_loss: 0.008712446317076683, g_loss: 7.275181770324707\n",
            "Epoch [1954/2500], Step [0/17], d_loss: 0.007515651639550924, g_loss: 6.826627731323242\n",
            "Epoch [1955/2500], Step [0/17], d_loss: 0.0016528555424883962, g_loss: 8.876806259155273\n",
            "Epoch [1956/2500], Step [0/17], d_loss: 0.008516493253409863, g_loss: 9.912355422973633\n",
            "Epoch [1957/2500], Step [0/17], d_loss: 0.00543044600635767, g_loss: 8.09508228302002\n",
            "Epoch [1958/2500], Step [0/17], d_loss: 0.007369893603026867, g_loss: 8.424229621887207\n",
            "Epoch [1959/2500], Step [0/17], d_loss: 0.007402584422379732, g_loss: 7.501821517944336\n",
            "Epoch [1960/2500], Step [0/17], d_loss: 0.0012414423981681466, g_loss: 17.868688583374023\n",
            "Epoch [1961/2500], Step [0/17], d_loss: 0.03147335350513458, g_loss: 14.879354476928711\n",
            "Epoch [1962/2500], Step [0/17], d_loss: 0.01517531555145979, g_loss: 8.248893737792969\n",
            "Epoch [1963/2500], Step [0/17], d_loss: 0.003905361983925104, g_loss: 7.843542575836182\n",
            "Epoch [1964/2500], Step [0/17], d_loss: 0.008394710719585419, g_loss: 8.56952953338623\n",
            "Epoch [1965/2500], Step [0/17], d_loss: 0.010236331261694431, g_loss: 7.267007827758789\n",
            "Epoch [1966/2500], Step [0/17], d_loss: 0.018447279930114746, g_loss: 8.02652645111084\n",
            "Epoch [1967/2500], Step [0/17], d_loss: 0.005661539733409882, g_loss: 7.439640522003174\n",
            "Epoch [1968/2500], Step [0/17], d_loss: 0.0016232478665187955, g_loss: 8.24228286743164\n",
            "Epoch [1969/2500], Step [0/17], d_loss: 0.008540515787899494, g_loss: 8.08767318725586\n",
            "Epoch [1970/2500], Step [0/17], d_loss: 0.0017883279360830784, g_loss: 8.617481231689453\n",
            "Epoch [1971/2500], Step [0/17], d_loss: 0.004081324208527803, g_loss: 7.563325881958008\n",
            "Epoch [1972/2500], Step [0/17], d_loss: 0.005773796234279871, g_loss: 7.124388217926025\n",
            "Epoch [1973/2500], Step [0/17], d_loss: 0.021094810217618942, g_loss: 8.714241027832031\n",
            "Epoch [1974/2500], Step [0/17], d_loss: 0.007587103638797998, g_loss: 9.265504837036133\n",
            "Epoch [1975/2500], Step [0/17], d_loss: 0.006531974300742149, g_loss: 7.639474868774414\n",
            "Epoch [1976/2500], Step [0/17], d_loss: 0.0024410283658653498, g_loss: 8.098554611206055\n",
            "Epoch [1977/2500], Step [0/17], d_loss: 0.007628167048096657, g_loss: 8.97698974609375\n",
            "Epoch [1978/2500], Step [0/17], d_loss: 0.006554953288286924, g_loss: 8.401359558105469\n",
            "Epoch [1979/2500], Step [0/17], d_loss: 0.005151655059307814, g_loss: 8.723477363586426\n",
            "Epoch [1980/2500], Step [0/17], d_loss: 0.002462159376591444, g_loss: 9.038642883300781\n",
            "Epoch [1981/2500], Step [0/17], d_loss: 0.006828689482063055, g_loss: 8.414306640625\n",
            "Epoch [1982/2500], Step [0/17], d_loss: 0.002414133632555604, g_loss: 16.252830505371094\n",
            "Epoch [1983/2500], Step [0/17], d_loss: 0.00290870014578104, g_loss: 15.612306594848633\n",
            "Epoch [1984/2500], Step [0/17], d_loss: 0.00213223765604198, g_loss: 9.13737678527832\n",
            "Epoch [1985/2500], Step [0/17], d_loss: 0.005429359152913094, g_loss: 7.923223495483398\n",
            "Epoch [1986/2500], Step [0/17], d_loss: 0.005593356676399708, g_loss: 8.013856887817383\n",
            "Epoch [1987/2500], Step [0/17], d_loss: 0.009998686611652374, g_loss: 8.009572982788086\n",
            "Epoch [1988/2500], Step [0/17], d_loss: 0.0024970415979623795, g_loss: 9.47529411315918\n",
            "Epoch [1989/2500], Step [0/17], d_loss: 0.004671315662562847, g_loss: 7.5981292724609375\n",
            "Epoch [1990/2500], Step [0/17], d_loss: 0.004770149476826191, g_loss: 7.850790500640869\n",
            "Epoch [1991/2500], Step [0/17], d_loss: 0.006025261711329222, g_loss: 18.49602508544922\n",
            "Epoch [1992/2500], Step [0/17], d_loss: 0.02270662784576416, g_loss: 10.880651473999023\n",
            "Epoch [1993/2500], Step [0/17], d_loss: 0.024127591401338577, g_loss: 9.4027099609375\n",
            "Epoch [1994/2500], Step [0/17], d_loss: 0.002331237308681011, g_loss: 12.814529418945312\n",
            "Epoch [1995/2500], Step [0/17], d_loss: 0.0024812445044517517, g_loss: 8.57808780670166\n",
            "Epoch [1996/2500], Step [0/17], d_loss: 0.015267794951796532, g_loss: 7.6647443771362305\n",
            "Epoch [1997/2500], Step [0/17], d_loss: 0.01219660509377718, g_loss: 7.828064918518066\n",
            "Epoch [1998/2500], Step [0/17], d_loss: 0.0013219636166468263, g_loss: 8.291015625\n",
            "Epoch [1999/2500], Step [0/17], d_loss: 0.0019213482737541199, g_loss: 8.323177337646484\n",
            "Epoch [2000/2500], Step [0/17], d_loss: 0.014768697321414948, g_loss: 9.071576118469238\n",
            "Epoch [2001/2500], Step [0/17], d_loss: 0.006907167378813028, g_loss: 9.679354667663574\n",
            "Epoch [2002/2500], Step [0/17], d_loss: 0.013827122747898102, g_loss: 8.42193603515625\n",
            "Epoch [2003/2500], Step [0/17], d_loss: 0.012424474582076073, g_loss: 8.324837684631348\n",
            "Epoch [2004/2500], Step [0/17], d_loss: 0.0027832314372062683, g_loss: 9.03537654876709\n",
            "Epoch [2005/2500], Step [0/17], d_loss: 0.0011162038426846266, g_loss: 16.898456573486328\n",
            "Epoch [2006/2500], Step [0/17], d_loss: 0.0094978716224432, g_loss: 12.744470596313477\n",
            "Epoch [2007/2500], Step [0/17], d_loss: 0.0034267257433384657, g_loss: 10.563279151916504\n",
            "Epoch [2008/2500], Step [0/17], d_loss: 0.0030746266711503267, g_loss: 8.148488998413086\n",
            "Epoch [2009/2500], Step [0/17], d_loss: 0.0021251116413623095, g_loss: 8.447482109069824\n",
            "Epoch [2010/2500], Step [0/17], d_loss: 0.00803916435688734, g_loss: 8.648439407348633\n",
            "Epoch [2011/2500], Step [0/17], d_loss: 1.628793716430664, g_loss: 15.209112167358398\n",
            "Epoch [2012/2500], Step [0/17], d_loss: 0.055440694093704224, g_loss: 37.286781311035156\n",
            "Epoch [2013/2500], Step [0/17], d_loss: 0.09050148725509644, g_loss: 44.555564880371094\n",
            "Epoch [2014/2500], Step [0/17], d_loss: 0.0036452391650527716, g_loss: 39.86732482910156\n",
            "Epoch [2015/2500], Step [0/17], d_loss: 0.006072079297155142, g_loss: 26.534570693969727\n",
            "Epoch [2016/2500], Step [0/17], d_loss: 0.0038879786152392626, g_loss: 26.616867065429688\n",
            "Epoch [2017/2500], Step [0/17], d_loss: 0.002387675689533353, g_loss: 14.627094268798828\n",
            "Epoch [2018/2500], Step [0/17], d_loss: 0.0032974774949252605, g_loss: 26.596725463867188\n",
            "Epoch [2019/2500], Step [0/17], d_loss: 0.012030853889882565, g_loss: 39.88111877441406\n",
            "Epoch [2020/2500], Step [0/17], d_loss: 0.014067609794437885, g_loss: 32.915748596191406\n",
            "Epoch [2021/2500], Step [0/17], d_loss: 0.2560429275035858, g_loss: 35.80118179321289\n",
            "Epoch [2022/2500], Step [0/17], d_loss: 0.01838403195142746, g_loss: 33.67821502685547\n",
            "Epoch [2023/2500], Step [0/17], d_loss: 0.021153196692466736, g_loss: 16.38953399658203\n",
            "Epoch [2024/2500], Step [0/17], d_loss: 0.05810599774122238, g_loss: 13.134586334228516\n",
            "Epoch [2025/2500], Step [0/17], d_loss: 0.02282043732702732, g_loss: 30.413806915283203\n",
            "Epoch [2026/2500], Step [0/17], d_loss: 0.0020621120929718018, g_loss: 28.464576721191406\n",
            "Epoch [2027/2500], Step [0/17], d_loss: 0.009892379865050316, g_loss: 13.846680641174316\n",
            "Epoch [2028/2500], Step [0/17], d_loss: 0.0030418208334594965, g_loss: 15.191917419433594\n",
            "Epoch [2029/2500], Step [0/17], d_loss: 0.03594585880637169, g_loss: 23.659265518188477\n",
            "Epoch [2030/2500], Step [0/17], d_loss: 0.012671408243477345, g_loss: 12.712715148925781\n",
            "Epoch [2031/2500], Step [0/17], d_loss: 0.022259876132011414, g_loss: 13.72266960144043\n",
            "Epoch [2032/2500], Step [0/17], d_loss: 0.00166022265329957, g_loss: 23.684200286865234\n",
            "Epoch [2033/2500], Step [0/17], d_loss: 0.01697506569325924, g_loss: 10.661542892456055\n",
            "Epoch [2034/2500], Step [0/17], d_loss: 0.005079448223114014, g_loss: 27.2933349609375\n",
            "Epoch [2035/2500], Step [0/17], d_loss: 0.01044788770377636, g_loss: 22.2416934967041\n",
            "Epoch [2036/2500], Step [0/17], d_loss: 0.021628309041261673, g_loss: 15.650999069213867\n",
            "Epoch [2037/2500], Step [0/17], d_loss: 0.002893000142648816, g_loss: 11.696182250976562\n",
            "Epoch [2038/2500], Step [0/17], d_loss: 0.005318738985806704, g_loss: 17.507949829101562\n",
            "Epoch [2039/2500], Step [0/17], d_loss: 0.010016011074185371, g_loss: 11.343755722045898\n",
            "Epoch [2040/2500], Step [0/17], d_loss: 0.0901569277048111, g_loss: 15.93885612487793\n",
            "Epoch [2041/2500], Step [0/17], d_loss: 0.03367435932159424, g_loss: 9.172904968261719\n",
            "Epoch [2042/2500], Step [0/17], d_loss: 0.007253675721585751, g_loss: 7.998817443847656\n",
            "Epoch [2043/2500], Step [0/17], d_loss: 0.03664475306868553, g_loss: 10.194701194763184\n",
            "Epoch [2044/2500], Step [0/17], d_loss: 0.03235434740781784, g_loss: 10.22684097290039\n",
            "Epoch [2045/2500], Step [0/17], d_loss: 0.029443731531500816, g_loss: 10.822653770446777\n",
            "Epoch [2046/2500], Step [0/17], d_loss: 0.002815844491124153, g_loss: 8.744901657104492\n",
            "Epoch [2047/2500], Step [0/17], d_loss: 0.00695645110681653, g_loss: 10.345418930053711\n",
            "Epoch [2048/2500], Step [0/17], d_loss: 0.009069979190826416, g_loss: 7.574488639831543\n",
            "Epoch [2049/2500], Step [0/17], d_loss: 0.0191759392619133, g_loss: 8.299393653869629\n",
            "Epoch [2050/2500], Step [0/17], d_loss: 0.015822257846593857, g_loss: 7.345669746398926\n",
            "Epoch [2051/2500], Step [0/17], d_loss: 0.017370130866765976, g_loss: 7.033238410949707\n",
            "Epoch [2052/2500], Step [0/17], d_loss: 0.0034142606891691685, g_loss: 8.063304901123047\n",
            "Epoch [2053/2500], Step [0/17], d_loss: 0.003360224422067404, g_loss: 13.737323760986328\n",
            "Epoch [2054/2500], Step [0/17], d_loss: 0.002511480124667287, g_loss: 13.324752807617188\n",
            "Epoch [2055/2500], Step [0/17], d_loss: 0.0027629754040390253, g_loss: 11.393621444702148\n",
            "Epoch [2056/2500], Step [0/17], d_loss: 0.012322360649704933, g_loss: 6.955015182495117\n",
            "Epoch [2057/2500], Step [0/17], d_loss: 0.006004794500768185, g_loss: 18.174915313720703\n",
            "Epoch [2058/2500], Step [0/17], d_loss: 0.0036515435203909874, g_loss: 14.873085021972656\n",
            "Epoch [2059/2500], Step [0/17], d_loss: 0.00921894982457161, g_loss: 9.704559326171875\n",
            "Epoch [2060/2500], Step [0/17], d_loss: 0.007045363076031208, g_loss: 7.553254127502441\n",
            "Epoch [2061/2500], Step [0/17], d_loss: 0.020156051963567734, g_loss: 7.270951747894287\n",
            "Epoch [2062/2500], Step [0/17], d_loss: 0.0037083160132169724, g_loss: 7.854546070098877\n",
            "Epoch [2063/2500], Step [0/17], d_loss: 0.021977443248033524, g_loss: 8.158586502075195\n",
            "Epoch [2064/2500], Step [0/17], d_loss: 0.011768534779548645, g_loss: 8.691579818725586\n",
            "Epoch [2065/2500], Step [0/17], d_loss: 0.00395860243588686, g_loss: 7.781920433044434\n",
            "Epoch [2066/2500], Step [0/17], d_loss: 0.021008852869272232, g_loss: 7.9279375076293945\n",
            "Epoch [2067/2500], Step [0/17], d_loss: 0.024755580350756645, g_loss: 8.691546440124512\n",
            "Epoch [2068/2500], Step [0/17], d_loss: 0.004293416626751423, g_loss: 7.046286582946777\n",
            "Epoch [2069/2500], Step [0/17], d_loss: 0.00421171635389328, g_loss: 9.334546089172363\n",
            "Epoch [2070/2500], Step [0/17], d_loss: 0.002117553958669305, g_loss: 15.203445434570312\n",
            "Epoch [2071/2500], Step [0/17], d_loss: 0.009324255399405956, g_loss: 7.8816704750061035\n",
            "Epoch [2072/2500], Step [0/17], d_loss: 0.0017273257253691554, g_loss: 8.138261795043945\n",
            "Epoch [2073/2500], Step [0/17], d_loss: 0.0038090490270406008, g_loss: 8.552461624145508\n",
            "Epoch [2074/2500], Step [0/17], d_loss: 0.003908968064934015, g_loss: 25.412147521972656\n",
            "Epoch [2075/2500], Step [0/17], d_loss: 0.0045531815849244595, g_loss: 17.983657836914062\n",
            "Epoch [2076/2500], Step [0/17], d_loss: 0.009613391943275928, g_loss: 8.915491104125977\n",
            "Epoch [2077/2500], Step [0/17], d_loss: 0.040990885347127914, g_loss: 33.18254852294922\n",
            "Epoch [2078/2500], Step [0/17], d_loss: 0.0019440754549577832, g_loss: 18.047677993774414\n",
            "Epoch [2079/2500], Step [0/17], d_loss: 0.0030564910266548395, g_loss: 9.952468872070312\n",
            "Epoch [2080/2500], Step [0/17], d_loss: 0.016423825174570084, g_loss: 14.822019577026367\n",
            "Epoch [2081/2500], Step [0/17], d_loss: 0.001087724813260138, g_loss: 12.494121551513672\n",
            "Epoch [2082/2500], Step [0/17], d_loss: 0.028171811252832413, g_loss: 9.667858123779297\n",
            "Epoch [2083/2500], Step [0/17], d_loss: 0.0038151810877025127, g_loss: 7.903293132781982\n",
            "Epoch [2084/2500], Step [0/17], d_loss: 0.00792745128273964, g_loss: 7.141849040985107\n",
            "Epoch [2085/2500], Step [0/17], d_loss: 0.013755346648395061, g_loss: 7.606525421142578\n",
            "Epoch [2086/2500], Step [0/17], d_loss: 0.008143474347889423, g_loss: 7.1359453201293945\n",
            "Epoch [2087/2500], Step [0/17], d_loss: 0.05683346465229988, g_loss: 6.987072944641113\n",
            "Epoch [2088/2500], Step [0/17], d_loss: 0.00709453783929348, g_loss: 7.599264144897461\n",
            "Epoch [2089/2500], Step [0/17], d_loss: 0.008770478889346123, g_loss: 7.530934810638428\n",
            "Epoch [2090/2500], Step [0/17], d_loss: 0.00489903474226594, g_loss: 7.845388412475586\n",
            "Epoch [2091/2500], Step [0/17], d_loss: 0.0047692847438156605, g_loss: 9.251853942871094\n",
            "Epoch [2092/2500], Step [0/17], d_loss: 0.0014580494025722146, g_loss: 50.50563049316406\n",
            "Epoch [2093/2500], Step [0/17], d_loss: 0.03507194668054581, g_loss: 47.80911636352539\n",
            "Epoch [2094/2500], Step [0/17], d_loss: 0.0005559572600759566, g_loss: 34.788963317871094\n",
            "Epoch [2095/2500], Step [0/17], d_loss: 0.05317695438861847, g_loss: 25.629228591918945\n",
            "Epoch [2096/2500], Step [0/17], d_loss: 0.0009835002711042762, g_loss: 15.110613822937012\n",
            "Epoch [2097/2500], Step [0/17], d_loss: 0.008914823643863201, g_loss: 11.000997543334961\n",
            "Epoch [2098/2500], Step [0/17], d_loss: 0.004675737582147121, g_loss: 13.258865356445312\n",
            "Epoch [2099/2500], Step [0/17], d_loss: 0.009422671049833298, g_loss: 27.51854705810547\n",
            "Epoch [2100/2500], Step [0/17], d_loss: 0.0031989600975066423, g_loss: 23.751693725585938\n",
            "Epoch [2101/2500], Step [0/17], d_loss: 0.018833495676517487, g_loss: 10.324642181396484\n",
            "Epoch [2102/2500], Step [0/17], d_loss: 0.006274361629039049, g_loss: 11.276317596435547\n",
            "Epoch [2103/2500], Step [0/17], d_loss: 0.01844531111419201, g_loss: 11.019234657287598\n",
            "Epoch [2104/2500], Step [0/17], d_loss: 0.011280487291514874, g_loss: 8.11974048614502\n",
            "Epoch [2105/2500], Step [0/17], d_loss: 0.009372856467962265, g_loss: 9.489471435546875\n",
            "Epoch [2106/2500], Step [0/17], d_loss: 0.03666705638170242, g_loss: 7.953055381774902\n",
            "Epoch [2107/2500], Step [0/17], d_loss: 0.012441079132258892, g_loss: 8.061615943908691\n",
            "Epoch [2108/2500], Step [0/17], d_loss: 0.01482697669416666, g_loss: 7.99137020111084\n",
            "Epoch [2109/2500], Step [0/17], d_loss: 0.08797407150268555, g_loss: 13.062417030334473\n",
            "Epoch [2110/2500], Step [0/17], d_loss: 0.01693812757730484, g_loss: 12.003144264221191\n",
            "Epoch [2111/2500], Step [0/17], d_loss: 0.007120020687580109, g_loss: 6.555367469787598\n",
            "Epoch [2112/2500], Step [0/17], d_loss: 0.0058341193944215775, g_loss: 7.079117298126221\n",
            "Epoch [2113/2500], Step [0/17], d_loss: 0.0184419434517622, g_loss: 7.901382923126221\n",
            "Epoch [2114/2500], Step [0/17], d_loss: 0.006774635054171085, g_loss: 7.704804420471191\n",
            "Epoch [2115/2500], Step [0/17], d_loss: 0.0058714947663247585, g_loss: 8.263946533203125\n",
            "Epoch [2116/2500], Step [0/17], d_loss: 0.005834683775901794, g_loss: 7.298489570617676\n",
            "Epoch [2117/2500], Step [0/17], d_loss: 0.11348368972539902, g_loss: 20.29290008544922\n",
            "Epoch [2118/2500], Step [0/17], d_loss: 0.025847507640719414, g_loss: 22.416122436523438\n",
            "Epoch [2119/2500], Step [0/17], d_loss: 0.005255196243524551, g_loss: 12.698246955871582\n",
            "Epoch [2120/2500], Step [0/17], d_loss: 0.16941183805465698, g_loss: 24.259601593017578\n",
            "Epoch [2121/2500], Step [0/17], d_loss: 0.019461236894130707, g_loss: 15.179237365722656\n",
            "Epoch [2122/2500], Step [0/17], d_loss: 0.03990665078163147, g_loss: 13.093670845031738\n",
            "Epoch [2123/2500], Step [0/17], d_loss: 0.0020297239534556866, g_loss: 9.379615783691406\n",
            "Epoch [2124/2500], Step [0/17], d_loss: 0.0013743635499849916, g_loss: 10.066144943237305\n",
            "Epoch [2125/2500], Step [0/17], d_loss: 0.0007224385626614094, g_loss: 13.231815338134766\n",
            "Epoch [2126/2500], Step [0/17], d_loss: 0.0035479310899972916, g_loss: 8.277435302734375\n",
            "Epoch [2127/2500], Step [0/17], d_loss: 0.004140128847211599, g_loss: 7.974945545196533\n",
            "Epoch [2128/2500], Step [0/17], d_loss: 0.3673582375049591, g_loss: 43.330440521240234\n",
            "Epoch [2129/2500], Step [0/17], d_loss: 0.0039016196969896555, g_loss: 37.47163391113281\n",
            "Epoch [2130/2500], Step [0/17], d_loss: 0.009870736859738827, g_loss: 20.303525924682617\n",
            "Epoch [2131/2500], Step [0/17], d_loss: 0.06293724477291107, g_loss: 25.29816246032715\n",
            "Epoch [2132/2500], Step [0/17], d_loss: 0.00896154623478651, g_loss: 13.79715633392334\n",
            "Epoch [2133/2500], Step [0/17], d_loss: 0.002423528814688325, g_loss: 10.013628005981445\n",
            "Epoch [2134/2500], Step [0/17], d_loss: 0.060028038918972015, g_loss: 24.98455047607422\n",
            "Epoch [2135/2500], Step [0/17], d_loss: 0.011268828064203262, g_loss: 20.746826171875\n",
            "Epoch [2136/2500], Step [0/17], d_loss: 0.0072803795337677, g_loss: 11.480622291564941\n",
            "Epoch [2137/2500], Step [0/17], d_loss: 0.003819848410785198, g_loss: 12.44738483428955\n",
            "Epoch [2138/2500], Step [0/17], d_loss: 0.004235530272126198, g_loss: 7.12809944152832\n",
            "Epoch [2139/2500], Step [0/17], d_loss: 0.0052813570946455, g_loss: 8.07423210144043\n",
            "Epoch [2140/2500], Step [0/17], d_loss: 0.002329102484509349, g_loss: 8.635750770568848\n",
            "Epoch [2141/2500], Step [0/17], d_loss: 0.01166300754994154, g_loss: 13.522834777832031\n",
            "Epoch [2142/2500], Step [0/17], d_loss: 0.0045869904570281506, g_loss: 9.751127243041992\n",
            "Epoch [2143/2500], Step [0/17], d_loss: 0.005372786894440651, g_loss: 7.765868186950684\n",
            "Epoch [2144/2500], Step [0/17], d_loss: 0.007438489701598883, g_loss: 7.392524719238281\n",
            "Epoch [2145/2500], Step [0/17], d_loss: 0.007563210092484951, g_loss: 7.566833019256592\n",
            "Epoch [2146/2500], Step [0/17], d_loss: 0.01610904186964035, g_loss: 9.137008666992188\n",
            "Epoch [2147/2500], Step [0/17], d_loss: 0.016277672722935677, g_loss: 8.002649307250977\n",
            "Epoch [2148/2500], Step [0/17], d_loss: 0.0035884827375411987, g_loss: 7.516404151916504\n",
            "Epoch [2149/2500], Step [0/17], d_loss: 0.0028832550160586834, g_loss: 7.651821136474609\n",
            "Epoch [2150/2500], Step [0/17], d_loss: 0.002900912659242749, g_loss: 9.025594711303711\n",
            "Epoch [2151/2500], Step [0/17], d_loss: 0.004038500599563122, g_loss: 10.034561157226562\n",
            "Epoch [2152/2500], Step [0/17], d_loss: 0.006912399083375931, g_loss: 9.110965728759766\n",
            "Epoch [2153/2500], Step [0/17], d_loss: 0.004531968850642443, g_loss: 7.489357948303223\n",
            "Epoch [2154/2500], Step [0/17], d_loss: 0.01120238658040762, g_loss: 7.8071513175964355\n",
            "Epoch [2155/2500], Step [0/17], d_loss: 0.018303725868463516, g_loss: 7.755236625671387\n",
            "Epoch [2156/2500], Step [0/17], d_loss: 0.003100010799244046, g_loss: 10.30278205871582\n",
            "Epoch [2157/2500], Step [0/17], d_loss: 0.017736120149493217, g_loss: 9.471982955932617\n",
            "Epoch [2158/2500], Step [0/17], d_loss: 0.0033459183759987354, g_loss: 11.394428253173828\n",
            "Epoch [2159/2500], Step [0/17], d_loss: 0.002381869126111269, g_loss: 8.749750137329102\n",
            "Epoch [2160/2500], Step [0/17], d_loss: 0.005828441120684147, g_loss: 8.136072158813477\n",
            "Epoch [2161/2500], Step [0/17], d_loss: 0.0003804379375651479, g_loss: 54.798789978027344\n",
            "Epoch [2162/2500], Step [0/17], d_loss: 0.0057126861065626144, g_loss: 53.427337646484375\n",
            "Epoch [2163/2500], Step [0/17], d_loss: 0.02761567011475563, g_loss: 33.858280181884766\n",
            "Epoch [2164/2500], Step [0/17], d_loss: 0.06007179245352745, g_loss: 37.02191162109375\n",
            "Epoch [2165/2500], Step [0/17], d_loss: 0.057183362543582916, g_loss: 29.372512817382812\n",
            "Epoch [2166/2500], Step [0/17], d_loss: 3.3149170875549316, g_loss: 64.46224975585938\n",
            "Epoch [2167/2500], Step [0/17], d_loss: 0.250344842672348, g_loss: 44.42301559448242\n",
            "Epoch [2168/2500], Step [0/17], d_loss: 0.2971169054508209, g_loss: 40.636714935302734\n",
            "Epoch [2169/2500], Step [0/17], d_loss: 0.07071436941623688, g_loss: 15.344505310058594\n",
            "Epoch [2170/2500], Step [0/17], d_loss: 0.015425005927681923, g_loss: 23.36321258544922\n",
            "Epoch [2171/2500], Step [0/17], d_loss: 0.12568259239196777, g_loss: 61.82757568359375\n",
            "Epoch [2172/2500], Step [0/17], d_loss: 0.30595171451568604, g_loss: 46.60886001586914\n",
            "Epoch [2173/2500], Step [0/17], d_loss: 0.009267293848097324, g_loss: 25.188770294189453\n",
            "Epoch [2174/2500], Step [0/17], d_loss: 0.11997303366661072, g_loss: 33.68476486206055\n",
            "Epoch [2175/2500], Step [0/17], d_loss: 0.04135001450777054, g_loss: 20.479595184326172\n",
            "Epoch [2176/2500], Step [0/17], d_loss: 0.10519994050264359, g_loss: 20.473758697509766\n",
            "Epoch [2177/2500], Step [0/17], d_loss: 0.004968991037458181, g_loss: 13.871637344360352\n",
            "Epoch [2178/2500], Step [0/17], d_loss: 0.007319240365177393, g_loss: 8.986648559570312\n",
            "Epoch [2179/2500], Step [0/17], d_loss: 0.023868320509791374, g_loss: 13.946828842163086\n",
            "Epoch [2180/2500], Step [0/17], d_loss: 0.007402460090816021, g_loss: 9.429445266723633\n",
            "Epoch [2181/2500], Step [0/17], d_loss: 0.5463661551475525, g_loss: 50.90895080566406\n",
            "Epoch [2182/2500], Step [0/17], d_loss: 9.830426279222593e-06, g_loss: 36.599212646484375\n",
            "Epoch [2183/2500], Step [0/17], d_loss: 0.00584523007273674, g_loss: 43.4570426940918\n",
            "Epoch [2184/2500], Step [0/17], d_loss: 0.046628497540950775, g_loss: 40.41242218017578\n",
            "Epoch [2185/2500], Step [0/17], d_loss: 0.01201751921325922, g_loss: 30.999752044677734\n",
            "Epoch [2186/2500], Step [0/17], d_loss: 0.006428129971027374, g_loss: 18.279552459716797\n",
            "Epoch [2187/2500], Step [0/17], d_loss: 0.004773871041834354, g_loss: 8.217723846435547\n",
            "Epoch [2188/2500], Step [0/17], d_loss: 0.004068932496011257, g_loss: 8.922908782958984\n",
            "Epoch [2189/2500], Step [0/17], d_loss: 0.018482699990272522, g_loss: 9.272749900817871\n",
            "Epoch [2190/2500], Step [0/17], d_loss: 0.002631921088322997, g_loss: 9.225654602050781\n",
            "Epoch [2191/2500], Step [0/17], d_loss: 0.00912532676011324, g_loss: 8.528451919555664\n",
            "Epoch [2192/2500], Step [0/17], d_loss: 0.0362793505191803, g_loss: 10.951852798461914\n",
            "Epoch [2193/2500], Step [0/17], d_loss: 0.03615124523639679, g_loss: 7.696489334106445\n",
            "Epoch [2194/2500], Step [0/17], d_loss: 0.01802530325949192, g_loss: 8.685722351074219\n",
            "Epoch [2195/2500], Step [0/17], d_loss: 0.012738791294395924, g_loss: 20.82878875732422\n",
            "Epoch [2196/2500], Step [0/17], d_loss: 0.02009315975010395, g_loss: 14.511180877685547\n",
            "Epoch [2197/2500], Step [0/17], d_loss: 0.017477378249168396, g_loss: 11.430215835571289\n",
            "Epoch [2198/2500], Step [0/17], d_loss: 0.008647145703434944, g_loss: 7.731988906860352\n",
            "Epoch [2199/2500], Step [0/17], d_loss: 0.02201436273753643, g_loss: 19.498828887939453\n",
            "Epoch [2200/2500], Step [0/17], d_loss: 0.010842552408576012, g_loss: 12.03569221496582\n",
            "Epoch [2201/2500], Step [0/17], d_loss: 0.01448371447622776, g_loss: 7.551051139831543\n",
            "Epoch [2202/2500], Step [0/17], d_loss: 0.03523344174027443, g_loss: 8.3953218460083\n",
            "Epoch [2203/2500], Step [0/17], d_loss: 0.002592009026557207, g_loss: 9.535017967224121\n",
            "Epoch [2204/2500], Step [0/17], d_loss: 0.007640952710062265, g_loss: 8.6068115234375\n",
            "Epoch [2205/2500], Step [0/17], d_loss: 0.008708133362233639, g_loss: 6.637758255004883\n",
            "Epoch [2206/2500], Step [0/17], d_loss: 0.01031271368265152, g_loss: 6.57256555557251\n",
            "Epoch [2207/2500], Step [0/17], d_loss: 0.03335866332054138, g_loss: 8.843457221984863\n",
            "Epoch [2208/2500], Step [0/17], d_loss: 0.016423586755990982, g_loss: 7.533840179443359\n",
            "Epoch [2209/2500], Step [0/17], d_loss: 0.026915332302451134, g_loss: 6.294043064117432\n",
            "Epoch [2210/2500], Step [0/17], d_loss: 0.005535604432225227, g_loss: 8.145301818847656\n",
            "Epoch [2211/2500], Step [0/17], d_loss: 0.002888654824346304, g_loss: 8.865703582763672\n",
            "Epoch [2212/2500], Step [0/17], d_loss: 0.0019072729628533125, g_loss: 7.762214660644531\n",
            "Epoch [2213/2500], Step [0/17], d_loss: 0.005631773266941309, g_loss: 7.243100166320801\n",
            "Epoch [2214/2500], Step [0/17], d_loss: 0.038370613008737564, g_loss: 18.657733917236328\n",
            "Epoch [2215/2500], Step [0/17], d_loss: 0.009176942519843578, g_loss: 6.8852362632751465\n",
            "Epoch [2216/2500], Step [0/17], d_loss: 0.0157126747071743, g_loss: 6.690409183502197\n",
            "Epoch [2217/2500], Step [0/17], d_loss: 0.0042849015444517136, g_loss: 8.264847755432129\n",
            "Epoch [2218/2500], Step [0/17], d_loss: 0.0070782871916890144, g_loss: 7.772157192230225\n",
            "Epoch [2219/2500], Step [0/17], d_loss: 0.055875468999147415, g_loss: 13.433944702148438\n",
            "Epoch [2220/2500], Step [0/17], d_loss: 0.004008959047496319, g_loss: 13.229135513305664\n",
            "Epoch [2221/2500], Step [0/17], d_loss: 0.001409495365805924, g_loss: 22.656278610229492\n",
            "Epoch [2222/2500], Step [0/17], d_loss: 0.0018862956203520298, g_loss: 14.339713096618652\n",
            "Epoch [2223/2500], Step [0/17], d_loss: 0.008835634216666222, g_loss: 13.823593139648438\n",
            "Epoch [2224/2500], Step [0/17], d_loss: 0.008120628073811531, g_loss: 7.400144577026367\n",
            "Epoch [2225/2500], Step [0/17], d_loss: 0.004285905510187149, g_loss: 7.214632987976074\n",
            "Epoch [2226/2500], Step [0/17], d_loss: 0.017634330317378044, g_loss: 7.743596076965332\n",
            "Epoch [2227/2500], Step [0/17], d_loss: 0.0055260080844163895, g_loss: 8.19070053100586\n",
            "Epoch [2228/2500], Step [0/17], d_loss: 0.03498488664627075, g_loss: 10.684866905212402\n",
            "Epoch [2229/2500], Step [0/17], d_loss: 0.018106695264577866, g_loss: 7.669538497924805\n",
            "Epoch [2230/2500], Step [0/17], d_loss: 0.00726662203669548, g_loss: 8.579425811767578\n",
            "Epoch [2231/2500], Step [0/17], d_loss: 0.018691040575504303, g_loss: 11.50107192993164\n",
            "Epoch [2232/2500], Step [0/17], d_loss: 0.009191667661070824, g_loss: 7.520907402038574\n",
            "Epoch [2233/2500], Step [0/17], d_loss: 8.213414548663422e-05, g_loss: 55.595523834228516\n",
            "Epoch [2234/2500], Step [0/17], d_loss: 0.07505600154399872, g_loss: 41.74067306518555\n",
            "Epoch [2235/2500], Step [0/17], d_loss: 0.9196471571922302, g_loss: 53.010162353515625\n",
            "Epoch [2236/2500], Step [0/17], d_loss: 0.16801118850708008, g_loss: 41.90578842163086\n",
            "Epoch [2237/2500], Step [0/17], d_loss: 0.06225895881652832, g_loss: 25.65614128112793\n",
            "Epoch [2238/2500], Step [0/17], d_loss: 0.0001731480733724311, g_loss: 31.749391555786133\n",
            "Epoch [2239/2500], Step [0/17], d_loss: 0.07497374713420868, g_loss: 17.629173278808594\n",
            "Epoch [2240/2500], Step [0/17], d_loss: 0.050509121268987656, g_loss: 11.278680801391602\n",
            "Epoch [2241/2500], Step [0/17], d_loss: 0.008880523964762688, g_loss: 8.335589408874512\n",
            "Epoch [2242/2500], Step [0/17], d_loss: 0.008941883221268654, g_loss: 20.16473388671875\n",
            "Epoch [2243/2500], Step [0/17], d_loss: 0.02912396751344204, g_loss: 17.1309814453125\n",
            "Epoch [2244/2500], Step [0/17], d_loss: 0.012791527435183525, g_loss: 8.337135314941406\n",
            "Epoch [2245/2500], Step [0/17], d_loss: 0.007152035366743803, g_loss: 38.167083740234375\n",
            "Epoch [2246/2500], Step [0/17], d_loss: 0.025144100189208984, g_loss: 22.375164031982422\n",
            "Epoch [2247/2500], Step [0/17], d_loss: 0.07199277728796005, g_loss: 19.412935256958008\n",
            "Epoch [2248/2500], Step [0/17], d_loss: 0.008500689640641212, g_loss: 11.37674331665039\n",
            "Epoch [2249/2500], Step [0/17], d_loss: 0.010984672233462334, g_loss: 7.881570816040039\n",
            "Epoch [2250/2500], Step [0/17], d_loss: 0.016854435205459595, g_loss: 7.422318458557129\n",
            "Epoch [2251/2500], Step [0/17], d_loss: 0.004417107440531254, g_loss: 7.932564735412598\n",
            "Epoch [2252/2500], Step [0/17], d_loss: 0.011694918386638165, g_loss: 26.793832778930664\n",
            "Epoch [2253/2500], Step [0/17], d_loss: 0.006475868169218302, g_loss: 19.42127227783203\n",
            "Epoch [2254/2500], Step [0/17], d_loss: 0.0461326465010643, g_loss: 14.952943801879883\n",
            "Epoch [2255/2500], Step [0/17], d_loss: 0.01108617801219225, g_loss: 7.226833820343018\n",
            "Epoch [2256/2500], Step [0/17], d_loss: 0.008428934030234814, g_loss: 7.772235870361328\n",
            "Epoch [2257/2500], Step [0/17], d_loss: 0.006948085967451334, g_loss: 7.769862174987793\n",
            "Epoch [2258/2500], Step [0/17], d_loss: 0.003623467404395342, g_loss: 13.337261199951172\n",
            "Epoch [2259/2500], Step [0/17], d_loss: 0.28303298354148865, g_loss: 30.50519561767578\n",
            "Epoch [2260/2500], Step [0/17], d_loss: 0.009774337522685528, g_loss: 27.469743728637695\n",
            "Epoch [2261/2500], Step [0/17], d_loss: 0.015397376380860806, g_loss: 15.166776657104492\n",
            "Epoch [2262/2500], Step [0/17], d_loss: 0.019749095663428307, g_loss: 8.76764965057373\n",
            "Epoch [2263/2500], Step [0/17], d_loss: 0.3476470112800598, g_loss: 32.61676788330078\n",
            "Epoch [2264/2500], Step [0/17], d_loss: 0.002582163317129016, g_loss: 19.832233428955078\n",
            "Epoch [2265/2500], Step [0/17], d_loss: 0.002640423132106662, g_loss: 11.025161743164062\n",
            "Epoch [2266/2500], Step [0/17], d_loss: 0.004694229923188686, g_loss: 8.51718521118164\n",
            "Epoch [2267/2500], Step [0/17], d_loss: 0.021847248077392578, g_loss: 8.564194679260254\n",
            "Epoch [2268/2500], Step [0/17], d_loss: 0.010896905325353146, g_loss: 7.45375919342041\n",
            "Epoch [2269/2500], Step [0/17], d_loss: 0.0030297173652797937, g_loss: 33.143646240234375\n",
            "Epoch [2270/2500], Step [0/17], d_loss: 0.09898890554904938, g_loss: 24.54635238647461\n",
            "Epoch [2271/2500], Step [0/17], d_loss: 0.006483706645667553, g_loss: 30.008989334106445\n",
            "Epoch [2272/2500], Step [0/17], d_loss: 0.006362231448292732, g_loss: 15.258964538574219\n",
            "Epoch [2273/2500], Step [0/17], d_loss: 0.029774853959679604, g_loss: 10.813929557800293\n",
            "Epoch [2274/2500], Step [0/17], d_loss: 0.008450156077742577, g_loss: 15.566805839538574\n",
            "Epoch [2275/2500], Step [0/17], d_loss: 0.014423003420233727, g_loss: 8.854424476623535\n",
            "Epoch [2276/2500], Step [0/17], d_loss: 0.010508503764867783, g_loss: 13.394908905029297\n",
            "Epoch [2277/2500], Step [0/17], d_loss: 0.019715730100870132, g_loss: 7.047593116760254\n",
            "Epoch [2278/2500], Step [0/17], d_loss: 0.011338023468852043, g_loss: 7.535870552062988\n",
            "Epoch [2279/2500], Step [0/17], d_loss: 0.004468271043151617, g_loss: 7.858116149902344\n",
            "Epoch [2280/2500], Step [0/17], d_loss: 0.006224155426025391, g_loss: 8.282121658325195\n",
            "Epoch [2281/2500], Step [0/17], d_loss: 0.011071967892348766, g_loss: 6.9240570068359375\n",
            "Epoch [2282/2500], Step [0/17], d_loss: 0.007001821417361498, g_loss: 6.771195411682129\n",
            "Epoch [2283/2500], Step [0/17], d_loss: 0.010324145667254925, g_loss: 11.794697761535645\n",
            "Epoch [2284/2500], Step [0/17], d_loss: 0.00502689927816391, g_loss: 8.01388168334961\n",
            "Epoch [2285/2500], Step [0/17], d_loss: 0.005455566570162773, g_loss: 6.426364421844482\n",
            "Epoch [2286/2500], Step [0/17], d_loss: 0.0026601876597851515, g_loss: 9.372533798217773\n",
            "Epoch [2287/2500], Step [0/17], d_loss: 0.006515682674944401, g_loss: 7.554384708404541\n",
            "Epoch [2288/2500], Step [0/17], d_loss: 0.008875887840986252, g_loss: 7.586828231811523\n",
            "Epoch [2289/2500], Step [0/17], d_loss: 0.015501179732382298, g_loss: 8.314709663391113\n",
            "Epoch [2290/2500], Step [0/17], d_loss: 0.008090343326330185, g_loss: 7.559124946594238\n",
            "Epoch [2291/2500], Step [0/17], d_loss: 0.06744565069675446, g_loss: 7.161124229431152\n",
            "Epoch [2292/2500], Step [0/17], d_loss: 0.007009036839008331, g_loss: 7.879216194152832\n",
            "Epoch [2293/2500], Step [0/17], d_loss: 0.0010811284882947803, g_loss: 10.596674919128418\n",
            "Epoch [2294/2500], Step [0/17], d_loss: 0.009314410388469696, g_loss: 6.918617248535156\n",
            "Epoch [2295/2500], Step [0/17], d_loss: 0.0075350673869252205, g_loss: 9.971685409545898\n",
            "Epoch [2296/2500], Step [0/17], d_loss: 0.003470733528956771, g_loss: 10.36728286743164\n",
            "Epoch [2297/2500], Step [0/17], d_loss: 0.021147996187210083, g_loss: 8.84493350982666\n",
            "Epoch [2298/2500], Step [0/17], d_loss: 0.20376835763454437, g_loss: 24.4393310546875\n",
            "Epoch [2299/2500], Step [0/17], d_loss: 0.04423011094331741, g_loss: 23.144845962524414\n",
            "Epoch [2300/2500], Step [0/17], d_loss: 0.00397502351552248, g_loss: 17.752376556396484\n",
            "Epoch [2301/2500], Step [0/17], d_loss: 0.005087078083306551, g_loss: 16.34328269958496\n",
            "Epoch [2302/2500], Step [0/17], d_loss: 0.01798991486430168, g_loss: 8.052868843078613\n",
            "Epoch [2303/2500], Step [0/17], d_loss: 0.010222082957625389, g_loss: 9.019013404846191\n",
            "Epoch [2304/2500], Step [0/17], d_loss: 0.016061196103692055, g_loss: 7.210668087005615\n",
            "Epoch [2305/2500], Step [0/17], d_loss: 0.004284850787371397, g_loss: 10.307147979736328\n",
            "Epoch [2306/2500], Step [0/17], d_loss: 0.003954987041652203, g_loss: 8.504533767700195\n",
            "Epoch [2307/2500], Step [0/17], d_loss: 0.017588095739483833, g_loss: 7.444417953491211\n",
            "Epoch [2308/2500], Step [0/17], d_loss: 0.005299300886690617, g_loss: 7.019576072692871\n",
            "Epoch [2309/2500], Step [0/17], d_loss: 0.008514312095940113, g_loss: 6.6024017333984375\n",
            "Epoch [2310/2500], Step [0/17], d_loss: 0.0005767536349594593, g_loss: 15.254878997802734\n",
            "Epoch [2311/2500], Step [0/17], d_loss: 0.007605420425534248, g_loss: 14.54719352722168\n",
            "Epoch [2312/2500], Step [0/17], d_loss: 0.01826127991080284, g_loss: 11.195308685302734\n",
            "Epoch [2313/2500], Step [0/17], d_loss: 0.0056750052608549595, g_loss: 10.049537658691406\n",
            "Epoch [2314/2500], Step [0/17], d_loss: 0.008217799477279186, g_loss: 9.31141471862793\n",
            "Epoch [2315/2500], Step [0/17], d_loss: 0.006688843481242657, g_loss: 8.402658462524414\n",
            "Epoch [2316/2500], Step [0/17], d_loss: 0.008751517161726952, g_loss: 8.677600860595703\n",
            "Epoch [2317/2500], Step [0/17], d_loss: 0.015148559585213661, g_loss: 7.914030075073242\n",
            "Epoch [2318/2500], Step [0/17], d_loss: 0.009543461725115776, g_loss: 16.771387100219727\n",
            "Epoch [2319/2500], Step [0/17], d_loss: 0.007392453029751778, g_loss: 10.263931274414062\n",
            "Epoch [2320/2500], Step [0/17], d_loss: 0.037762850522994995, g_loss: 9.782149314880371\n",
            "Epoch [2321/2500], Step [0/17], d_loss: 0.0088669303804636, g_loss: 7.296561241149902\n",
            "Epoch [2322/2500], Step [0/17], d_loss: 0.020417382940649986, g_loss: 15.427578926086426\n",
            "Epoch [2323/2500], Step [0/17], d_loss: 0.0014872963074594736, g_loss: 18.747459411621094\n",
            "Epoch [2324/2500], Step [0/17], d_loss: 0.012581164948642254, g_loss: 10.284961700439453\n",
            "Epoch [2325/2500], Step [0/17], d_loss: 0.0033012055791914463, g_loss: 8.174076080322266\n",
            "Epoch [2326/2500], Step [0/17], d_loss: 0.008614879101514816, g_loss: 8.110282897949219\n",
            "Epoch [2327/2500], Step [0/17], d_loss: 0.014054707251489162, g_loss: 7.867341995239258\n",
            "Epoch [2328/2500], Step [0/17], d_loss: 0.00690267002210021, g_loss: 13.31126594543457\n",
            "Epoch [2329/2500], Step [0/17], d_loss: 0.004280663561075926, g_loss: 10.36074161529541\n",
            "Epoch [2330/2500], Step [0/17], d_loss: 0.015530591830611229, g_loss: 13.185599327087402\n",
            "Epoch [2331/2500], Step [0/17], d_loss: 0.009971177205443382, g_loss: 11.308135986328125\n",
            "Epoch [2332/2500], Step [0/17], d_loss: 0.01394823007285595, g_loss: 7.101698875427246\n",
            "Epoch [2333/2500], Step [0/17], d_loss: 0.004388213157653809, g_loss: 6.901064395904541\n",
            "Epoch [2334/2500], Step [0/17], d_loss: 0.011270520277321339, g_loss: 14.950387954711914\n",
            "Epoch [2335/2500], Step [0/17], d_loss: 0.007863768376410007, g_loss: 8.023181915283203\n",
            "Epoch [2336/2500], Step [0/17], d_loss: 0.015380912460386753, g_loss: 6.883928298950195\n",
            "Epoch [2337/2500], Step [0/17], d_loss: 0.004067227244377136, g_loss: 7.617392063140869\n",
            "Epoch [2338/2500], Step [0/17], d_loss: 0.0019047728274017572, g_loss: 8.3094482421875\n",
            "Epoch [2339/2500], Step [0/17], d_loss: 0.007679719477891922, g_loss: 7.4674882888793945\n",
            "Epoch [2340/2500], Step [0/17], d_loss: 0.0066063376143574715, g_loss: 7.672881603240967\n",
            "Epoch [2341/2500], Step [0/17], d_loss: 0.0016096474137157202, g_loss: 11.264579772949219\n",
            "Epoch [2342/2500], Step [0/17], d_loss: 0.004751106258481741, g_loss: 10.417055130004883\n",
            "Epoch [2343/2500], Step [0/17], d_loss: 0.010253976099193096, g_loss: 8.332979202270508\n",
            "Epoch [2344/2500], Step [0/17], d_loss: 0.0016785981133580208, g_loss: 8.50845718383789\n",
            "Epoch [2345/2500], Step [0/17], d_loss: 0.003389658872038126, g_loss: 14.088767051696777\n",
            "Epoch [2346/2500], Step [0/17], d_loss: 0.006202236283570528, g_loss: 13.934073448181152\n",
            "Epoch [2347/2500], Step [0/17], d_loss: 0.005556018091738224, g_loss: 12.28589153289795\n",
            "Epoch [2348/2500], Step [0/17], d_loss: 0.014670701697468758, g_loss: 7.5244035720825195\n",
            "Epoch [2349/2500], Step [0/17], d_loss: 0.005290774628520012, g_loss: 7.212850093841553\n",
            "Epoch [2350/2500], Step [0/17], d_loss: 0.1945592164993286, g_loss: 28.95110511779785\n",
            "Epoch [2351/2500], Step [0/17], d_loss: 5.936514935456216e-05, g_loss: 63.49550247192383\n",
            "Epoch [2352/2500], Step [0/17], d_loss: 1.2213097761559766e-05, g_loss: 52.36549377441406\n",
            "Epoch [2353/2500], Step [0/17], d_loss: 0.004101562779396772, g_loss: 27.612102508544922\n",
            "Epoch [2354/2500], Step [0/17], d_loss: 0.13332004845142365, g_loss: 31.615400314331055\n",
            "Epoch [2355/2500], Step [0/17], d_loss: 0.089554563164711, g_loss: 23.95146942138672\n",
            "Epoch [2356/2500], Step [0/17], d_loss: 0.005822223611176014, g_loss: 29.52272605895996\n",
            "Epoch [2357/2500], Step [0/17], d_loss: 0.12651285529136658, g_loss: 43.84881591796875\n",
            "Epoch [2358/2500], Step [0/17], d_loss: 0.0014563770964741707, g_loss: 24.46611785888672\n",
            "Epoch [2359/2500], Step [0/17], d_loss: 0.01164427399635315, g_loss: 16.44371795654297\n",
            "Epoch [2360/2500], Step [0/17], d_loss: 0.0037932912819087505, g_loss: 10.617027282714844\n",
            "Epoch [2361/2500], Step [0/17], d_loss: 0.012506471946835518, g_loss: 7.01788330078125\n",
            "Epoch [2362/2500], Step [0/17], d_loss: 0.005488521885126829, g_loss: 15.618148803710938\n",
            "Epoch [2363/2500], Step [0/17], d_loss: 0.0030343043617904186, g_loss: 11.006943702697754\n",
            "Epoch [2364/2500], Step [0/17], d_loss: 0.024736061692237854, g_loss: 14.231101989746094\n",
            "Epoch [2365/2500], Step [0/17], d_loss: 0.000777164357714355, g_loss: 12.591599464416504\n",
            "Epoch [2366/2500], Step [0/17], d_loss: 0.02416180819272995, g_loss: 7.168745517730713\n",
            "Epoch [2367/2500], Step [0/17], d_loss: 0.0032357582822442055, g_loss: 10.601157188415527\n",
            "Epoch [2368/2500], Step [0/17], d_loss: 0.005133333615958691, g_loss: 8.709637641906738\n",
            "Epoch [2369/2500], Step [0/17], d_loss: 0.003743431530892849, g_loss: 8.604509353637695\n",
            "Epoch [2370/2500], Step [0/17], d_loss: 0.009261637926101685, g_loss: 11.560652732849121\n",
            "Epoch [2371/2500], Step [0/17], d_loss: 0.0013010856928303838, g_loss: 11.373922348022461\n",
            "Epoch [2372/2500], Step [0/17], d_loss: 0.0021761171519756317, g_loss: 10.6423921585083\n",
            "Epoch [2373/2500], Step [0/17], d_loss: 0.0026528500020503998, g_loss: 9.248153686523438\n",
            "Epoch [2374/2500], Step [0/17], d_loss: 0.029033640399575233, g_loss: 6.320240020751953\n",
            "Epoch [2375/2500], Step [0/17], d_loss: 0.038148827850818634, g_loss: 9.852734565734863\n",
            "Epoch [2376/2500], Step [0/17], d_loss: 0.004809353034943342, g_loss: 8.922317504882812\n",
            "Epoch [2377/2500], Step [0/17], d_loss: 0.0019723682198673487, g_loss: 10.55029010772705\n",
            "Epoch [2378/2500], Step [0/17], d_loss: 0.0053274077363312244, g_loss: 7.149754047393799\n",
            "Epoch [2379/2500], Step [0/17], d_loss: 0.20598453283309937, g_loss: 33.869503021240234\n",
            "Epoch [2380/2500], Step [0/17], d_loss: 0.013922560960054398, g_loss: 27.607912063598633\n",
            "Epoch [2381/2500], Step [0/17], d_loss: 0.002468678168952465, g_loss: 22.06618881225586\n",
            "Epoch [2382/2500], Step [0/17], d_loss: 0.0013929044362157583, g_loss: 15.594522476196289\n",
            "Epoch [2383/2500], Step [0/17], d_loss: 0.011515103280544281, g_loss: 7.27885103225708\n",
            "Epoch [2384/2500], Step [0/17], d_loss: 0.01191265881061554, g_loss: 7.201164245605469\n",
            "Epoch [2385/2500], Step [0/17], d_loss: 0.036790501326322556, g_loss: 11.41783618927002\n",
            "Epoch [2386/2500], Step [0/17], d_loss: 0.004714214242994785, g_loss: 7.887275695800781\n",
            "Epoch [2387/2500], Step [0/17], d_loss: 0.0021767346188426018, g_loss: 8.720754623413086\n",
            "Epoch [2388/2500], Step [0/17], d_loss: 0.0012383477296680212, g_loss: 46.53569793701172\n",
            "Epoch [2389/2500], Step [0/17], d_loss: 0.006048537325114012, g_loss: 28.87662696838379\n",
            "Epoch [2390/2500], Step [0/17], d_loss: 0.002507956000044942, g_loss: 15.626219749450684\n",
            "Epoch [2391/2500], Step [0/17], d_loss: 0.025358155369758606, g_loss: 12.933597564697266\n",
            "Epoch [2392/2500], Step [0/17], d_loss: 0.028316842392086983, g_loss: 7.441011428833008\n",
            "Epoch [2393/2500], Step [0/17], d_loss: 0.03706291690468788, g_loss: 24.077804565429688\n",
            "Epoch [2394/2500], Step [0/17], d_loss: 0.018526306375861168, g_loss: 18.91134262084961\n",
            "Epoch [2395/2500], Step [0/17], d_loss: 0.0008835794869810343, g_loss: 12.391901016235352\n",
            "Epoch [2396/2500], Step [0/17], d_loss: 0.0038860654458403587, g_loss: 10.420059204101562\n",
            "Epoch [2397/2500], Step [0/17], d_loss: 0.004435327835381031, g_loss: 9.934087753295898\n",
            "Epoch [2398/2500], Step [0/17], d_loss: 0.004285130649805069, g_loss: 14.502045631408691\n",
            "Epoch [2399/2500], Step [0/17], d_loss: 0.003325928933918476, g_loss: 7.977458477020264\n",
            "Epoch [2400/2500], Step [0/17], d_loss: 0.016447903588414192, g_loss: 8.51307487487793\n",
            "Epoch [2401/2500], Step [0/17], d_loss: 0.0035733026452362537, g_loss: 7.378026962280273\n",
            "Epoch [2402/2500], Step [0/17], d_loss: 0.0011987572070211172, g_loss: 9.77821159362793\n",
            "Epoch [2403/2500], Step [0/17], d_loss: 0.005910366773605347, g_loss: 8.073473930358887\n",
            "Epoch [2404/2500], Step [0/17], d_loss: 0.01232851855456829, g_loss: 8.013436317443848\n",
            "Epoch [2405/2500], Step [0/17], d_loss: 0.003672827035188675, g_loss: 31.797229766845703\n",
            "Epoch [2406/2500], Step [0/17], d_loss: 0.001607005950063467, g_loss: 27.60943031311035\n",
            "Epoch [2407/2500], Step [0/17], d_loss: 0.0018697917694225907, g_loss: 17.164127349853516\n",
            "Epoch [2408/2500], Step [0/17], d_loss: 0.019236598163843155, g_loss: 7.480895042419434\n",
            "Epoch [2409/2500], Step [0/17], d_loss: 0.0015265134861692786, g_loss: 11.322189331054688\n",
            "Epoch [2410/2500], Step [0/17], d_loss: 0.0011386517435312271, g_loss: 9.703421592712402\n",
            "Epoch [2411/2500], Step [0/17], d_loss: 0.0018254572059959173, g_loss: 8.86584186553955\n",
            "Epoch [2412/2500], Step [0/17], d_loss: 0.04547207057476044, g_loss: 13.704221725463867\n",
            "Epoch [2413/2500], Step [0/17], d_loss: 0.02609618380665779, g_loss: 14.909013748168945\n",
            "Epoch [2414/2500], Step [0/17], d_loss: 0.00698029063642025, g_loss: 11.002641677856445\n",
            "Epoch [2415/2500], Step [0/17], d_loss: 0.009561600163578987, g_loss: 10.529539108276367\n",
            "Epoch [2416/2500], Step [0/17], d_loss: 0.0023480532690882683, g_loss: 11.20052719116211\n",
            "Epoch [2417/2500], Step [0/17], d_loss: 0.002517442684620619, g_loss: 10.295193672180176\n",
            "Epoch [2418/2500], Step [0/17], d_loss: 0.001399862114340067, g_loss: 15.312732696533203\n",
            "Epoch [2419/2500], Step [0/17], d_loss: 0.0073041426949203014, g_loss: 11.881875038146973\n",
            "Epoch [2420/2500], Step [0/17], d_loss: 0.01314203068614006, g_loss: 8.150723457336426\n",
            "Epoch [2421/2500], Step [0/17], d_loss: 0.06697976589202881, g_loss: 16.9251651763916\n",
            "Epoch [2422/2500], Step [0/17], d_loss: 0.0019891266711056232, g_loss: 49.08714294433594\n",
            "Epoch [2423/2500], Step [0/17], d_loss: 0.0008754211594350636, g_loss: 52.84748077392578\n",
            "Epoch [2424/2500], Step [0/17], d_loss: 0.2848520278930664, g_loss: 51.086212158203125\n",
            "Epoch [2425/2500], Step [0/17], d_loss: 0.0051934923976659775, g_loss: 32.47233581542969\n",
            "Epoch [2426/2500], Step [0/17], d_loss: 0.007132095750421286, g_loss: 48.01643371582031\n",
            "Epoch [2427/2500], Step [0/17], d_loss: 0.08241666853427887, g_loss: 47.981971740722656\n",
            "Epoch [2428/2500], Step [0/17], d_loss: 0.02102523297071457, g_loss: 20.470930099487305\n",
            "Epoch [2429/2500], Step [0/17], d_loss: 0.39231348037719727, g_loss: 50.27748489379883\n",
            "Epoch [2430/2500], Step [0/17], d_loss: 0.03394834324717522, g_loss: 30.495670318603516\n",
            "Epoch [2431/2500], Step [0/17], d_loss: 0.06310537457466125, g_loss: 17.365699768066406\n",
            "Epoch [2432/2500], Step [0/17], d_loss: 0.23655354976654053, g_loss: 29.432558059692383\n",
            "Epoch [2433/2500], Step [0/17], d_loss: 0.0026814970187842846, g_loss: 23.29697608947754\n",
            "Epoch [2434/2500], Step [0/17], d_loss: 0.007888360880315304, g_loss: 16.67233657836914\n",
            "Epoch [2435/2500], Step [0/17], d_loss: 0.006838737986981869, g_loss: 18.59095001220703\n",
            "Epoch [2436/2500], Step [0/17], d_loss: 0.01297044474631548, g_loss: 10.166658401489258\n",
            "Epoch [2437/2500], Step [0/17], d_loss: 0.00787128321826458, g_loss: 12.21487808227539\n",
            "Epoch [2438/2500], Step [0/17], d_loss: 0.015957118943333626, g_loss: 13.741267204284668\n",
            "Epoch [2439/2500], Step [0/17], d_loss: 0.011565139517188072, g_loss: 11.90192985534668\n",
            "Epoch [2440/2500], Step [0/17], d_loss: 0.006578856613487005, g_loss: 9.021206855773926\n",
            "Epoch [2441/2500], Step [0/17], d_loss: 0.08196575939655304, g_loss: 14.084091186523438\n",
            "Epoch [2442/2500], Step [0/17], d_loss: 0.0009404374286532402, g_loss: 16.488609313964844\n",
            "Epoch [2443/2500], Step [0/17], d_loss: 0.041981589049100876, g_loss: 11.904597282409668\n",
            "Epoch [2444/2500], Step [0/17], d_loss: 0.001267807325348258, g_loss: 30.82906150817871\n",
            "Epoch [2445/2500], Step [0/17], d_loss: 0.05057155340909958, g_loss: 23.621822357177734\n",
            "Epoch [2446/2500], Step [0/17], d_loss: 0.016668803989887238, g_loss: 15.442014694213867\n",
            "Epoch [2447/2500], Step [0/17], d_loss: 0.0021461055148392916, g_loss: 9.795663833618164\n",
            "Epoch [2448/2500], Step [0/17], d_loss: 0.0034179019276052713, g_loss: 9.756032943725586\n",
            "Epoch [2449/2500], Step [0/17], d_loss: 0.004423321224749088, g_loss: 6.803025245666504\n",
            "Epoch [2450/2500], Step [0/17], d_loss: 0.022198505699634552, g_loss: 17.48334503173828\n",
            "Epoch [2451/2500], Step [0/17], d_loss: 0.012055732309818268, g_loss: 7.51283073425293\n",
            "Epoch [2452/2500], Step [0/17], d_loss: 0.00308666517958045, g_loss: 14.144369125366211\n",
            "Epoch [2453/2500], Step [0/17], d_loss: 0.008257489651441574, g_loss: 8.459173202514648\n",
            "Epoch [2454/2500], Step [0/17], d_loss: 0.01484161987900734, g_loss: 21.109830856323242\n",
            "Epoch [2455/2500], Step [0/17], d_loss: 0.0058283074758946896, g_loss: 15.88560676574707\n",
            "Epoch [2456/2500], Step [0/17], d_loss: 0.008002144284546375, g_loss: 13.490337371826172\n",
            "Epoch [2457/2500], Step [0/17], d_loss: 0.008616865612566471, g_loss: 7.852719306945801\n",
            "Epoch [2458/2500], Step [0/17], d_loss: 0.007433395832777023, g_loss: 11.098752975463867\n",
            "Epoch [2459/2500], Step [0/17], d_loss: 0.01111612282693386, g_loss: 8.34238052368164\n",
            "Epoch [2460/2500], Step [0/17], d_loss: 0.02966238372027874, g_loss: 9.41978931427002\n",
            "Epoch [2461/2500], Step [0/17], d_loss: 0.01878300867974758, g_loss: 7.963216781616211\n",
            "Epoch [2462/2500], Step [0/17], d_loss: 0.006132661364972591, g_loss: 8.771846771240234\n",
            "Epoch [2463/2500], Step [0/17], d_loss: 0.004972349852323532, g_loss: 7.444877624511719\n",
            "Epoch [2464/2500], Step [0/17], d_loss: 0.004909529350697994, g_loss: 11.620353698730469\n",
            "Epoch [2465/2500], Step [0/17], d_loss: 0.01544985268265009, g_loss: 11.179672241210938\n",
            "Epoch [2466/2500], Step [0/17], d_loss: 0.013436207547783852, g_loss: 16.745283126831055\n",
            "Epoch [2467/2500], Step [0/17], d_loss: 0.025736860930919647, g_loss: 17.92407989501953\n",
            "Epoch [2468/2500], Step [0/17], d_loss: 0.004590396769344807, g_loss: 15.741320610046387\n",
            "Epoch [2469/2500], Step [0/17], d_loss: 0.011178318411111832, g_loss: 7.497982501983643\n",
            "Epoch [2470/2500], Step [0/17], d_loss: 0.01293813157826662, g_loss: 7.426046848297119\n",
            "Epoch [2471/2500], Step [0/17], d_loss: 0.0072622401639819145, g_loss: 7.542306423187256\n",
            "Epoch [2472/2500], Step [0/17], d_loss: 0.0990251749753952, g_loss: 17.655174255371094\n",
            "Epoch [2473/2500], Step [0/17], d_loss: 0.0047905161045491695, g_loss: 18.25189208984375\n",
            "Epoch [2474/2500], Step [0/17], d_loss: 0.0022599417716264725, g_loss: 11.507826805114746\n",
            "Epoch [2475/2500], Step [0/17], d_loss: 0.0025378090795129538, g_loss: 15.560731887817383\n",
            "Epoch [2476/2500], Step [0/17], d_loss: 0.002478900598362088, g_loss: 8.722832679748535\n",
            "Epoch [2477/2500], Step [0/17], d_loss: 0.0030242428183555603, g_loss: 8.747678756713867\n",
            "Epoch [2478/2500], Step [0/17], d_loss: 0.0050440398044884205, g_loss: 7.18287467956543\n",
            "Epoch [2479/2500], Step [0/17], d_loss: 0.008932607248425484, g_loss: 6.9554443359375\n",
            "Epoch [2480/2500], Step [0/17], d_loss: 0.001821319106966257, g_loss: 11.399715423583984\n",
            "Epoch [2481/2500], Step [0/17], d_loss: 0.00560386385768652, g_loss: 9.691495895385742\n",
            "Epoch [2482/2500], Step [0/17], d_loss: 0.004515077918767929, g_loss: 7.0964813232421875\n",
            "Epoch [2483/2500], Step [0/17], d_loss: 0.000384577113436535, g_loss: 17.457033157348633\n",
            "Epoch [2484/2500], Step [0/17], d_loss: 0.33214297890663147, g_loss: 53.11219024658203\n",
            "Epoch [2485/2500], Step [0/17], d_loss: 0.0014516878873109818, g_loss: 38.175140380859375\n",
            "Epoch [2486/2500], Step [0/17], d_loss: 0.0004242206341587007, g_loss: 37.433616638183594\n",
            "Epoch [2487/2500], Step [0/17], d_loss: 0.0036053634248673916, g_loss: 21.383792877197266\n",
            "Epoch [2488/2500], Step [0/17], d_loss: 0.0007305201143026352, g_loss: 18.815536499023438\n",
            "Epoch [2489/2500], Step [0/17], d_loss: 0.03505273535847664, g_loss: 12.925024032592773\n",
            "Epoch [2490/2500], Step [0/17], d_loss: 0.007756256498396397, g_loss: 9.114273071289062\n",
            "Epoch [2491/2500], Step [0/17], d_loss: 0.0001910740975290537, g_loss: 21.172557830810547\n",
            "Epoch [2492/2500], Step [0/17], d_loss: 0.001404479262419045, g_loss: 15.126205444335938\n",
            "Epoch [2493/2500], Step [0/17], d_loss: 0.011675402522087097, g_loss: 8.946549415588379\n",
            "Epoch [2494/2500], Step [0/17], d_loss: 0.0016961699584499002, g_loss: 10.013495445251465\n",
            "Epoch [2495/2500], Step [0/17], d_loss: 0.0015473447274416685, g_loss: 25.87460708618164\n",
            "Epoch [2496/2500], Step [0/17], d_loss: 0.012613192200660706, g_loss: 17.237051010131836\n",
            "Epoch [2497/2500], Step [0/17], d_loss: 0.0024007889442145824, g_loss: 11.748624801635742\n",
            "Epoch [2498/2500], Step [0/17], d_loss: 0.005776389501988888, g_loss: 26.651107788085938\n",
            "Epoch [2499/2500], Step [0/17], d_loss: 0.007698189467191696, g_loss: 23.74436378479004\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image\n",
        "#from monet_dataset import dataloader\n",
        "\n",
        "# Create a folder to save generated images and model checkpoints\n",
        "output_folder = 'generated_images'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Define the Generator architecture\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Define the Discriminator architecture\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "num_epochs = 2500\n",
        "latent_dim = 100\n",
        "\n",
        "# Initialize models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Lists to keep track of loss values\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # Train Discriminator\n",
        "        real_images = data.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        outputs = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(dataloader)}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
        "\n",
        "        # Store loss values\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    # Save generated images for every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        save_image(fake_images, os.path.join(output_folder, f'generated_images_{epoch}.png'))\n",
        "\n",
        "    # Save model checkpoints every 50 epochs\n",
        "    if epoch % 50 == 0:\n",
        "        torch.save(generator.state_dict(), f'paths/monet_generator_epoch_{epoch}.pth')\n",
        "        torch.save(discriminator.state_dict(), f'paths/monet_discriminator_epoch_{epoch}.pth')\n",
        "\n",
        "# Final save\n",
        "torch.save(generator.state_dict(), 'monet_generator_final.pth')\n",
        "torch.save(discriminator.state_dict(), 'monet_discriminator_final.pth')\n",
        "\n",
        "# Plot the loss curves\n",
        "# Goal of gen is to produce realistic images that fool the discriminator\n",
        "plt.figure(figsize=(10, 5))\n",
        "# Discriminator loss indicates how well the discrim distinguishes real images from fake ones\n",
        "plt.plot(d_losses, label='Discriminator Loss')  # Low discrim loss = discrim is better at classification\n",
        "# Gen loss measures how well the gen achieves the goal of fooling the discriminator\n",
        "# gen loss quantifies how successful the gen is at creating images that the discriminator classifies as real\n",
        "plt.plot(g_losses, label='Generator Loss')  # Low gen loss = the gen produces more realistic images\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('GAN Loss during Training')\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPkduBv4yHyI"
      },
      "source": [
        "USING GAN WEIGHTS (PTH) GENERATE ONE IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMkuf4j_DAnL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "#Define the Generator architecture (same as before)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "#Load the pre-trained GAN generator\n",
        "generator = Generator()\n",
        "generator.load_state_dict(torch.load('/content/monet_generator_final.pth'))\n",
        "generator.eval()\n",
        "\n",
        "#Generate an initial styled image using GAN\n",
        "z = torch.randn(1, 100, 1, 1)  # Latent vector\n",
        "initial_image = generator(z).detach()  # Generate image and detach from graph\n",
        "\n",
        "#Save the initial GAN-generated image\n",
        "save_image(initial_image, 'initial_gan_image.png')\n",
        "\n",
        "#Load the initial image using PIL\n",
        "initial_image_pil = Image.open('initial_gan_image.png')\n",
        "\n",
        "#Resize the image to 500 x 500\n",
        "resized_image = initial_image_pil.resize((256, 256), Image.BILINEAR)\n",
        "\n",
        "#Save the resized image\n",
        "resized_image.save('resized_gan_image.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4o8AfyC6sqH",
        "outputId": "b85818da-f014-4d32-9ad6-b04712a27fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/paths/ (stored 0%)\n",
            "  adding: content/paths/monet_discriminator_epoch_300.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_750.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1700.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_200.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1050.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_50.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1650.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2200.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2400.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1250.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1150.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1800.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2150.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2250.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1500.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1200.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_750.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1350.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_850.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1350.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2050.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_550.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_100.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1400.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1250.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1550.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_400.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_0.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_50.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_650.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2400.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1600.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2150.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1300.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_150.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_2450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1800.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_500.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_900.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_600.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1550.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_600.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2350.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_0.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_250.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_300.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_850.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1750.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2350.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_800.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1600.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1750.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2100.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1200.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1000.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1150.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_350.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2050.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1950.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2200.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1950.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_550.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1900.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1100.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1900.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1050.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_700.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1650.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2000.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_950.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_500.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_650.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2250.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1850.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1700.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2000.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1400.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_400.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1850.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_800.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1000.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_950.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1500.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_900.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_700.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_1100.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_350.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_250.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_1300.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_200.pth (deflated 8%)\n",
            "  adding: content/paths/monet_generator_epoch_100.pth (deflated 8%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2300.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_150.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_450.pth (deflated 7%)\n",
            "  adding: content/paths/monet_discriminator_epoch_2100.pth (deflated 7%)\n",
            "  adding: content/paths/monet_generator_epoch_2300.pth (deflated 7%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/paths.zip /content/paths/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4nxDkZ3QGf3"
      },
      "source": [
        "MONETIFY: NST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "PW0fqOiRP67R",
        "outputId": "e1b8779c-56f7-4ea8-eb3e-59206efc4c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://31c6bb59142d3049b2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://31c6bb59142d3049b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import time  # track time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gradio as gr  # Import Gradio for the UI\n",
        "from PIL import Image  # This will be used to load the image (or images)\n",
        "import torchvision.transforms as transforms  # Transforms to convert image to a tensor\n",
        "import torchvision.models as models  # Loads vgg19 (which is also manually installed in computer, code in the path*)\n",
        "from torchvision.models import VGG19_Weights  # Rids outdated pretrained parameter\n",
        "from torchvision.utils import save_image  # Store the generated image at the end\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "image_size = 256  # Specify image size\n",
        "\n",
        "# Image loader\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # All images must be of the SAME SIZE! or we won't be able to subtract them when computing loss\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# VGG model class\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.chosen_features = ['0', '5', '10', '19', '28']  # Conv layers we are taking\n",
        "        self.model = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features[:29]  # Go up to 29, where we have inclusive of 28\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []  # We're going to store features in an empty array/list, these features will be the relevant features\n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x = layer(x)  # We can send in x through the layer and our output will be called x\n",
        "            if str(layer_num) in self.chosen_features:  # If string of layer_num is in self.chosen_features, then store it\n",
        "                features.append(x)\n",
        "        return features  # Lastly, return features\n",
        "\n",
        "# Now create a function that can load an image, PIL library used here\n",
        "def load_image(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0)  # Unsqueeze 0 to add another dimension for the batch size, which will be 1\n",
        "    return image.to(device)\n",
        "\n",
        "# Monetification function\n",
        "def monetify_image(original_image_path, total_steps=1100, learning_rate=0.001):\n",
        "    style_image_path = '/content/refined/6003.jpg'  # Path to your Monet style image\n",
        "    original_img = load_image(original_image_path)\n",
        "    style_img = load_image(style_image_path)\n",
        "\n",
        "    model = VGG().to(device).eval()  # .eval() to freeze the weights\n",
        "    generated = original_img.clone().requires_grad_(True)  # Essential to freeze the network, so the only thing that changes is the generated image\n",
        "\n",
        "    alpha = 1  # Different numbers in the paper\n",
        "    beta = 0.01  # Different numbers in the paper\n",
        "    optimizer = optim.Adam([generated], lr=learning_rate)  # Normally would do model.parameters, but we use generated to optimize the image\n",
        "    # then send in the learning rate\n",
        "\n",
        "    start_time = time.time()  # Track start time\n",
        "\n",
        "    for step in range(total_steps):  # How many times the image will be modified\n",
        "        generated_features = model(generated)  # We need to send in each of the 3 images through the VGG network\n",
        "        original_features = model(original_img)\n",
        "        style_features = model(style_img)\n",
        "\n",
        "        style_loss = original_loss = 0\n",
        "        # now we'll iterate through all the features for the chosen layers:\n",
        "        for gen_feature, orig_feature, style_feature in zip(generated_features, original_features, style_features):  # Everything in the 5 conv layers\n",
        "            batch_size, channel, height, width = gen_feature.shape  # remember our batch size is only one!\n",
        "            original_loss += torch.mean((gen_feature - orig_feature) ** 2)  # From equation\n",
        "\n",
        "            # Compute the Gram Matrix\n",
        "            # Here, we've multiplied every pixel value from each channel with every other channel for the generated features\n",
        "            # this is then later subtracted with style gram matrix\n",
        "            G = gen_feature.view(channel, height * width).mm(  # Matrix multiplication\n",
        "                gen_feature.view(channel, height * width).t()  # Transpose\n",
        "            )\n",
        "\n",
        "            # Calculate gram matrix for style\n",
        "            A = style_feature.view(channel, height * width).mm(\n",
        "                style_feature.view(channel, height * width).t()\n",
        "            )\n",
        "\n",
        "            # Now that we have both matrices, we calculate style loss\n",
        "            style_loss += torch.mean((G - A) ** 2)\n",
        "\n",
        "        # After, calculate total loss\n",
        "        total_loss = alpha * original_loss + beta * style_loss\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:  # Log more frequently for testing\n",
        "            print(f'Step [{step}/{total_steps}], Total Loss: {total_loss.item()}')\n",
        "            save_image(generated, f'/content/generated/generated_{step}.png')\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f'Total time for {total_steps} steps: {elapsed_time} seconds')\n",
        "\n",
        "    output_image = generated.detach().cpu()\n",
        "    output_image_pil = transforms.ToPILImage()(output_image.squeeze(0))\n",
        "    return output_image_pil\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_interface(original_image):\n",
        "    # Save the uploaded image temporarily\n",
        "    original_image.save(\"original_image.jpg\")\n",
        "\n",
        "    # Run the monetify function\n",
        "    result_image = monetify_image(\"original_image.jpg\")\n",
        "    return result_image\n",
        "\n",
        "# Gradio UI setup\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Image(type=\"pil\"),\n",
        "    title=\"Monetify Your Image\",\n",
        "    description=\"Upload an image to see it transformed in the style of Monet!\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zHrD4iqFY5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}